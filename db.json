{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"4ee9f53b2f454d366c75978ad3c8dd83ca7614b2","modified":1516765368488},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1516765368508},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1516765368508},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1516765368508},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1516765368508},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1516765368508},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1516765368508},{"_id":"themes/next/README.en.md","hash":"3b0c7998cf17f9cf9e1a5bfcd65679a43a00c817","modified":1516765368508},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1516765368508},{"_id":"themes/next/_config.yml","hash":"a1d10d04b44b397a8c0a7e697698b1162bd04b1e","modified":1516765368508},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1516765368508},{"_id":"themes/next/gulpfile.coffee","hash":"61ef0606a8134894d7ac796bc8d0fa4ba6a94483","modified":1516765368508},{"_id":"themes/next/package.json","hash":"877cb98025e59015532c4c9a04a33e2af4ad56f9","modified":1516765368508},{"_id":"source/_posts/caffe-configuration.md","hash":"8daca41d462aafd288a0a3c13c690ae7dce17d63","modified":1516765368488},{"_id":"source/_posts/caffe_1_layer.md","hash":"265f14e638460bdbbace98b7c2d5b3bc57be0bde","modified":1516765368488},{"_id":"source/_posts/caffe_2_rnn.md","hash":"4ef886a5b1b770a3b78b679669e5a90abaf01829","modified":1516765368488},{"_id":"source/_posts/caffe_3_s2vt_data_process.md","hash":"fd4b46c89bff90952839d5e5aa15adff5a9b7dd1","modified":1516765368488},{"_id":"source/_posts/caffe_4_lstm.md","hash":"8c8b083764de39e570d507480fb535bc9827040b","modified":1516765368488},{"_id":"source/_posts/caffe_5_s2vt_captioner.md","hash":"b5427b7ae7a6a456a986b4e2a8e8de888f5a65e2","modified":1516765368488},{"_id":"source/_posts/imagenet.md","hash":"8bb948100ee41b5c6ed6437f3de227d9fa9a808e","modified":1516765368488},{"_id":"source/_posts/mindmap-goutongdeyishu.md","hash":"cc494412d7fb21e8c9f8b250a9a891f7f9c1154d","modified":1516765368488},{"_id":"source/_posts/mindmap-jingjin.md","hash":"64667d55001c323e09277d9924c25f5acdfaef78","modified":1516765368488},{"_id":"source/_posts/mindmap-zhexuejiamendouganlexieshenme.md","hash":"ab7e1e29e78f782cbb3a14a9c78ea3075a01b45a","modified":1516765368488},{"_id":"source/_posts/note-essence-of-linear-algrbra.md","hash":"88b47159c4953973ec2d84af470e3d5ec77d7da5","modified":1516765368488},{"_id":"source/_posts/note-learning-how-to-learn.md","hash":"a7d3f9421f1ab83892310fc48752bf0da77b07dc","modified":1516765368488},{"_id":"source/_posts/note-linear-algebra.md","hash":"2fae61a9039de51214223bf6d12c4f8b023a8d00","modified":1516765368488},{"_id":"source/_posts/note-reinforcement-learning.md","hash":"e47e621703b7572c0373fae19d9390bbf381b6c9","modified":1516765368492},{"_id":"source/_posts/paper-orca.md","hash":"45a7f4491102fd14cf5546f079288a146dff719e","modified":1516765368496},{"_id":"source/_posts/paper-rvo.md","hash":"1ec8523135848e2677cb767e96e7ed92cdaecc38","modified":1516765368496},{"_id":"source/_posts/paper_rcnn.md","hash":"6dc2e9f10392312fcdc46d994f0eb51cc0173035","modified":1516765368500},{"_id":"source/tags/index.md","hash":"94bbb384e5e284787dbcef35f9ea0208409c2527","modified":1516765368500},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1516765368508},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1516765368508},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1516765368508},{"_id":"themes/next/languages/default.yml","hash":"767470a80dc257e23e14c3a78e8c52a46c9d6209","modified":1516765368508},{"_id":"themes/next/languages/en.yml","hash":"40057d6608e825d06e0864bac4dcd27ed88ada87","modified":1516765368508},{"_id":"themes/next/languages/fr-FR.yml","hash":"9fca01ef917d33ae2ae6bc04561ec6799dff5351","modified":1516765368508},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1516765368508},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1516765368508},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1516765368508},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1516765368508},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1516765368508},{"_id":"themes/next/languages/ru.yml","hash":"257d11e626cbe4b9b78785a764190b9278f95c28","modified":1516765368508},{"_id":"themes/next/languages/zh-hk.yml","hash":"34c84c6d04447a25bd5eac576922a13947c000e2","modified":1516765368508},{"_id":"themes/next/languages/zh-Hans.yml","hash":"f6c9fafa0f5f0050cd07ca2cf5e38fbae3e28145","modified":1516765368508},{"_id":"themes/next/languages/zh-tw.yml","hash":"c97a5c41149de9b17f33439b0ecf0eff6fdae50e","modified":1516765368508},{"_id":"themes/next/layout/_layout.swig","hash":"2fa3c74066843a859fac77803324a1de51044da9","modified":1516765368508},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1516765368508},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1516765368508},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1516765368508},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1516765368508},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1516765368508},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1516765368508},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1516765368508},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1516765368508},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1516765368516},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1516765368516},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1516765368516},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"source/_posts/note-essence-of-linear-algrbra/combination.png","hash":"7fff8d00a0245e19564c05b2037f3f7d836a6e36","modified":1516765368488},{"_id":"source/_posts/note-essence-of-linear-algrbra/dot_product_order.png","hash":"7e98037126903e36880c30bacc83b6232787f9ca","modified":1516765368488},{"_id":"source/_posts/note-essence-of-linear-algrbra/dot_product_projection.png","hash":"d85a3e7b264bb1bff7624a85a6f9f6ef2de727f3","modified":1516765368488},{"_id":"source/_posts/note-essence-of-linear-algrbra/representation.png","hash":"19ad51b9238092074ec0b0dae6c47aaf3940c7b7","modified":1516765368488},{"_id":"source/_posts/note-essence-of-linear-algrbra/transformation.png","hash":"0bf2d5acd0d7e7a81df341aca79c9082a2b194a6","modified":1516765368488},{"_id":"source/_posts/note-reinforcement-learning/lec1_Intro_to_RL.png","hash":"7ea12e992a3e1286dab7a50b1868db90399f9033","modified":1516765368492},{"_id":"source/_posts/note-reinforcement-learning/lec2_MDP.png","hash":"89c39e318abdc30fc5c65db7d79453488ff6e975","modified":1516765368492},{"_id":"source/_posts/note-reinforcement-learning/lec3_Planning_by_DP.png","hash":"3768d82b093bc44890435da2da314704c5d92485","modified":1516765368492},{"_id":"source/_posts/note-reinforcement-learning/lec9_Exploration_and_Exploitation.png","hash":"a5b60b7951e0aaf8a1be5fdbda7dfa14b63739f6","modified":1516765368496},{"_id":"source/_posts/paper-orca/opt.png","hash":"92aef50ae799ffc7ad40a672c6ac824f2ed81342","modified":1516765368496},{"_id":"source/_posts/paper-orca/orca_sol.png","hash":"18e1ed3a4bd3ca056c1a1944e866c224f4d5be62","modified":1516765368496},{"_id":"source/_posts/paper-rvo/left_right.png","hash":"4c42a652cdc5dea16e432cebfea3edf9a8c32189","modified":1516765368500},{"_id":"source/_posts/paper-rvo/oscillation.png","hash":"fb6232bb65cb8c3550060247c48d2aa9283547d6","modified":1516765368500},{"_id":"source/_posts/paper-rvo/prob_disc.png","hash":"3580292f8e4e853a79dff930bf07be0d6752350f","modified":1516765368500},{"_id":"source/_posts/paper-rvo/prob_sol.png","hash":"021f98d1225200335f37a63881d7733986ea7264","modified":1516765368500},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1516765368508},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1516765368508},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1516765368508},{"_id":"themes/next/layout/_macro/post.swig","hash":"e6016def9b512188f4c2725399c9adc7bc41cdae","modified":1516765368508},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1516765368508},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43d8830bb19da4fc7a5773866be19fa066b62645","modified":1516765368508},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1516765368508},{"_id":"themes/next/layout/_partials/comments.swig","hash":"78ccfc1dc915247c1fec3c86d742e0f4c2f6d99c","modified":1516765368508},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1516765368508},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1516765368508},{"_id":"themes/next/layout/_partials/head.swig","hash":"ca56f92e2fa82b03853869f5073ee1a5626a4796","modified":1516765368508},{"_id":"themes/next/layout/_partials/header.swig","hash":"adab5c3f7b173f1b45454787f39dde07aea03483","modified":1516765368508},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"39d613e5a9f8389d4ea52d6082502af8e833b9f2","modified":1516765368508},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1516765368508},{"_id":"themes/next/layout/_partials/search.swig","hash":"1431719d1dbba3f5ee385eebc46376d1a960b2d5","modified":1516765368508},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1516765368508},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1516765368508},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1516765368508},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4512867d80d9eddfc3a0f5fea3c456f33aa9d522","modified":1516765368508},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1516765368508},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1516765368508},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1516765368508},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1516765368508},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1516765368508},{"_id":"themes/next/source/css/main.styl","hash":"f4d6504b783a4944d8d456dd7acb7ba21b64f8c7","modified":1516765368512},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1516765368512},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1516765368512},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1516765368512},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1516765368512},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1516765368512},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1516765368512},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1516765368512},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1516765368512},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1516765368512},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1516765368512},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1516765368512},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1516765368512},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1516765368512},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1516765368512},{"_id":"source/_posts/note-essence-of-linear-algrbra/det.png","hash":"ebb7e873f5987d657e4fc7121eb7f8800604278f","modified":1516765368488},{"_id":"source/_posts/note-reinforcement-learning/lec4_Model_Free_Prediction.png","hash":"71162a00429564ceef6dfd329d58748d01006286","modified":1516765368492},{"_id":"source/_posts/note-reinforcement-learning/lec7_Policy_Gradient.png","hash":"e6b33b6be0d0c2de7aea59fcb75e83b5c241a03f","modified":1516765368496},{"_id":"source/_posts/note-reinforcement-learning/lec6_Value_Function_Approximation.png","hash":"50434bdb5c6c0ba3d3a80f3ecb786b599e0d1267","modified":1516765368496},{"_id":"source/_posts/note-reinforcement-learning/lec8_Integrating_Learning_and_Planning.png","hash":"fb387a3f2e67a1d08f0d892cf56111e363d47d67","modified":1516765368496},{"_id":"source/_posts/paper-orca/orca_app.png","hash":"ebcca0d12534dbc566480dbd7184cbd6ee6722dd","modified":1516765368496},{"_id":"source/_posts/paper-orca/time_interval.png","hash":"0421ecf30741ed973344ce3728d9674c87efe308","modified":1516765368496},{"_id":"source/_posts/paper-rvo/rvo.png","hash":"d22c1078da30825c16b942a4f2aff8155c32181b","modified":1516765368500},{"_id":"source/_posts/paper-rvo/vo.png","hash":"84b84f74b4f83c979842f181b632cc314b30b611","modified":1516765368500},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368508},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368508},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516765368512},{"_id":"source/_posts/note-reinforcement-learning/lec5_Model_Free_Control.png","hash":"b5bb0f24c087f240a8d349ea72f542f6ffd06b83","modified":1516765368492},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1516765368508},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"636f1181dd5887a70b4a08ca8f655d4e46635792","modified":1516765368508},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1516765368508},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1516765368508},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1516765368508},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1516765368508},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1516765368508},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1516765368508},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1516765368508},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1516765368508},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1516765368508},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1516765368508},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"394d9fff7951287cc90f52acc2d4cbfd1bae079d","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"4abc01bc870e1d7a783cdbd26166edc782a6a4f4","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"b460e27db3dcd4ab40b17d8926a5c4e624f293a9","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1516765368508},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1516765368512},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1516765368512},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1516765368512},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1516765368512},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c459aa6d607d8bcb747544e74f6ad0b8374aa3b1","modified":1516765368512},{"_id":"themes/next/source/css/_variables/base.styl","hash":"fc185c6cec79593775d1c2440dbe2a71cfbe2e99","modified":1516765368512},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1516765368512},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"96b29f69b8b916b22f62c9959a117b5a968200a5","modified":1516765368512},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1516765368512},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1516765368512},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1516765368512},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1516765368512},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1516765368512},{"_id":"themes/next/source/js/src/utils.js","hash":"384e17ff857f073060f5bf8c6e4f4b7353236331","modified":1516765368512},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1516765368512},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1516765368512},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1516765368516},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1516765368516},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1516765368516},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"c1072942459fa0880e8a33a1bd929176b62b4171","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1516765368516},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1516765368516},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1516765368516},{"_id":"source/_posts/note-learning-how-to-learn/learning_how_to_learn.png","hash":"a059657d99823762027a024dc3f7b8bff72278a8","modified":1516765368488},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1516765368516},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1516765368508},{"_id":"themes/next/layout/_scripts/third-party/comments/youyan.swig","hash":"ea8078fa9e10be2bb042749d8b6a97adc38f914c","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1516765368508},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1516765368512},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1516765368512},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1516765368512},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1516765368512},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1516765368512},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"bbc82c34b5815ff32974f9675e2e9aeff67ac50f","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl.bak","hash":"bbc82c34b5815ff32974f9675e2e9aeff67ac50f","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"ff9f163bb05c0709577040a875924d36c9ab99d6","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"dcf9fe43b2ef78b923118ba39efedb38760e76b1","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"1408209dfb9a22a0982a30bdbd14842c2b53f264","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9b63bd8effc7cf4b96acdea4d73add7df934a222","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1516765368512},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"9ccee9189c910b8a264802d7b2ec305d12dedcd0","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1516765368512},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1516765368516},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"4eda182cbcc046dbf449aef97c02c230cf80a494","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"fb5b49426dee7f1508500e698d1b3c6b04c8fcce","modified":1516765368516},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1516765368516},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1516765368516},{"_id":"source/_posts/note-linear-algebra/linear_algebra.png","hash":"f88d5ce92c3b630755fc4c945f1f1bfaf6d2a10b","modified":1516765368492},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1516765368516},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1516765368516},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1516765368516},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4b7f81e1006e7acee3d1c840ccba155239f830cc","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"fdfadbb4483043c7e0afd541ee9712389e633517","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"7f1aab694caf603809e33cff82beea84cd0128fd","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"c6dab7661a6b8c678b21b7eb273cef7100f970f6","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"bfd806d0a9f21446a22df82ac02e37d0075cc3b5","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1516765368508},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"3eb73cee103b810fa56901577ecb9c9bb1793cff","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"eba491ae624b4c843c8be4c94a044085dad4ba0f","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"637c6b32c58ecf40041be6e911471cd82671919b","modified":1516765368512},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1516765368512},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1516765368512},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1516765368512},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1516765368516},{"_id":"source/_posts/paper-rvo/five_agents.gif","hash":"808251205f52fe3f90dc17cb14e7a5da62d86bc4","modified":1516765368500},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1516765368512},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1516765368516}],"Category":[{"name":"环境配置","_id":"cjcsiyrp40002xuvt9fs6adoy"},{"name":"caffe","_id":"cjcsiyrpe0007xuvtud26nh1n"},{"name":"论文笔记","_id":"cjcsiyrq7000wxuvtlccsm7gm"},{"name":"思维导图","_id":"cjcsiyrqc0012xuvtb9d5iijj"},{"name":"课程笔记","_id":"cjcsiyrqj001ixuvtu89u5sa5"}],"Data":[],"Page":[{"title":"tags","date":"2016-06-29T11:14:59.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-06-29 19:14:59\ntype: \"tags\"\n---\n","updated":"2018-01-24T03:42:48.500Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjcsiyrux002bxuvtls9ftgoo","content":"","excerpt":"","more":""}],"Post":[{"title":"Ubuntu 16.04配置Caffe环境","date":"2016-06-28T11:21:42.000Z","description":["Ubuntu 16.04配置Caffe环境"],"_content":"\n## 基本配置流程\n1. 从官网克隆到本地 [caffe官网](https://github.com/BVLC/caffe)\n> git clone git@github.com:BVLC/caffe.git\n\n2. 按照官方教程安装 [官方安装教程](http://caffe.berkeleyvision.org/installation.html) \n\t1. 配置依赖文件 [依赖文件](http://caffe.berkeleyvision.org/install_apt.html)\n\t> sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n\tsudo apt-get install --no-install-recommends libboost-all-dev  \n\tsudo apt-get install libatlas-base-dev  \n\tsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n\n\t2. 编译(**默认含GPU，可以用-j4多核加速编译**) [编译](http://caffe.berkeleyvision.org/installation.html#compilation)\n\t> cp Makefile.config.example Makefile.config  \n\tmake all  \n\tmake test  \n\tmake runtest\n\n## 踩过的坑\n- **can't find hdf5.h when build caffe**: 用以下sh改变include路径 [source](https://github.com/NVIDIA/DIGITS/issues/156)\n```\n#!/bin/bash\n# manipulate header path, before building caffe on debian jessie\n# usage:\n# 1. cd root of caffe\n# 2. bash <this_script>\n# 3. build\n\n# transformations :\n#  #include \"hdf5/serial/hdf5.h\" -> #include \"hdf5/serial/hdf5.h\"\n#  #include \"hdf5_hl.h\" -> #include \"hdf5/serial/hdf5_hl.h\"\n\nfind . -type f -exec sed -i -e 's^\"hdf5.h\"^\"hdf5/serial/hdf5.h\"^g' -e 's^\"hdf5_hl.h\"^\"hdf5/serial/hdf5_hl.h\"^g' '{}' \\;\n```\n\n- **/usr/bin/ld: cannot find -lhdf5_hl**: 修改文件名字 [source](https://github.com/NVIDIA/DIGITS/issues/156)  \n> cd /usr/lib/x86_64-linux-gnu  \nsudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so  \nsudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so\n\n- **‘memcpy’ was not declared in this scope**: 修改Makefile.config [source](https://github.com/BVLC/caffe/issues/4046)\n> 将Makefile.config中的  \nNVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  \n修改为  \nNVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  \n\n- **安装Cuda**:  Cuda官网只有给出14.04和15.04的Cuda7.5版本，但实际上可以直接通过apt-get获取16.04的Cuda7.5\n\t1. 安装cuda\n\t> sudo apt-get install nvidia-cuda-toolkit\n\t2. 修改caffe/Makefile.config中的CUDA_DIR\n\n\n- **安装显卡驱动**: 系统设置-软件和更新-附加驱动，选择最新的显卡驱动，应用。\n\t* 安装驱动后屏幕无法调节亮度的解决办法 [调节亮度](http://askubuntu.com/questions/76081/brightness-not-working-after-installing-nvidia-driver)\n\t* 使用`nvidia-setttings`查看gpu使用情况\n\n\n\n\n","source":"_posts/caffe-configuration.md","raw":"---\ntitle: Ubuntu 16.04配置Caffe环境\ndate: 2016-06-28 19:21:42\ntags: \n  - caffe配置\ndescription:\n  - Ubuntu 16.04配置Caffe环境\ncategories:\n  - 环境配置\n---\n\n## 基本配置流程\n1. 从官网克隆到本地 [caffe官网](https://github.com/BVLC/caffe)\n> git clone git@github.com:BVLC/caffe.git\n\n2. 按照官方教程安装 [官方安装教程](http://caffe.berkeleyvision.org/installation.html) \n\t1. 配置依赖文件 [依赖文件](http://caffe.berkeleyvision.org/install_apt.html)\n\t> sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n\tsudo apt-get install --no-install-recommends libboost-all-dev  \n\tsudo apt-get install libatlas-base-dev  \n\tsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n\n\t2. 编译(**默认含GPU，可以用-j4多核加速编译**) [编译](http://caffe.berkeleyvision.org/installation.html#compilation)\n\t> cp Makefile.config.example Makefile.config  \n\tmake all  \n\tmake test  \n\tmake runtest\n\n## 踩过的坑\n- **can't find hdf5.h when build caffe**: 用以下sh改变include路径 [source](https://github.com/NVIDIA/DIGITS/issues/156)\n```\n#!/bin/bash\n# manipulate header path, before building caffe on debian jessie\n# usage:\n# 1. cd root of caffe\n# 2. bash <this_script>\n# 3. build\n\n# transformations :\n#  #include \"hdf5/serial/hdf5.h\" -> #include \"hdf5/serial/hdf5.h\"\n#  #include \"hdf5_hl.h\" -> #include \"hdf5/serial/hdf5_hl.h\"\n\nfind . -type f -exec sed -i -e 's^\"hdf5.h\"^\"hdf5/serial/hdf5.h\"^g' -e 's^\"hdf5_hl.h\"^\"hdf5/serial/hdf5_hl.h\"^g' '{}' \\;\n```\n\n- **/usr/bin/ld: cannot find -lhdf5_hl**: 修改文件名字 [source](https://github.com/NVIDIA/DIGITS/issues/156)  \n> cd /usr/lib/x86_64-linux-gnu  \nsudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so  \nsudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so\n\n- **‘memcpy’ was not declared in this scope**: 修改Makefile.config [source](https://github.com/BVLC/caffe/issues/4046)\n> 将Makefile.config中的  \nNVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  \n修改为  \nNVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  \n\n- **安装Cuda**:  Cuda官网只有给出14.04和15.04的Cuda7.5版本，但实际上可以直接通过apt-get获取16.04的Cuda7.5\n\t1. 安装cuda\n\t> sudo apt-get install nvidia-cuda-toolkit\n\t2. 修改caffe/Makefile.config中的CUDA_DIR\n\n\n- **安装显卡驱动**: 系统设置-软件和更新-附加驱动，选择最新的显卡驱动，应用。\n\t* 安装驱动后屏幕无法调节亮度的解决办法 [调节亮度](http://askubuntu.com/questions/76081/brightness-not-working-after-installing-nvidia-driver)\n\t* 使用`nvidia-setttings`查看gpu使用情况\n\n\n\n\n","slug":"caffe-configuration","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyroy0000xuvt1whvqdlk","content":"<h2 id=\"基本配置流程\"><a href=\"#基本配置流程\" class=\"headerlink\" title=\"基本配置流程\"></a>基本配置流程</h2><ol>\n<li><p>从官网克隆到本地 <a href=\"https://github.com/BVLC/caffe\" target=\"_blank\" rel=\"external\">caffe官网</a></p>\n<blockquote>\n<p>git clone git@github.com:BVLC/caffe.git</p>\n</blockquote>\n</li>\n<li><p>按照官方教程安装 <a href=\"http://caffe.berkeleyvision.org/installation.html\" target=\"_blank\" rel=\"external\">官方安装教程</a> </p>\n<ol>\n<li><p>配置依赖文件 <a href=\"http://caffe.berkeleyvision.org/install_apt.html\" target=\"_blank\" rel=\"external\">依赖文件</a></p>\n<blockquote>\n<p>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler<br>sudo apt-get install –no-install-recommends libboost-all-dev<br>sudo apt-get install libatlas-base-dev<br>sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</p>\n</blockquote>\n</li>\n<li><p>编译(<strong>默认含GPU，可以用-j4多核加速编译</strong>) <a href=\"http://caffe.berkeleyvision.org/installation.html#compilation\" target=\"_blank\" rel=\"external\">编译</a></p>\n<blockquote>\n<p>cp Makefile.config.example Makefile.config<br>make all<br>make test<br>make runtest</p>\n</blockquote>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"踩过的坑\"><a href=\"#踩过的坑\" class=\"headerlink\" title=\"踩过的坑\"></a>踩过的坑</h2><ul>\n<li><p><strong>can’t find hdf5.h when build caffe</strong>: 用以下sh改变include路径 <a href=\"https://github.com/NVIDIA/DIGITS/issues/156\" target=\"_blank\" rel=\"external\">source</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/bash</div><div class=\"line\"># manipulate header path, before building caffe on debian jessie</div><div class=\"line\"># usage:</div><div class=\"line\"># 1. cd root of caffe</div><div class=\"line\"># 2. bash &lt;this_script&gt;</div><div class=\"line\"># 3. build</div><div class=\"line\"></div><div class=\"line\"># transformations :</div><div class=\"line\">#  #include &quot;hdf5/serial/hdf5.h&quot; -&gt; #include &quot;hdf5/serial/hdf5.h&quot;</div><div class=\"line\">#  #include &quot;hdf5_hl.h&quot; -&gt; #include &quot;hdf5/serial/hdf5_hl.h&quot;</div><div class=\"line\"></div><div class=\"line\">find . -type f -exec sed -i -e &apos;s^&quot;hdf5.h&quot;^&quot;hdf5/serial/hdf5.h&quot;^g&apos; -e &apos;s^&quot;hdf5_hl.h&quot;^&quot;hdf5/serial/hdf5_hl.h&quot;^g&apos; &apos;&#123;&#125;&apos; \\;</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>/usr/bin/ld: cannot find -lhdf5_hl</strong>: 修改文件名字 <a href=\"https://github.com/NVIDIA/DIGITS/issues/156\" target=\"_blank\" rel=\"external\">source</a>  </p>\n<blockquote>\n<p>cd /usr/lib/x86_64-linux-gnu<br>sudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so<br>sudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so</p>\n</blockquote>\n</li>\n<li><p><strong>‘memcpy’ was not declared in this scope</strong>: 修改Makefile.config <a href=\"https://github.com/BVLC/caffe/issues/4046\" target=\"_blank\" rel=\"external\">source</a></p>\n<blockquote>\n<p>将Makefile.config中的<br>NVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)<br>修改为<br>NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  </p>\n</blockquote>\n</li>\n<li><p><strong>安装Cuda</strong>:  Cuda官网只有给出14.04和15.04的Cuda7.5版本，但实际上可以直接通过apt-get获取16.04的Cuda7.5</p>\n<ol>\n<li>安装cuda<blockquote>\n<p>sudo apt-get install nvidia-cuda-toolkit</p>\n</blockquote>\n</li>\n<li>修改caffe/Makefile.config中的CUDA_DIR</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><strong>安装显卡驱动</strong>: 系统设置-软件和更新-附加驱动，选择最新的显卡驱动，应用。<ul>\n<li>安装驱动后屏幕无法调节亮度的解决办法 <a href=\"http://askubuntu.com/questions/76081/brightness-not-working-after-installing-nvidia-driver\" target=\"_blank\" rel=\"external\">调节亮度</a></li>\n<li>使用<code>nvidia-setttings</code>查看gpu使用情况</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"基本配置流程\"><a href=\"#基本配置流程\" class=\"headerlink\" title=\"基本配置流程\"></a>基本配置流程</h2><ol>\n<li><p>从官网克隆到本地 <a href=\"https://github.com/BVLC/caffe\">caffe官网</a></p>\n<blockquote>\n<p>git clone git@github.com:BVLC/caffe.git</p>\n</blockquote>\n</li>\n<li><p>按照官方教程安装 <a href=\"http://caffe.berkeleyvision.org/installation.html\">官方安装教程</a> </p>\n<ol>\n<li><p>配置依赖文件 <a href=\"http://caffe.berkeleyvision.org/install_apt.html\">依赖文件</a></p>\n<blockquote>\n<p>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler<br>sudo apt-get install –no-install-recommends libboost-all-dev<br>sudo apt-get install libatlas-base-dev<br>sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</p>\n</blockquote>\n</li>\n<li><p>编译(<strong>默认含GPU，可以用-j4多核加速编译</strong>) <a href=\"http://caffe.berkeleyvision.org/installation.html#compilation\">编译</a></p>\n<blockquote>\n<p>cp Makefile.config.example Makefile.config<br>make all<br>make test<br>make runtest</p>\n</blockquote>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"踩过的坑\"><a href=\"#踩过的坑\" class=\"headerlink\" title=\"踩过的坑\"></a>踩过的坑</h2><ul>\n<li><p><strong>can’t find hdf5.h when build caffe</strong>: 用以下sh改变include路径 <a href=\"https://github.com/NVIDIA/DIGITS/issues/156\">source</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/bash</div><div class=\"line\"># manipulate header path, before building caffe on debian jessie</div><div class=\"line\"># usage:</div><div class=\"line\"># 1. cd root of caffe</div><div class=\"line\"># 2. bash &lt;this_script&gt;</div><div class=\"line\"># 3. build</div><div class=\"line\"></div><div class=\"line\"># transformations :</div><div class=\"line\">#  #include &quot;hdf5/serial/hdf5.h&quot; -&gt; #include &quot;hdf5/serial/hdf5.h&quot;</div><div class=\"line\">#  #include &quot;hdf5_hl.h&quot; -&gt; #include &quot;hdf5/serial/hdf5_hl.h&quot;</div><div class=\"line\"></div><div class=\"line\">find . -type f -exec sed -i -e &apos;s^&quot;hdf5.h&quot;^&quot;hdf5/serial/hdf5.h&quot;^g&apos; -e &apos;s^&quot;hdf5_hl.h&quot;^&quot;hdf5/serial/hdf5_hl.h&quot;^g&apos; &apos;&#123;&#125;&apos; \\;</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>/usr/bin/ld: cannot find -lhdf5_hl</strong>: 修改文件名字 <a href=\"https://github.com/NVIDIA/DIGITS/issues/156\">source</a>  </p>\n<blockquote>\n<p>cd /usr/lib/x86_64-linux-gnu<br>sudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so<br>sudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so</p>\n</blockquote>\n</li>\n<li><p><strong>‘memcpy’ was not declared in this scope</strong>: 修改Makefile.config <a href=\"https://github.com/BVLC/caffe/issues/4046\">source</a></p>\n<blockquote>\n<p>将Makefile.config中的<br>NVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)<br>修改为<br>NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)  </p>\n</blockquote>\n</li>\n<li><p><strong>安装Cuda</strong>:  Cuda官网只有给出14.04和15.04的Cuda7.5版本，但实际上可以直接通过apt-get获取16.04的Cuda7.5</p>\n<ol>\n<li>安装cuda<blockquote>\n<p>sudo apt-get install nvidia-cuda-toolkit</p>\n</blockquote>\n</li>\n<li>修改caffe/Makefile.config中的CUDA_DIR</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><strong>安装显卡驱动</strong>: 系统设置-软件和更新-附加驱动，选择最新的显卡驱动，应用。<ul>\n<li>安装驱动后屏幕无法调节亮度的解决办法 <a href=\"http://askubuntu.com/questions/76081/brightness-not-working-after-installing-nvidia-driver\">调节亮度</a></li>\n<li>使用<code>nvidia-setttings</code>查看gpu使用情况</li>\n</ul>\n</li>\n</ul>\n"},{"title":"Caffe学习：自定义数据读取","date":"2016-07-04T11:23:42.000Z","description":["Caffe基本框架，阅读和修改image_data_layer源码，使用Caffe的基本流程"],"_content":"\n## Caffe架构\n\n### 简介\nCaffe将层的实现（输入层，卷积层，全连接层），整体网络的定义（AlexNet，LeNet），以及超参数的定义（步长，迭代次数）解耦。用c++实现各层，并使用protocol buffer来定义网络和传入参数\n\n### 基础构件\n\n.cpp文件实现一个个层，作为基础构件供上层调用\n\n- **CAFFE_ROOT/include/caffe/layers**: .hpp为层的头文件\n- **CAFFE_ROOT/src/caffe/layers**: .cpp为层的实现文件\n- **CAFFE_ROOT/src/caffe/proto**: .proto为传参数用的protocol buffer文件\n\n### 自定义网络\n\n- **MODEL_ROOT/train_val.prototxt**: 用作定义整体网络\n- **MODEL_ROOT/solver.prototxt**: 为网络传递超参数\n- **MODEL_ROOT/train.sh**: 开始训练的脚本\n- **MODEL_ROOT/test.sh**: 开始测试的脚本\n\n## 自定义数据读取层\n\n### 数据描述\n[MSRA 10K Salient Object Database](http://mmcheng.net/msra10k/)。含有1w个数据，.jpg结尾的是原图，.png结尾的是标注了的图。用9k张作为训练集，用1k张作为测试集\n\n### image_data_layer总览\n\nimage_data_layer实现了三个函数和调用了两个宏\n\n- **DataLayerSetUp**: 把容器的规格设置好（设好N, K, H, W)\n- **ShuffleImages**: 打乱顺序\n- **load_batch**: 真正把图片读入内存\n- **INSTANTIATE_CLASS**和**REGISTER_LAYER_CLASS**: 注册创建的层\n\n\n### DataLayerSetUp\n\n- **读取图片规格**: 从protocol buffer中读取目标图片的k,h,w（有可能原图800\\*600，但想要256\\*256，这时就需要压缩）\n```cpp\n  const int new_height = this->layer_param_.image_data_param().new_height();\n  const int new_width  = this->layer_param_.image_data_param().new_width();\n  const bool is_color  = this->layer_param_.image_data_param().is_color();\n  string root_folder = this->layer_param_.image_data_param().root_folder();\n\n```\n- **读取图片列表**: 从一个txt文件中读取图片名及其对应label，写进lines_\n```cpp\n  while (std::getline(infile, line)) {\n    pos = line.find_last_of(' ');\n    label = atoi(line.substr(pos + 1).c_str());\n    lines_.push_back(std::make_pair(line.substr(0, pos), label));\n  }\n```\n- **shuffle & skip**\n- **设置容器规格**: 读一张图进来，并用它的规格来设置容器的规格\n```cpp\n  // Read an image, and use it to initialize the top blob.\n  cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,\n                                    new_height, new_width, is_color);\n  CHECK(cv_img.data) << \"Could not load \" << lines_[lines_id_].first;\n  // Use data_transformer to infer the expected blob shape from a cv_image.\n  vector<int> top_shape = this->data_transformer_->InferBlobShape(cv_img);\n  this->transformed_data_.Reshape(top_shape);\n  // Reshape prefetch_data and top[0] according to the batch_size.\n  const int batch_size = this->layer_param_.image_data_param().batch_size();\n  CHECK_GT(batch_size, 0) << \"Positive batch size required\";\n  top_shape[0] = batch_size;\n  for (int i = 0; i < this->PREFETCH_COUNT; ++i) {\n    this->prefetch_[i].data_.Reshape(top_shape);\n  }\n  top[0]->Reshape(top_shape);\n\n```\n\n### load_batch\n\n- **读取图片规格**\n- **设置容器规格**\n- **读入图片**: prefetch\\_data为起始地址，offset根据容器的规格来求，然后在prefetch\\_data+offset的位置设置一个容器transformed\\_data\\_，并将图片写到容器里面\n```cpp\n    cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,\n        new_height, new_width, is_color);\n    CHECK(cv_img.data) << \"Could not load \" << lines_[lines_id_].first;\n    read_time += timer.MicroSeconds();\n    timer.Start();\n    // Apply transformations (mirror, crop...) to the image\n    int offset = batch->data_.offset(item_id);\n    this->transformed_data_.set_cpu_data(prefetch_data + offset);\n    this->data_transformer_->Transform(cv_img, &(this->transformed_data_));\n    trans_time += timer.MicroSeconds();\n```\n\n### 自定义image_data_layer_disc\n\n复现的模型为[DISC](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7372470&tag=1)，label不是一个简单的int，而是一张图片，所以要对数据输入层进行修改，来产生一个图片label\n\n- **image_data_layer_disc.hpp**: 修改lines_的定义，改成两个string的pair\n- **image_data_layer_disc.cpp**: 参考读入data的方式处理label\n- **caffe.proto**: 添加一个disc_image_data_para来添加关于label的参数\n\n## 训练和测试\n\n- **train_val.prototxt**: 赋值models里面的AlexNet，将data层改成我们的image_data_layer_disc，将loss层改成欧式距离，删除accuracy层\n- **solver.prototxt**: 网络指定为train_val.prototxt，学习率指定为1e-9\n- **train.sh**: \n```shell\n#!/usr/bin/env sh\n\n./build/tools/caffe train --solver=models/disc/solver.prototxt\n\n```\n- **test.sh**:\n```shell\n#!/usr/bin/env sh\n\n./build/tools/caffe.bin test -model=models/disc/train_val.prototxt -weights=models/disc/caffe_alexnet_train_iter_370000.caffemodel -gpu=1\n```\n## Protocol Buffer\n给出一些介绍Protocol Buffer的网站\n- [IMB](https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/)\n- [Google](https://developers.google.com/protocol-buffers/docs/encoding#optional)\n- [pb1](http://www.iloveandroid.net/2015/10/08/studyPtorobuf/)\n- [pb2](http://www.cnblogs.com/royenhome/archive/2010/10/29/1864860.html)","source":"_posts/caffe_1_layer.md","raw":"---\ntitle: Caffe学习：自定义数据读取\ndate: 2016-07-04 19:23:42\ntags: \n  - caffe学习\ndescription:\n  - Caffe基本框架，阅读和修改image_data_layer源码，使用Caffe的基本流程\ncategories:\n  - caffe\n---\n\n## Caffe架构\n\n### 简介\nCaffe将层的实现（输入层，卷积层，全连接层），整体网络的定义（AlexNet，LeNet），以及超参数的定义（步长，迭代次数）解耦。用c++实现各层，并使用protocol buffer来定义网络和传入参数\n\n### 基础构件\n\n.cpp文件实现一个个层，作为基础构件供上层调用\n\n- **CAFFE_ROOT/include/caffe/layers**: .hpp为层的头文件\n- **CAFFE_ROOT/src/caffe/layers**: .cpp为层的实现文件\n- **CAFFE_ROOT/src/caffe/proto**: .proto为传参数用的protocol buffer文件\n\n### 自定义网络\n\n- **MODEL_ROOT/train_val.prototxt**: 用作定义整体网络\n- **MODEL_ROOT/solver.prototxt**: 为网络传递超参数\n- **MODEL_ROOT/train.sh**: 开始训练的脚本\n- **MODEL_ROOT/test.sh**: 开始测试的脚本\n\n## 自定义数据读取层\n\n### 数据描述\n[MSRA 10K Salient Object Database](http://mmcheng.net/msra10k/)。含有1w个数据，.jpg结尾的是原图，.png结尾的是标注了的图。用9k张作为训练集，用1k张作为测试集\n\n### image_data_layer总览\n\nimage_data_layer实现了三个函数和调用了两个宏\n\n- **DataLayerSetUp**: 把容器的规格设置好（设好N, K, H, W)\n- **ShuffleImages**: 打乱顺序\n- **load_batch**: 真正把图片读入内存\n- **INSTANTIATE_CLASS**和**REGISTER_LAYER_CLASS**: 注册创建的层\n\n\n### DataLayerSetUp\n\n- **读取图片规格**: 从protocol buffer中读取目标图片的k,h,w（有可能原图800\\*600，但想要256\\*256，这时就需要压缩）\n```cpp\n  const int new_height = this->layer_param_.image_data_param().new_height();\n  const int new_width  = this->layer_param_.image_data_param().new_width();\n  const bool is_color  = this->layer_param_.image_data_param().is_color();\n  string root_folder = this->layer_param_.image_data_param().root_folder();\n\n```\n- **读取图片列表**: 从一个txt文件中读取图片名及其对应label，写进lines_\n```cpp\n  while (std::getline(infile, line)) {\n    pos = line.find_last_of(' ');\n    label = atoi(line.substr(pos + 1).c_str());\n    lines_.push_back(std::make_pair(line.substr(0, pos), label));\n  }\n```\n- **shuffle & skip**\n- **设置容器规格**: 读一张图进来，并用它的规格来设置容器的规格\n```cpp\n  // Read an image, and use it to initialize the top blob.\n  cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,\n                                    new_height, new_width, is_color);\n  CHECK(cv_img.data) << \"Could not load \" << lines_[lines_id_].first;\n  // Use data_transformer to infer the expected blob shape from a cv_image.\n  vector<int> top_shape = this->data_transformer_->InferBlobShape(cv_img);\n  this->transformed_data_.Reshape(top_shape);\n  // Reshape prefetch_data and top[0] according to the batch_size.\n  const int batch_size = this->layer_param_.image_data_param().batch_size();\n  CHECK_GT(batch_size, 0) << \"Positive batch size required\";\n  top_shape[0] = batch_size;\n  for (int i = 0; i < this->PREFETCH_COUNT; ++i) {\n    this->prefetch_[i].data_.Reshape(top_shape);\n  }\n  top[0]->Reshape(top_shape);\n\n```\n\n### load_batch\n\n- **读取图片规格**\n- **设置容器规格**\n- **读入图片**: prefetch\\_data为起始地址，offset根据容器的规格来求，然后在prefetch\\_data+offset的位置设置一个容器transformed\\_data\\_，并将图片写到容器里面\n```cpp\n    cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,\n        new_height, new_width, is_color);\n    CHECK(cv_img.data) << \"Could not load \" << lines_[lines_id_].first;\n    read_time += timer.MicroSeconds();\n    timer.Start();\n    // Apply transformations (mirror, crop...) to the image\n    int offset = batch->data_.offset(item_id);\n    this->transformed_data_.set_cpu_data(prefetch_data + offset);\n    this->data_transformer_->Transform(cv_img, &(this->transformed_data_));\n    trans_time += timer.MicroSeconds();\n```\n\n### 自定义image_data_layer_disc\n\n复现的模型为[DISC](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7372470&tag=1)，label不是一个简单的int，而是一张图片，所以要对数据输入层进行修改，来产生一个图片label\n\n- **image_data_layer_disc.hpp**: 修改lines_的定义，改成两个string的pair\n- **image_data_layer_disc.cpp**: 参考读入data的方式处理label\n- **caffe.proto**: 添加一个disc_image_data_para来添加关于label的参数\n\n## 训练和测试\n\n- **train_val.prototxt**: 赋值models里面的AlexNet，将data层改成我们的image_data_layer_disc，将loss层改成欧式距离，删除accuracy层\n- **solver.prototxt**: 网络指定为train_val.prototxt，学习率指定为1e-9\n- **train.sh**: \n```shell\n#!/usr/bin/env sh\n\n./build/tools/caffe train --solver=models/disc/solver.prototxt\n\n```\n- **test.sh**:\n```shell\n#!/usr/bin/env sh\n\n./build/tools/caffe.bin test -model=models/disc/train_val.prototxt -weights=models/disc/caffe_alexnet_train_iter_370000.caffemodel -gpu=1\n```\n## Protocol Buffer\n给出一些介绍Protocol Buffer的网站\n- [IMB](https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/)\n- [Google](https://developers.google.com/protocol-buffers/docs/encoding#optional)\n- [pb1](http://www.iloveandroid.net/2015/10/08/studyPtorobuf/)\n- [pb2](http://www.cnblogs.com/royenhome/archive/2010/10/29/1864860.html)","slug":"caffe_1_layer","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrp20001xuvtt3d3w5v9","content":"<h2 id=\"Caffe架构\"><a href=\"#Caffe架构\" class=\"headerlink\" title=\"Caffe架构\"></a>Caffe架构</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>Caffe将层的实现（输入层，卷积层，全连接层），整体网络的定义（AlexNet，LeNet），以及超参数的定义（步长，迭代次数）解耦。用c++实现各层，并使用protocol buffer来定义网络和传入参数</p>\n<h3 id=\"基础构件\"><a href=\"#基础构件\" class=\"headerlink\" title=\"基础构件\"></a>基础构件</h3><p>.cpp文件实现一个个层，作为基础构件供上层调用</p>\n<ul>\n<li><strong>CAFFE_ROOT/include/caffe/layers</strong>: .hpp为层的头文件</li>\n<li><strong>CAFFE_ROOT/src/caffe/layers</strong>: .cpp为层的实现文件</li>\n<li><strong>CAFFE_ROOT/src/caffe/proto</strong>: .proto为传参数用的protocol buffer文件</li>\n</ul>\n<h3 id=\"自定义网络\"><a href=\"#自定义网络\" class=\"headerlink\" title=\"自定义网络\"></a>自定义网络</h3><ul>\n<li><strong>MODEL_ROOT/train_val.prototxt</strong>: 用作定义整体网络</li>\n<li><strong>MODEL_ROOT/solver.prototxt</strong>: 为网络传递超参数</li>\n<li><strong>MODEL_ROOT/train.sh</strong>: 开始训练的脚本</li>\n<li><strong>MODEL_ROOT/test.sh</strong>: 开始测试的脚本</li>\n</ul>\n<h2 id=\"自定义数据读取层\"><a href=\"#自定义数据读取层\" class=\"headerlink\" title=\"自定义数据读取层\"></a>自定义数据读取层</h2><h3 id=\"数据描述\"><a href=\"#数据描述\" class=\"headerlink\" title=\"数据描述\"></a>数据描述</h3><p><a href=\"http://mmcheng.net/msra10k/\" target=\"_blank\" rel=\"external\">MSRA 10K Salient Object Database</a>。含有1w个数据，.jpg结尾的是原图，.png结尾的是标注了的图。用9k张作为训练集，用1k张作为测试集</p>\n<h3 id=\"image-data-layer总览\"><a href=\"#image-data-layer总览\" class=\"headerlink\" title=\"image_data_layer总览\"></a>image_data_layer总览</h3><p>image_data_layer实现了三个函数和调用了两个宏</p>\n<ul>\n<li><strong>DataLayerSetUp</strong>: 把容器的规格设置好（设好N, K, H, W)</li>\n<li><strong>ShuffleImages</strong>: 打乱顺序</li>\n<li><strong>load_batch</strong>: 真正把图片读入内存</li>\n<li><strong>INSTANTIATE_CLASS</strong>和<strong>REGISTER_LAYER_CLASS</strong>: 注册创建的层</li>\n</ul>\n<h3 id=\"DataLayerSetUp\"><a href=\"#DataLayerSetUp\" class=\"headerlink\" title=\"DataLayerSetUp\"></a>DataLayerSetUp</h3><ul>\n<li><p><strong>读取图片规格</strong>: 从protocol buffer中读取目标图片的k,h,w（有可能原图800*600，但想要256*256，这时就需要压缩）</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> new_height = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().new_height();</div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> new_width  = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().new_width();</div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">bool</span> is_color  = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().is_color();</div><div class=\"line\"><span class=\"built_in\">string</span> root_folder = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().root_folder();</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>读取图片列表</strong>: 从一个txt文件中读取图片名及其对应label，写进lines_</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> (<span class=\"built_in\">std</span>::getline(infile, line)) &#123;</div><div class=\"line\">  pos = line.find_last_of(<span class=\"string\">' '</span>);</div><div class=\"line\">  label = atoi(line.substr(pos + <span class=\"number\">1</span>).c_str());</div><div class=\"line\">  lines_.push_back(<span class=\"built_in\">std</span>::make_pair(line.substr(<span class=\"number\">0</span>, pos), label));</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>shuffle &amp; skip</strong></p>\n</li>\n<li><strong>设置容器规格</strong>: 读一张图进来，并用它的规格来设置容器的规格<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Read an image, and use it to initialize the top blob.</span></div><div class=\"line\">cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</div><div class=\"line\">                                  new_height, new_width, is_color);</div><div class=\"line\">CHECK(cv_img.data) &lt;&lt; <span class=\"string\">\"Could not load \"</span> &lt;&lt; lines_[lines_id_].first;</div><div class=\"line\"><span class=\"comment\">// Use data_transformer to infer the expected blob shape from a cv_image.</span></div><div class=\"line\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; top_shape = <span class=\"keyword\">this</span>-&gt;data_transformer_-&gt;InferBlobShape(cv_img);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;transformed_data_.Reshape(top_shape);</div><div class=\"line\"><span class=\"comment\">// Reshape prefetch_data and top[0] according to the batch_size.</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> batch_size = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().batch_size();</div><div class=\"line\">CHECK_GT(batch_size, <span class=\"number\">0</span>) &lt;&lt; <span class=\"string\">\"Positive batch size required\"</span>;</div><div class=\"line\">top_shape[<span class=\"number\">0</span>] = batch_size;</div><div class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;PREFETCH_COUNT; ++i) &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;prefetch_[i].data_.Reshape(top_shape);</div><div class=\"line\">&#125;</div><div class=\"line\">top[<span class=\"number\">0</span>]-&gt;Reshape(top_shape);</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"load-batch\"><a href=\"#load-batch\" class=\"headerlink\" title=\"load_batch\"></a>load_batch</h3><ul>\n<li><strong>读取图片规格</strong></li>\n<li><strong>设置容器规格</strong></li>\n<li><strong>读入图片</strong>: prefetch_data为起始地址，offset根据容器的规格来求，然后在prefetch_data+offset的位置设置一个容器transformed_data_，并将图片写到容器里面<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</div><div class=\"line\">    new_height, new_width, is_color);</div><div class=\"line\">CHECK(cv_img.data) &lt;&lt; <span class=\"string\">\"Could not load \"</span> &lt;&lt; lines_[lines_id_].first;</div><div class=\"line\">read_time += timer.MicroSeconds();</div><div class=\"line\">timer.Start();</div><div class=\"line\"><span class=\"comment\">// Apply transformations (mirror, crop...) to the image</span></div><div class=\"line\"><span class=\"keyword\">int</span> offset = batch-&gt;data_.offset(item_id);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;transformed_data_.set_cpu_data(prefetch_data + offset);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;data_transformer_-&gt;Transform(cv_img, &amp;(<span class=\"keyword\">this</span>-&gt;transformed_data_));</div><div class=\"line\">trans_time += timer.MicroSeconds();</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"自定义image-data-layer-disc\"><a href=\"#自定义image-data-layer-disc\" class=\"headerlink\" title=\"自定义image_data_layer_disc\"></a>自定义image_data_layer_disc</h3><p>复现的模型为<a href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7372470&amp;tag=1\" target=\"_blank\" rel=\"external\">DISC</a>，label不是一个简单的int，而是一张图片，所以要对数据输入层进行修改，来产生一个图片label</p>\n<ul>\n<li><strong>image_data_layer_disc.hpp</strong>: 修改lines_的定义，改成两个string的pair</li>\n<li><strong>image_data_layer_disc.cpp</strong>: 参考读入data的方式处理label</li>\n<li><strong>caffe.proto</strong>: 添加一个disc_image_data_para来添加关于label的参数</li>\n</ul>\n<h2 id=\"训练和测试\"><a href=\"#训练和测试\" class=\"headerlink\" title=\"训练和测试\"></a>训练和测试</h2><ul>\n<li><strong>train_val.prototxt</strong>: 赋值models里面的AlexNet，将data层改成我们的image_data_layer_disc，将loss层改成欧式距离，删除accuracy层</li>\n<li><strong>solver.prototxt</strong>: 网络指定为train_val.prototxt，学习率指定为1e-9</li>\n<li><p><strong>train.sh</strong>: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/usr/bin/env sh</div><div class=\"line\"></div><div class=\"line\">./build/tools/caffe train --solver=models/disc/solver.prototxt</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>test.sh</strong>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/usr/bin/env sh</div><div class=\"line\"></div><div class=\"line\">./build/tools/caffe.bin test -model=models/disc/train_val.prototxt -weights=models/disc/caffe_alexnet_train_iter_370000.caffemodel -gpu=1</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"Protocol-Buffer\"><a href=\"#Protocol-Buffer\" class=\"headerlink\" title=\"Protocol Buffer\"></a>Protocol Buffer</h2><p>给出一些介绍Protocol Buffer的网站</p>\n<ul>\n<li><a href=\"https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/\" target=\"_blank\" rel=\"external\">IMB</a></li>\n<li><a href=\"https://developers.google.com/protocol-buffers/docs/encoding#optional\" target=\"_blank\" rel=\"external\">Google</a></li>\n<li><a href=\"http://www.iloveandroid.net/2015/10/08/studyPtorobuf/\" target=\"_blank\" rel=\"external\">pb1</a></li>\n<li><a href=\"http://www.cnblogs.com/royenhome/archive/2010/10/29/1864860.html\" target=\"_blank\" rel=\"external\">pb2</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"Caffe架构\"><a href=\"#Caffe架构\" class=\"headerlink\" title=\"Caffe架构\"></a>Caffe架构</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>Caffe将层的实现（输入层，卷积层，全连接层），整体网络的定义（AlexNet，LeNet），以及超参数的定义（步长，迭代次数）解耦。用c++实现各层，并使用protocol buffer来定义网络和传入参数</p>\n<h3 id=\"基础构件\"><a href=\"#基础构件\" class=\"headerlink\" title=\"基础构件\"></a>基础构件</h3><p>.cpp文件实现一个个层，作为基础构件供上层调用</p>\n<ul>\n<li><strong>CAFFE_ROOT/include/caffe/layers</strong>: .hpp为层的头文件</li>\n<li><strong>CAFFE_ROOT/src/caffe/layers</strong>: .cpp为层的实现文件</li>\n<li><strong>CAFFE_ROOT/src/caffe/proto</strong>: .proto为传参数用的protocol buffer文件</li>\n</ul>\n<h3 id=\"自定义网络\"><a href=\"#自定义网络\" class=\"headerlink\" title=\"自定义网络\"></a>自定义网络</h3><ul>\n<li><strong>MODEL_ROOT/train_val.prototxt</strong>: 用作定义整体网络</li>\n<li><strong>MODEL_ROOT/solver.prototxt</strong>: 为网络传递超参数</li>\n<li><strong>MODEL_ROOT/train.sh</strong>: 开始训练的脚本</li>\n<li><strong>MODEL_ROOT/test.sh</strong>: 开始测试的脚本</li>\n</ul>\n<h2 id=\"自定义数据读取层\"><a href=\"#自定义数据读取层\" class=\"headerlink\" title=\"自定义数据读取层\"></a>自定义数据读取层</h2><h3 id=\"数据描述\"><a href=\"#数据描述\" class=\"headerlink\" title=\"数据描述\"></a>数据描述</h3><p><a href=\"http://mmcheng.net/msra10k/\">MSRA 10K Salient Object Database</a>。含有1w个数据，.jpg结尾的是原图，.png结尾的是标注了的图。用9k张作为训练集，用1k张作为测试集</p>\n<h3 id=\"image-data-layer总览\"><a href=\"#image-data-layer总览\" class=\"headerlink\" title=\"image_data_layer总览\"></a>image_data_layer总览</h3><p>image_data_layer实现了三个函数和调用了两个宏</p>\n<ul>\n<li><strong>DataLayerSetUp</strong>: 把容器的规格设置好（设好N, K, H, W)</li>\n<li><strong>ShuffleImages</strong>: 打乱顺序</li>\n<li><strong>load_batch</strong>: 真正把图片读入内存</li>\n<li><strong>INSTANTIATE_CLASS</strong>和<strong>REGISTER_LAYER_CLASS</strong>: 注册创建的层</li>\n</ul>\n<h3 id=\"DataLayerSetUp\"><a href=\"#DataLayerSetUp\" class=\"headerlink\" title=\"DataLayerSetUp\"></a>DataLayerSetUp</h3><ul>\n<li><p><strong>读取图片规格</strong>: 从protocol buffer中读取目标图片的k,h,w（有可能原图800*600，但想要256*256，这时就需要压缩）</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> new_height = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().new_height();</div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> new_width  = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().new_width();</div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">bool</span> is_color  = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().is_color();</div><div class=\"line\"><span class=\"built_in\">string</span> root_folder = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().root_folder();</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>读取图片列表</strong>: 从一个txt文件中读取图片名及其对应label，写进lines_</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> (<span class=\"built_in\">std</span>::getline(infile, line)) &#123;</div><div class=\"line\">  pos = line.find_last_of(<span class=\"string\">' '</span>);</div><div class=\"line\">  label = atoi(line.substr(pos + <span class=\"number\">1</span>).c_str());</div><div class=\"line\">  lines_.push_back(<span class=\"built_in\">std</span>::make_pair(line.substr(<span class=\"number\">0</span>, pos), label));</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>shuffle &amp; skip</strong></p>\n</li>\n<li><strong>设置容器规格</strong>: 读一张图进来，并用它的规格来设置容器的规格<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Read an image, and use it to initialize the top blob.</span></div><div class=\"line\">cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</div><div class=\"line\">                                  new_height, new_width, is_color);</div><div class=\"line\">CHECK(cv_img.data) &lt;&lt; <span class=\"string\">\"Could not load \"</span> &lt;&lt; lines_[lines_id_].first;</div><div class=\"line\"><span class=\"comment\">// Use data_transformer to infer the expected blob shape from a cv_image.</span></div><div class=\"line\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; top_shape = <span class=\"keyword\">this</span>-&gt;data_transformer_-&gt;InferBlobShape(cv_img);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;transformed_data_.Reshape(top_shape);</div><div class=\"line\"><span class=\"comment\">// Reshape prefetch_data and top[0] according to the batch_size.</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span> batch_size = <span class=\"keyword\">this</span>-&gt;layer_param_.image_data_param().batch_size();</div><div class=\"line\">CHECK_GT(batch_size, <span class=\"number\">0</span>) &lt;&lt; <span class=\"string\">\"Positive batch size required\"</span>;</div><div class=\"line\">top_shape[<span class=\"number\">0</span>] = batch_size;</div><div class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;PREFETCH_COUNT; ++i) &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;prefetch_[i].data_.Reshape(top_shape);</div><div class=\"line\">&#125;</div><div class=\"line\">top[<span class=\"number\">0</span>]-&gt;Reshape(top_shape);</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"load-batch\"><a href=\"#load-batch\" class=\"headerlink\" title=\"load_batch\"></a>load_batch</h3><ul>\n<li><strong>读取图片规格</strong></li>\n<li><strong>设置容器规格</strong></li>\n<li><strong>读入图片</strong>: prefetch_data为起始地址，offset根据容器的规格来求，然后在prefetch_data+offset的位置设置一个容器transformed_data_，并将图片写到容器里面<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</div><div class=\"line\">    new_height, new_width, is_color);</div><div class=\"line\">CHECK(cv_img.data) &lt;&lt; <span class=\"string\">\"Could not load \"</span> &lt;&lt; lines_[lines_id_].first;</div><div class=\"line\">read_time += timer.MicroSeconds();</div><div class=\"line\">timer.Start();</div><div class=\"line\"><span class=\"comment\">// Apply transformations (mirror, crop...) to the image</span></div><div class=\"line\"><span class=\"keyword\">int</span> offset = batch-&gt;data_.offset(item_id);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;transformed_data_.set_cpu_data(prefetch_data + offset);</div><div class=\"line\"><span class=\"keyword\">this</span>-&gt;data_transformer_-&gt;Transform(cv_img, &amp;(<span class=\"keyword\">this</span>-&gt;transformed_data_));</div><div class=\"line\">trans_time += timer.MicroSeconds();</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"自定义image-data-layer-disc\"><a href=\"#自定义image-data-layer-disc\" class=\"headerlink\" title=\"自定义image_data_layer_disc\"></a>自定义image_data_layer_disc</h3><p>复现的模型为<a href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7372470&amp;tag=1\">DISC</a>，label不是一个简单的int，而是一张图片，所以要对数据输入层进行修改，来产生一个图片label</p>\n<ul>\n<li><strong>image_data_layer_disc.hpp</strong>: 修改lines_的定义，改成两个string的pair</li>\n<li><strong>image_data_layer_disc.cpp</strong>: 参考读入data的方式处理label</li>\n<li><strong>caffe.proto</strong>: 添加一个disc_image_data_para来添加关于label的参数</li>\n</ul>\n<h2 id=\"训练和测试\"><a href=\"#训练和测试\" class=\"headerlink\" title=\"训练和测试\"></a>训练和测试</h2><ul>\n<li><strong>train_val.prototxt</strong>: 赋值models里面的AlexNet，将data层改成我们的image_data_layer_disc，将loss层改成欧式距离，删除accuracy层</li>\n<li><strong>solver.prototxt</strong>: 网络指定为train_val.prototxt，学习率指定为1e-9</li>\n<li><p><strong>train.sh</strong>: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/usr/bin/env sh</div><div class=\"line\"></div><div class=\"line\">./build/tools/caffe train --solver=models/disc/solver.prototxt</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>test.sh</strong>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/usr/bin/env sh</div><div class=\"line\"></div><div class=\"line\">./build/tools/caffe.bin test -model=models/disc/train_val.prototxt -weights=models/disc/caffe_alexnet_train_iter_370000.caffemodel -gpu=1</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"Protocol-Buffer\"><a href=\"#Protocol-Buffer\" class=\"headerlink\" title=\"Protocol Buffer\"></a>Protocol Buffer</h2><p>给出一些介绍Protocol Buffer的网站</p>\n<ul>\n<li><a href=\"https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/\">IMB</a></li>\n<li><a href=\"https://developers.google.com/protocol-buffers/docs/encoding#optional\">Google</a></li>\n<li><a href=\"http://www.iloveandroid.net/2015/10/08/studyPtorobuf/\">pb1</a></li>\n<li><a href=\"http://www.cnblogs.com/royenhome/archive/2010/10/29/1864860.html\">pb2</a></li>\n</ul>\n"},{"title":"Caffe学习：RNN源码阅读","date":"2016-07-13T07:20:42.000Z","description":["Caffe中的RNN源码阅读"],"_content":"\n## 简介\nRNN(Recurrent Neural Network)是一种能考虑上下文信息的神经网络，在求解的时候不止考虑当前的输入是什么，还考虑上一次的输出是什么，有种马尔可夫链的感觉，比较适用于包含上下文语义的任务。这里选了标准RNN源码入手，学习RNN的实现。\n\n## 源码框架\n\n### 目录结构\n- **sequence_layers.hpp:**  抽象类RecurrentLayer，子类RNN和LSTM的头文件\n- **recurrent_layer.cpp:** 抽象类RecurrentLayer的定义文件\n- **rnn_layer.cpp:** 子类RNN的定义文件\n\n### 逻辑结构\n- 使用了**模板方法设计模式**\n- **RecurrentLayer:** 定义了网络的通用框架，包括输入，输出，循环部分的入口输入\n- **RNNLayer:** 定义了循环部分的网络结构\n- **bottom:**\n\t* bottom[0]: T，帧数\n\t* bottom[1]: N，视频数\n\t* ...: 真正的data\n- **top:** 同bottom\n\n## sequence_layers.hpp\n\n### 成员变量\n- **shared\\_ptr< Net< Dtype> \\> unrolled\\_net\\_:** 最终生成的网络\n- **int N\\_:** 视频流的数量\n- **int T\\_:** 视频帧数\n- **bool static\\_input\\_:** 输入是否静态\n- **vector< Blob< Dtype\\>* \\> recur\\_input\\_blobs\\_:** 子类循环部分的入口输入($h_0$)\n- **vector< Blob< Dtype\\>* \\> recur\\_output\\_blobs\\_:** 子类循环部分的出口输出($h_T$)\n- **vector< Blob< Dtype\\>* \\> output\\_blobs\\_:** 总输出(top)\n- **Blob< Dtype\\>* x\\_input\\_blob\\_:** 总视频输入(bottom[0])\n- **Blob< Dtype\\>* x\\_static\\_input\\_blob\\_:** 总静态输入(bottom[2])\n- **Blob< Dtype\\>* cont\\_input\\_blob\\_:** 总标识输入(bottom[1])。cont为0表示应该新开一个序列，不用再参照上一次的输出结果\n\n### 成员函数\n- **FillUnrolledNet(NetParameter* net\\_param):** \t\n\t* 子类根据自己的内部网络修改net\\_param\n\t* 原理同我们在外面写prototxt生成网络\n\t* 类比安卓开发中动态生成按钮选项，都是因为数量经常变化，写死在xml文件中不便于改动\n- **xxxBlobNames:** \n\t* 返回与主控部分交互的blob名字，主控部分根据名字找到对应的blob\n\t* 类比安卓开发中findViewByID将控件和变量绑定\n\n## recurrent_layer.cpp\n\n### Reshape\n适配blob，以及绑定blob，使得操作变量等价于修改bottom与top\n\n1. **Reshape:** 将x\\_input\\_blob\\_, cont\\_input\\_blob\\_和x\\_static\\_input\\_blob\\_几个输入blob的shape适配bottom\n2. **Reshape:** 将recur\\_input\\_blobs\\_适配子类的入口输出blob(即$h_0$)\n3. **Share:** 将x\\_input\\_blob\\_, cont\\_input\\_blob\\_和x\\_static\\_input\\_blob\\_与bottom的data及diff进行绑定\n4. **Share:** 将output\\_blobs\\_与top的data及diff进行绑定\n\n### LayerSetUp\n等价于将prototxt转为代码，核心为：初始化net\\_param来生成一个Net\n\n1. **检查bottom是否规范**\n\t- bottom[0]: 各帧，各视频的数据 (T \\* N \\* ...)\n\t- bottom[1]: 各帧，各视频的标识，0为开始，1为继续 (T \\* N \\* 1)\n\t- bottom[2]: 静态输入\n2. **为网络接入总输入，并为各输入命名**\n\t- x: bottom[0]\n\t- cont: bottom[1]\n\t- x_static: bottom[2]\n3. **进一步构建网络**，调用FillUnrolledNet修改net\\_param\n4. **用net\\_param生成最终的网络**\n```cpp\n  unrolled_net_.reset(new Net<Dtype>(net_param));\n```\n5. **将变量与网络对应部分绑定**\n\t- 在上一步中网络已经生成完毕，但为了操作方便，将要用到的网络部分跟变量绑定，以后直接用变量进行操作\n\t- 绑定的时候使用net->blob\\_by\\_name(\"xxx\")，类似findViewById。找blob的key是名字，这就是为什么要有xxxBlobNames这样的函数\n6. **设置辅助参数**\n\t- blobs\\_: 这个参数应该要记录本层所有的blob，但由于我们的网络定义不仅是prototxt，还有中途动态生成的部分，所以不能依赖caffe帮我们自动生成的blobs\\_，要手动将所有参数添加\n\t- param\\_propagate\\_down\\_: 记录是否要bp\n\t- recur\\_output\\_blobs\\_全部置0**（不懂）**\n\n## rnn_layer.cpp\n\n### 跟主控交接部分\n\n- **循环输入入口:** $h_0$，1 \\* N \\* num\\_output（循环输入跟循环输出shape一致，所以这里既说明了输入由说明了输出，即对于每一帧，生成num\\_output维的向量）\n- **循环输出出口:** $h_t$\n- **总输出:** o\n\n### FillUnrolledNet\n\n核心部分，实现网络的定义。实际效果近似于这样的prototxt\n\n``` \n################################\n#            input             #\n################################\nlayer{\n\t#sliced x\n\tbottom: x\n\ttop: x_1\n\ttop: x_2\n\t.\n\t.\n\t.\n\ttop: x_t\n}\nlayer{\n\t#sliced cont\n\tbottom: cont\n\ttop: cont_1\n\ttop: cont_2\n\t.\n\t.\n\t.\n\ttop: cont_t\n}\n################################\n#          recur layer         #\n################################\nlayer{\n\t#recur_unit_1\n\tbottom: x_1, cont_1, h_0\n\ttop: o_1, h_1\n}\nlayer{\n\t#recur_unit_2\n\tbottom: x_2, cont_2, h_1\n\ttop: o_2, h_2\n}\n\t\t.\n\t\t.\n\t\t.\n\nlayer{\n\t#recur_unit_t\n\tbottom: x_t, cont_t, h_t-1\n\ttop: o_t, h_t\n}\n################################\n#           output             #\n################################\nlayer{\n\t#concated output\n\tbottom: o_1\n\tbottom: o_2\n\t.\n\t.\n\t.\n\tbottom: o_t\n\ttop: o\n}\n```\n\n## 收获\n\n- 阅读源码前前思考代码的**输入**是什么，**输出**是什么，给你这样的任务**你会怎么做**\n- 然后在阅读的时候合理地进行**假设**，一步步验证并修改\n- 先看**数据结构**再看算法，**粒度**从粗到细（类级，函数级，块级，行级）\n\n\n## Reference\n- 论文地址：[s2vt](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)\n- 项目地址：[Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487)\n- 参考资料：[LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Sequences in Caffe](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-sequences.pdf)\n","source":"_posts/caffe_2_rnn.md","raw":"---\ntitle: Caffe学习：RNN源码阅读\ndate: 2016-07-13 15:20:42\ntags: \n  - rnn\ndescription:\n  - Caffe中的RNN源码阅读\ncategories:\n  - caffe\n---\n\n## 简介\nRNN(Recurrent Neural Network)是一种能考虑上下文信息的神经网络，在求解的时候不止考虑当前的输入是什么，还考虑上一次的输出是什么，有种马尔可夫链的感觉，比较适用于包含上下文语义的任务。这里选了标准RNN源码入手，学习RNN的实现。\n\n## 源码框架\n\n### 目录结构\n- **sequence_layers.hpp:**  抽象类RecurrentLayer，子类RNN和LSTM的头文件\n- **recurrent_layer.cpp:** 抽象类RecurrentLayer的定义文件\n- **rnn_layer.cpp:** 子类RNN的定义文件\n\n### 逻辑结构\n- 使用了**模板方法设计模式**\n- **RecurrentLayer:** 定义了网络的通用框架，包括输入，输出，循环部分的入口输入\n- **RNNLayer:** 定义了循环部分的网络结构\n- **bottom:**\n\t* bottom[0]: T，帧数\n\t* bottom[1]: N，视频数\n\t* ...: 真正的data\n- **top:** 同bottom\n\n## sequence_layers.hpp\n\n### 成员变量\n- **shared\\_ptr< Net< Dtype> \\> unrolled\\_net\\_:** 最终生成的网络\n- **int N\\_:** 视频流的数量\n- **int T\\_:** 视频帧数\n- **bool static\\_input\\_:** 输入是否静态\n- **vector< Blob< Dtype\\>* \\> recur\\_input\\_blobs\\_:** 子类循环部分的入口输入($h_0$)\n- **vector< Blob< Dtype\\>* \\> recur\\_output\\_blobs\\_:** 子类循环部分的出口输出($h_T$)\n- **vector< Blob< Dtype\\>* \\> output\\_blobs\\_:** 总输出(top)\n- **Blob< Dtype\\>* x\\_input\\_blob\\_:** 总视频输入(bottom[0])\n- **Blob< Dtype\\>* x\\_static\\_input\\_blob\\_:** 总静态输入(bottom[2])\n- **Blob< Dtype\\>* cont\\_input\\_blob\\_:** 总标识输入(bottom[1])。cont为0表示应该新开一个序列，不用再参照上一次的输出结果\n\n### 成员函数\n- **FillUnrolledNet(NetParameter* net\\_param):** \t\n\t* 子类根据自己的内部网络修改net\\_param\n\t* 原理同我们在外面写prototxt生成网络\n\t* 类比安卓开发中动态生成按钮选项，都是因为数量经常变化，写死在xml文件中不便于改动\n- **xxxBlobNames:** \n\t* 返回与主控部分交互的blob名字，主控部分根据名字找到对应的blob\n\t* 类比安卓开发中findViewByID将控件和变量绑定\n\n## recurrent_layer.cpp\n\n### Reshape\n适配blob，以及绑定blob，使得操作变量等价于修改bottom与top\n\n1. **Reshape:** 将x\\_input\\_blob\\_, cont\\_input\\_blob\\_和x\\_static\\_input\\_blob\\_几个输入blob的shape适配bottom\n2. **Reshape:** 将recur\\_input\\_blobs\\_适配子类的入口输出blob(即$h_0$)\n3. **Share:** 将x\\_input\\_blob\\_, cont\\_input\\_blob\\_和x\\_static\\_input\\_blob\\_与bottom的data及diff进行绑定\n4. **Share:** 将output\\_blobs\\_与top的data及diff进行绑定\n\n### LayerSetUp\n等价于将prototxt转为代码，核心为：初始化net\\_param来生成一个Net\n\n1. **检查bottom是否规范**\n\t- bottom[0]: 各帧，各视频的数据 (T \\* N \\* ...)\n\t- bottom[1]: 各帧，各视频的标识，0为开始，1为继续 (T \\* N \\* 1)\n\t- bottom[2]: 静态输入\n2. **为网络接入总输入，并为各输入命名**\n\t- x: bottom[0]\n\t- cont: bottom[1]\n\t- x_static: bottom[2]\n3. **进一步构建网络**，调用FillUnrolledNet修改net\\_param\n4. **用net\\_param生成最终的网络**\n```cpp\n  unrolled_net_.reset(new Net<Dtype>(net_param));\n```\n5. **将变量与网络对应部分绑定**\n\t- 在上一步中网络已经生成完毕，但为了操作方便，将要用到的网络部分跟变量绑定，以后直接用变量进行操作\n\t- 绑定的时候使用net->blob\\_by\\_name(\"xxx\")，类似findViewById。找blob的key是名字，这就是为什么要有xxxBlobNames这样的函数\n6. **设置辅助参数**\n\t- blobs\\_: 这个参数应该要记录本层所有的blob，但由于我们的网络定义不仅是prototxt，还有中途动态生成的部分，所以不能依赖caffe帮我们自动生成的blobs\\_，要手动将所有参数添加\n\t- param\\_propagate\\_down\\_: 记录是否要bp\n\t- recur\\_output\\_blobs\\_全部置0**（不懂）**\n\n## rnn_layer.cpp\n\n### 跟主控交接部分\n\n- **循环输入入口:** $h_0$，1 \\* N \\* num\\_output（循环输入跟循环输出shape一致，所以这里既说明了输入由说明了输出，即对于每一帧，生成num\\_output维的向量）\n- **循环输出出口:** $h_t$\n- **总输出:** o\n\n### FillUnrolledNet\n\n核心部分，实现网络的定义。实际效果近似于这样的prototxt\n\n``` \n################################\n#            input             #\n################################\nlayer{\n\t#sliced x\n\tbottom: x\n\ttop: x_1\n\ttop: x_2\n\t.\n\t.\n\t.\n\ttop: x_t\n}\nlayer{\n\t#sliced cont\n\tbottom: cont\n\ttop: cont_1\n\ttop: cont_2\n\t.\n\t.\n\t.\n\ttop: cont_t\n}\n################################\n#          recur layer         #\n################################\nlayer{\n\t#recur_unit_1\n\tbottom: x_1, cont_1, h_0\n\ttop: o_1, h_1\n}\nlayer{\n\t#recur_unit_2\n\tbottom: x_2, cont_2, h_1\n\ttop: o_2, h_2\n}\n\t\t.\n\t\t.\n\t\t.\n\nlayer{\n\t#recur_unit_t\n\tbottom: x_t, cont_t, h_t-1\n\ttop: o_t, h_t\n}\n################################\n#           output             #\n################################\nlayer{\n\t#concated output\n\tbottom: o_1\n\tbottom: o_2\n\t.\n\t.\n\t.\n\tbottom: o_t\n\ttop: o\n}\n```\n\n## 收获\n\n- 阅读源码前前思考代码的**输入**是什么，**输出**是什么，给你这样的任务**你会怎么做**\n- 然后在阅读的时候合理地进行**假设**，一步步验证并修改\n- 先看**数据结构**再看算法，**粒度**从粗到细（类级，函数级，块级，行级）\n\n\n## Reference\n- 论文地址：[s2vt](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)\n- 项目地址：[Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487)\n- 参考资料：[LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Sequences in Caffe](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-sequences.pdf)\n","slug":"caffe_2_rnn","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrp80004xuvteyei9jiw","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>RNN(Recurrent Neural Network)是一种能考虑上下文信息的神经网络，在求解的时候不止考虑当前的输入是什么，还考虑上一次的输出是什么，有种马尔可夫链的感觉，比较适用于包含上下文语义的任务。这里选了标准RNN源码入手，学习RNN的实现。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h3><ul>\n<li><strong>sequence_layers.hpp:</strong>  抽象类RecurrentLayer，子类RNN和LSTM的头文件</li>\n<li><strong>recurrent_layer.cpp:</strong> 抽象类RecurrentLayer的定义文件</li>\n<li><strong>rnn_layer.cpp:</strong> 子类RNN的定义文件</li>\n</ul>\n<h3 id=\"逻辑结构\"><a href=\"#逻辑结构\" class=\"headerlink\" title=\"逻辑结构\"></a>逻辑结构</h3><ul>\n<li>使用了<strong>模板方法设计模式</strong></li>\n<li><strong>RecurrentLayer:</strong> 定义了网络的通用框架，包括输入，输出，循环部分的入口输入</li>\n<li><strong>RNNLayer:</strong> 定义了循环部分的网络结构</li>\n<li><strong>bottom:</strong><ul>\n<li>bottom[0]: T，帧数</li>\n<li>bottom[1]: N，视频数</li>\n<li>…: 真正的data</li>\n</ul>\n</li>\n<li><strong>top:</strong> 同bottom</li>\n</ul>\n<h2 id=\"sequence-layers-hpp\"><a href=\"#sequence-layers-hpp\" class=\"headerlink\" title=\"sequence_layers.hpp\"></a>sequence_layers.hpp</h2><h3 id=\"成员变量\"><a href=\"#成员变量\" class=\"headerlink\" title=\"成员变量\"></a>成员变量</h3><ul>\n<li><strong>shared_ptr&lt; Net&lt; Dtype&gt; > unrolled_net_:</strong> 最终生成的网络</li>\n<li><strong>int N_:</strong> 视频流的数量</li>\n<li><strong>int T_:</strong> 视频帧数</li>\n<li><strong>bool static_input_:</strong> 输入是否静态</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > recur_input_blobs_:</strong> 子类循环部分的入口输入($h_0$)</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > recur_output_blobs_:</strong> 子类循环部分的出口输出($h_T$)</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > output_blobs_:</strong> 总输出(top)</li>\n<li><strong>Blob&lt; Dtype>* x_input_blob_:</strong> 总视频输入(bottom[0])</li>\n<li><strong>Blob&lt; Dtype>* x_static_input_blob_:</strong> 总静态输入(bottom[2])</li>\n<li><strong>Blob&lt; Dtype>* cont_input_blob_:</strong> 总标识输入(bottom[1])。cont为0表示应该新开一个序列，不用再参照上一次的输出结果</li>\n</ul>\n<h3 id=\"成员函数\"><a href=\"#成员函数\" class=\"headerlink\" title=\"成员函数\"></a>成员函数</h3><ul>\n<li><strong>FillUnrolledNet(NetParameter* net_param):</strong>     <ul>\n<li>子类根据自己的内部网络修改net_param</li>\n<li>原理同我们在外面写prototxt生成网络</li>\n<li>类比安卓开发中动态生成按钮选项，都是因为数量经常变化，写死在xml文件中不便于改动</li>\n</ul>\n</li>\n<li><strong>xxxBlobNames:</strong> <ul>\n<li>返回与主控部分交互的blob名字，主控部分根据名字找到对应的blob</li>\n<li>类比安卓开发中findViewByID将控件和变量绑定</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"recurrent-layer-cpp\"><a href=\"#recurrent-layer-cpp\" class=\"headerlink\" title=\"recurrent_layer.cpp\"></a>recurrent_layer.cpp</h2><h3 id=\"Reshape\"><a href=\"#Reshape\" class=\"headerlink\" title=\"Reshape\"></a>Reshape</h3><p>适配blob，以及绑定blob，使得操作变量等价于修改bottom与top</p>\n<ol>\n<li><strong>Reshape:</strong> 将x_input_blob_, cont_input_blob_和x_static_input_blob_几个输入blob的shape适配bottom</li>\n<li><strong>Reshape:</strong> 将recur_input_blobs_适配子类的入口输出blob(即$h_0$)</li>\n<li><strong>Share:</strong> 将x_input_blob_, cont_input_blob_和x_static_input_blob_与bottom的data及diff进行绑定</li>\n<li><strong>Share:</strong> 将output_blobs_与top的data及diff进行绑定</li>\n</ol>\n<h3 id=\"LayerSetUp\"><a href=\"#LayerSetUp\" class=\"headerlink\" title=\"LayerSetUp\"></a>LayerSetUp</h3><p>等价于将prototxt转为代码，核心为：初始化net_param来生成一个Net</p>\n<ol>\n<li><strong>检查bottom是否规范</strong><ul>\n<li>bottom[0]: 各帧，各视频的数据 (T * N * …)</li>\n<li>bottom[1]: 各帧，各视频的标识，0为开始，1为继续 (T * N * 1)</li>\n<li>bottom[2]: 静态输入</li>\n</ul>\n</li>\n<li><strong>为网络接入总输入，并为各输入命名</strong><ul>\n<li>x: bottom[0]</li>\n<li>cont: bottom[1]</li>\n<li>x_static: bottom[2]</li>\n</ul>\n</li>\n<li><strong>进一步构建网络</strong>，调用FillUnrolledNet修改net_param</li>\n<li><p><strong>用net_param生成最终的网络</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">unrolled_net_.reset(<span class=\"keyword\">new</span> Net&lt;Dtype&gt;(net_param));</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>将变量与网络对应部分绑定</strong></p>\n<ul>\n<li>在上一步中网络已经生成完毕，但为了操作方便，将要用到的网络部分跟变量绑定，以后直接用变量进行操作</li>\n<li>绑定的时候使用net-&gt;blob_by_name(“xxx”)，类似findViewById。找blob的key是名字，这就是为什么要有xxxBlobNames这样的函数</li>\n</ul>\n</li>\n<li><strong>设置辅助参数</strong><ul>\n<li>blobs_: 这个参数应该要记录本层所有的blob，但由于我们的网络定义不仅是prototxt，还有中途动态生成的部分，所以不能依赖caffe帮我们自动生成的blobs_，要手动将所有参数添加</li>\n<li>param_propagate_down_: 记录是否要bp</li>\n<li>recur_output_blobs_全部置0<strong>（不懂）</strong></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"rnn-layer-cpp\"><a href=\"#rnn-layer-cpp\" class=\"headerlink\" title=\"rnn_layer.cpp\"></a>rnn_layer.cpp</h2><h3 id=\"跟主控交接部分\"><a href=\"#跟主控交接部分\" class=\"headerlink\" title=\"跟主控交接部分\"></a>跟主控交接部分</h3><ul>\n<li><strong>循环输入入口:</strong> $h_0$，1 * N * num_output（循环输入跟循环输出shape一致，所以这里既说明了输入由说明了输出，即对于每一帧，生成num_output维的向量）</li>\n<li><strong>循环输出出口:</strong> $h_t$</li>\n<li><strong>总输出:</strong> o</li>\n</ul>\n<h3 id=\"FillUnrolledNet\"><a href=\"#FillUnrolledNet\" class=\"headerlink\" title=\"FillUnrolledNet\"></a>FillUnrolledNet</h3><p>核心部分，实现网络的定义。实际效果近似于这样的prototxt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\">################################</div><div class=\"line\">#            input             #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#sliced x</div><div class=\"line\">\tbottom: x</div><div class=\"line\">\ttop: x_1</div><div class=\"line\">\ttop: x_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\ttop: x_t</div><div class=\"line\">&#125;</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#sliced cont</div><div class=\"line\">\tbottom: cont</div><div class=\"line\">\ttop: cont_1</div><div class=\"line\">\ttop: cont_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\ttop: cont_t</div><div class=\"line\">&#125;</div><div class=\"line\">################################</div><div class=\"line\">#          recur layer         #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_1</div><div class=\"line\">\tbottom: x_1, cont_1, h_0</div><div class=\"line\">\ttop: o_1, h_1</div><div class=\"line\">&#125;</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_2</div><div class=\"line\">\tbottom: x_2, cont_2, h_1</div><div class=\"line\">\ttop: o_2, h_2</div><div class=\"line\">&#125;</div><div class=\"line\">\t\t.</div><div class=\"line\">\t\t.</div><div class=\"line\">\t\t.</div><div class=\"line\"></div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_t</div><div class=\"line\">\tbottom: x_t, cont_t, h_t-1</div><div class=\"line\">\ttop: o_t, h_t</div><div class=\"line\">&#125;</div><div class=\"line\">################################</div><div class=\"line\">#           output             #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#concated output</div><div class=\"line\">\tbottom: o_1</div><div class=\"line\">\tbottom: o_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\tbottom: o_t</div><div class=\"line\">\ttop: o</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>阅读源码前前思考代码的<strong>输入</strong>是什么，<strong>输出</strong>是什么，给你这样的任务<strong>你会怎么做</strong></li>\n<li>然后在阅读的时候合理地进行<strong>假设</strong>，一步步验证并修改</li>\n<li>先看<strong>数据结构</strong>再看算法，<strong>粒度</strong>从粗到细（类级，函数级，块级，行级）</li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li>论文地址：<a href=\"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\" target=\"_blank\" rel=\"external\">s2vt</a></li>\n<li>项目地址：<a href=\"https://arxiv.org/abs/1505.00487\" target=\"_blank\" rel=\"external\">Sequence to Sequence – Video to Text</a></li>\n<li>参考资料：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"external\">LSTM</a>, <a href=\"http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-sequences.pdf\" target=\"_blank\" rel=\"external\">Sequences in Caffe</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>RNN(Recurrent Neural Network)是一种能考虑上下文信息的神经网络，在求解的时候不止考虑当前的输入是什么，还考虑上一次的输出是什么，有种马尔可夫链的感觉，比较适用于包含上下文语义的任务。这里选了标准RNN源码入手，学习RNN的实现。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h3><ul>\n<li><strong>sequence_layers.hpp:</strong>  抽象类RecurrentLayer，子类RNN和LSTM的头文件</li>\n<li><strong>recurrent_layer.cpp:</strong> 抽象类RecurrentLayer的定义文件</li>\n<li><strong>rnn_layer.cpp:</strong> 子类RNN的定义文件</li>\n</ul>\n<h3 id=\"逻辑结构\"><a href=\"#逻辑结构\" class=\"headerlink\" title=\"逻辑结构\"></a>逻辑结构</h3><ul>\n<li>使用了<strong>模板方法设计模式</strong></li>\n<li><strong>RecurrentLayer:</strong> 定义了网络的通用框架，包括输入，输出，循环部分的入口输入</li>\n<li><strong>RNNLayer:</strong> 定义了循环部分的网络结构</li>\n<li><strong>bottom:</strong><ul>\n<li>bottom[0]: T，帧数</li>\n<li>bottom[1]: N，视频数</li>\n<li>…: 真正的data</li>\n</ul>\n</li>\n<li><strong>top:</strong> 同bottom</li>\n</ul>\n<h2 id=\"sequence-layers-hpp\"><a href=\"#sequence-layers-hpp\" class=\"headerlink\" title=\"sequence_layers.hpp\"></a>sequence_layers.hpp</h2><h3 id=\"成员变量\"><a href=\"#成员变量\" class=\"headerlink\" title=\"成员变量\"></a>成员变量</h3><ul>\n<li><strong>shared_ptr&lt; Net&lt; Dtype&gt; > unrolled_net_:</strong> 最终生成的网络</li>\n<li><strong>int N_:</strong> 视频流的数量</li>\n<li><strong>int T_:</strong> 视频帧数</li>\n<li><strong>bool static_input_:</strong> 输入是否静态</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > recur_input_blobs_:</strong> 子类循环部分的入口输入($h_0$)</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > recur_output_blobs_:</strong> 子类循环部分的出口输出($h_T$)</li>\n<li><strong>vector&lt; Blob&lt; Dtype>* > output_blobs_:</strong> 总输出(top)</li>\n<li><strong>Blob&lt; Dtype>* x_input_blob_:</strong> 总视频输入(bottom[0])</li>\n<li><strong>Blob&lt; Dtype>* x_static_input_blob_:</strong> 总静态输入(bottom[2])</li>\n<li><strong>Blob&lt; Dtype>* cont_input_blob_:</strong> 总标识输入(bottom[1])。cont为0表示应该新开一个序列，不用再参照上一次的输出结果</li>\n</ul>\n<h3 id=\"成员函数\"><a href=\"#成员函数\" class=\"headerlink\" title=\"成员函数\"></a>成员函数</h3><ul>\n<li><strong>FillUnrolledNet(NetParameter* net_param):</strong>     <ul>\n<li>子类根据自己的内部网络修改net_param</li>\n<li>原理同我们在外面写prototxt生成网络</li>\n<li>类比安卓开发中动态生成按钮选项，都是因为数量经常变化，写死在xml文件中不便于改动</li>\n</ul>\n</li>\n<li><strong>xxxBlobNames:</strong> <ul>\n<li>返回与主控部分交互的blob名字，主控部分根据名字找到对应的blob</li>\n<li>类比安卓开发中findViewByID将控件和变量绑定</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"recurrent-layer-cpp\"><a href=\"#recurrent-layer-cpp\" class=\"headerlink\" title=\"recurrent_layer.cpp\"></a>recurrent_layer.cpp</h2><h3 id=\"Reshape\"><a href=\"#Reshape\" class=\"headerlink\" title=\"Reshape\"></a>Reshape</h3><p>适配blob，以及绑定blob，使得操作变量等价于修改bottom与top</p>\n<ol>\n<li><strong>Reshape:</strong> 将x_input_blob_, cont_input_blob_和x_static_input_blob_几个输入blob的shape适配bottom</li>\n<li><strong>Reshape:</strong> 将recur_input_blobs_适配子类的入口输出blob(即$h_0$)</li>\n<li><strong>Share:</strong> 将x_input_blob_, cont_input_blob_和x_static_input_blob_与bottom的data及diff进行绑定</li>\n<li><strong>Share:</strong> 将output_blobs_与top的data及diff进行绑定</li>\n</ol>\n<h3 id=\"LayerSetUp\"><a href=\"#LayerSetUp\" class=\"headerlink\" title=\"LayerSetUp\"></a>LayerSetUp</h3><p>等价于将prototxt转为代码，核心为：初始化net_param来生成一个Net</p>\n<ol>\n<li><strong>检查bottom是否规范</strong><ul>\n<li>bottom[0]: 各帧，各视频的数据 (T * N * …)</li>\n<li>bottom[1]: 各帧，各视频的标识，0为开始，1为继续 (T * N * 1)</li>\n<li>bottom[2]: 静态输入</li>\n</ul>\n</li>\n<li><strong>为网络接入总输入，并为各输入命名</strong><ul>\n<li>x: bottom[0]</li>\n<li>cont: bottom[1]</li>\n<li>x_static: bottom[2]</li>\n</ul>\n</li>\n<li><strong>进一步构建网络</strong>，调用FillUnrolledNet修改net_param</li>\n<li><p><strong>用net_param生成最终的网络</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">unrolled_net_.reset(<span class=\"keyword\">new</span> Net&lt;Dtype&gt;(net_param));</div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>将变量与网络对应部分绑定</strong></p>\n<ul>\n<li>在上一步中网络已经生成完毕，但为了操作方便，将要用到的网络部分跟变量绑定，以后直接用变量进行操作</li>\n<li>绑定的时候使用net-&gt;blob_by_name(“xxx”)，类似findViewById。找blob的key是名字，这就是为什么要有xxxBlobNames这样的函数</li>\n</ul>\n</li>\n<li><strong>设置辅助参数</strong><ul>\n<li>blobs_: 这个参数应该要记录本层所有的blob，但由于我们的网络定义不仅是prototxt，还有中途动态生成的部分，所以不能依赖caffe帮我们自动生成的blobs_，要手动将所有参数添加</li>\n<li>param_propagate_down_: 记录是否要bp</li>\n<li>recur_output_blobs_全部置0<strong>（不懂）</strong></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"rnn-layer-cpp\"><a href=\"#rnn-layer-cpp\" class=\"headerlink\" title=\"rnn_layer.cpp\"></a>rnn_layer.cpp</h2><h3 id=\"跟主控交接部分\"><a href=\"#跟主控交接部分\" class=\"headerlink\" title=\"跟主控交接部分\"></a>跟主控交接部分</h3><ul>\n<li><strong>循环输入入口:</strong> $h_0$，1 * N * num_output（循环输入跟循环输出shape一致，所以这里既说明了输入由说明了输出，即对于每一帧，生成num_output维的向量）</li>\n<li><strong>循环输出出口:</strong> $h_t$</li>\n<li><strong>总输出:</strong> o</li>\n</ul>\n<h3 id=\"FillUnrolledNet\"><a href=\"#FillUnrolledNet\" class=\"headerlink\" title=\"FillUnrolledNet\"></a>FillUnrolledNet</h3><p>核心部分，实现网络的定义。实际效果近似于这样的prototxt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\">################################</div><div class=\"line\">#            input             #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#sliced x</div><div class=\"line\">\tbottom: x</div><div class=\"line\">\ttop: x_1</div><div class=\"line\">\ttop: x_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\ttop: x_t</div><div class=\"line\">&#125;</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#sliced cont</div><div class=\"line\">\tbottom: cont</div><div class=\"line\">\ttop: cont_1</div><div class=\"line\">\ttop: cont_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\ttop: cont_t</div><div class=\"line\">&#125;</div><div class=\"line\">################################</div><div class=\"line\">#          recur layer         #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_1</div><div class=\"line\">\tbottom: x_1, cont_1, h_0</div><div class=\"line\">\ttop: o_1, h_1</div><div class=\"line\">&#125;</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_2</div><div class=\"line\">\tbottom: x_2, cont_2, h_1</div><div class=\"line\">\ttop: o_2, h_2</div><div class=\"line\">&#125;</div><div class=\"line\">\t\t.</div><div class=\"line\">\t\t.</div><div class=\"line\">\t\t.</div><div class=\"line\"></div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#recur_unit_t</div><div class=\"line\">\tbottom: x_t, cont_t, h_t-1</div><div class=\"line\">\ttop: o_t, h_t</div><div class=\"line\">&#125;</div><div class=\"line\">################################</div><div class=\"line\">#           output             #</div><div class=\"line\">################################</div><div class=\"line\">layer&#123;</div><div class=\"line\">\t#concated output</div><div class=\"line\">\tbottom: o_1</div><div class=\"line\">\tbottom: o_2</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\t.</div><div class=\"line\">\tbottom: o_t</div><div class=\"line\">\ttop: o</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>阅读源码前前思考代码的<strong>输入</strong>是什么，<strong>输出</strong>是什么，给你这样的任务<strong>你会怎么做</strong></li>\n<li>然后在阅读的时候合理地进行<strong>假设</strong>，一步步验证并修改</li>\n<li>先看<strong>数据结构</strong>再看算法，<strong>粒度</strong>从粗到细（类级，函数级，块级，行级）</li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li>论文地址：<a href=\"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\">s2vt</a></li>\n<li>项目地址：<a href=\"https://arxiv.org/abs/1505.00487\">Sequence to Sequence – Video to Text</a></li>\n<li>参考资料：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">LSTM</a>, <a href=\"http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-sequences.pdf\">Sequences in Caffe</a></li>\n</ul>\n"},{"title":"Caffe学习：s2vt数据处理部分源码阅读","date":"2016-07-22T03:32:32.000Z","description":["s2vt数据预处理部分源码，即txt转hdf5格式的python脚本"],"_content":"\n## 任务简介\ns2vt做的是从视频生成文字，输入端的数据量较于传统任务庞大很多，对数据流的输入速率提出了要求。传统的以txt方式保存数据的读取方式不再适用，转而使用了更大但是更快的hdf5格式。于是就需要实现两种功能，分别是将数据从**txt格式转换成hdf5格式的脚本**，以及能够**读取hdf5的数据输入层**。\n\n## 源码框架\n\n### txt转hdf5的python脚本\n\n- 采用了**模板方法**的设计模式\n- **hdf5\\_npstreamsequence\\_generator.py**: 其下定义了两个类，分别是\n\t* **SequenceGenerator: **txt转hdf5类的父类，定义了获取下一个batch内容的算法框架\n\t* **HDF5SequenceWriter: **I/O类，给定一个SequenceGenerator，将其内容输出到.h5文件\n- **framefc7\\_stream\\_text\\_to\\_hdf5\\_data.py**: 子类，定义了获取最小粒度数据的方法\n\n### 读取hdf5数据的输入层\n\n- **hdf5\\_data\\_layer.cpp**\n\n### 核心调用关系(从上往下调用)\n\n- **HDF5SequenceWriter::write\\_to\\_exhaustion**\n\t* 调用write\\_batch，直到所有数据读完\n- **HDF5SequenceWriter::write\\_batch**\n\t* **input:** get\\_next\\_batch获取的batch\n\t* **output:** 将batch内容输出为.h5文件\n- **SequenceGenerator::get\\_next\\_batch**\n\t* **intput:** 流数据\n\t* **output:** batch\\[Name]\\[T]\\[N]\n- **SequenceGenerator::reset\\_stream**\n\t* **input:** 最小粒度流数据\n\t* **output:** 重设batch中第i条流为下一条输入流\n- **fc7FrameSequenceGenerator::get_streams**\n\t* **input:** txt文件\n\t* **output:** 由下段line及其对应视频的frames所生成的数据\n\n## hdf5\\_npstreamsequence\\_generator.py\n\n### HDF5SequenceWriter::write\\_to\\_exhaustion\n\n不停地调用write\\_batch，直到所有输入的流（即line）都读完\n\n```python\ndef write_to_exhaustion(self):\n    while not self.generator.streams_exhausted():\n      self.write_batch(stop_at_exhaustion=True)\n```\n\n### HDF5SequenceWriter::write\\_batch\n\n将batch的内容以hdf5的格式保存为.h5文件，hdf5的基本操作流程如下（基本复用了NumPy的表示方式），详情参考[h5py.doc](http://docs.h5py.org/en/latest/high/dataset.html#creating-datasets)\n\n```python\nh5file = h5py.File(filename, 'w')\n# return the container\ndataset = h5file.create_dataset('cont', shape=cont_indicators.shape, dtype=cont_indicators.dtype)\n# write data intot the container\ndataset[:] = cont_indicators\nh5file.close()\n```\n\n### SequenceGenerator::get\\_next\\_batch\n\n变量解释：\n\n- **batch:** batch\\[Name]\\[T]\\[N]\n\t* 存放数据内容\n\t* **Name:** 是指framefc7, intput_sentence之类的\n\t* **T:** 是时间，注意到batch的T是1000，而每一个输入的T是80，所以说在相通的N下，T=80的流跟T=81的流不是同一个流，而是相隔了N的两个流\n- **batch\\_indicators:** batch\\_indicators\\[T]\\[N]\n\t* 指示对应流的开始和结束，0是开始，1是延续\n- **self.substream\\_names:** batch中Name维的值域，即framefc7, input_sentence\n- **self.array\\_type\\_inputs:** 类型是数组的name，比如framefc7\\[num_frames]\\[4096]\n- **exhausted:** vector<bool>(N)\n\t* 指示batch中第N个流是否已经结束（80的倍数或者轮到第N个流时输入已经结束）\n- **all\\_exhausted:** true if all exahusted are true\n- **reached\\_exhaustion:** 基本同streams_exhausted()\n- **self.stream\\_indices:** 标识当前第N个流读到T几\n\n过程解释：\n\n初始化batch和batch\\_indicator的shape，以及各种辅助变量\n```python\nif not self.streams_initialized:\n\tself.init_streams()\n# format: len0: [s0, s2, num of streams, s_n]\n#         len1: [s0, s2, num of streams, s_n]\n#         len2: [s0, s2, num of streams, s_n]\nbatch_size = self.batch_num_streams * self.batch_stream_length\nbatch = {}\nbatch_indicators = np.zeros((self.batch_stream_length, self.batch_num_streams))\n# reshape batch[name] like batch_indicators\n# and set value to pad value\nfor name in self.substream_names:\n\t# if value is high dimension\n\tif name in self.array_type_inputs.keys():\n    \tdim = self.array_type_inputs[name]\n        batch[name] = self.get_pad_value(name) * np.ones((self.batch_stream_length, self.batch_num_streams, dim))\n\t# if value is 1d\n\telse:\n        # each batch[name] is a T * N * dim blob\n        batch[name] = self.get_pad_value(name) * np.ones_like(batch_indicators)\n```\n假如第i个流从来没有用过或者上一个位于i位置的流已经读完，就reset\\_stream(i)\n```python\n# never been initialized or come to the end of a stream\nif self.streams[i] is None or \\\n\tself.stream_indices[i] == len(self.streams[i][self.substream_names[0]]): \n\tself.stream_indices[i] = 0\n\t# Q: self.streams_exhausted() always return false, so the expression is meaningless?\n\t# A: derived class will override function streams_exhausted\n\t# reached_exhaustion is forever True after pass through all lines\n\treached_exhaustion = reached_exhaustion or self.streams_exhausted()\n\t# exhausted[i] indicates the end of ith stream i.e. all lines in ith stream are read\n\tif reached_exhaustion: exhausted[i] = True\n\t# Q: why reset stream i? self.streams is the same data for all stream i\n\t# A: get_streams() in reset_stream() is wrapped around\n\tif not reached_exhaustion or not truncate_at_exhaustion:\n\t\tself.reset_stream(i)\n\telse:\n\t\tcontinue\n```\n\n将各个name的t, i写到对应的batch\\[name]\\[t][i]\n```python\nfor name in self.substream_names:\n    if isinstance(self.streams[i][name], np.ndarray) and self.streams[i][name].ndim > 1:\n        batch[name].resize((batch_size, self.streams[i][name].shape[1],1))\n        batch[name][(t*self.batch_num_streams + i), :,0] = self.streams[i][name][self.stream_indices[i],:]\n    elif name in self.array_type_inputs.keys():\n        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]][0,:]\n    else:\n        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]]\n```\n\n### SequenceGenerator::reset\\_stream\n\n1. 通过get_streams()得到下一个数据流（即下一个line对应的input, framefc7 ...)\n2. 修改实例变量streams[stream_index]为下一个数据流\n\n## framefc7\\_stream\\_text\\_to\\_hdf5\\_data.py\n\n### fc7FrameSequenceGenerator::\\__init__\n\n从txt中读取数据并将数据存入以下变量\n\n- self.vid\\_framefeats[video\\_id]: 存放video_id对应的frames(frame1, frame2)的feats(4096)\n- self.lines: pair< vid, line >\n\n### fc7FrameSequenceGenerator::get_streams\n\n将下一条line对应的frames feats及其他数据规范化为MAX_WORD长度的out，out的示意如下\n\n```\n\t\t     MAX_WORD\ncont_sentence\tx x x x ... x x x x\ninput_sentence  x x x x ... x x x x \nframe_fc7\tx x x x ... x x x x\n\t\t| | | |     | | | |  \n\t\t| | | |     | | | |  4096\n\n```\n\n## 收获\n\n- 在看源码前大概**交流**一下各个函数是干嘛用的，把握整体思路\n- 在纸上画出**核心函数调用链**\n- 像python这样的弱类型语言，可以看**被调函数返回数据**的数据结构\n\n## resource\n- [含注释的hdf5\\_npstreamsequence\\_generator.py](https://github.com/meltycriss/commented_src/blob/master/s2vt_data/hdf5_npstreamsequence_generator.py)\n\n## reference\n\n- 项目地址：[Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487)\n\n","source":"_posts/caffe_3_s2vt_data_process.md","raw":"---\ntitle: Caffe学习：s2vt数据处理部分源码阅读\ndate: 2016-07-22 11:32:32\ntags: \n  - s2vt data\ndescription:\n  - s2vt数据预处理部分源码，即txt转hdf5格式的python脚本\ncategories:\n  - caffe\n---\n\n## 任务简介\ns2vt做的是从视频生成文字，输入端的数据量较于传统任务庞大很多，对数据流的输入速率提出了要求。传统的以txt方式保存数据的读取方式不再适用，转而使用了更大但是更快的hdf5格式。于是就需要实现两种功能，分别是将数据从**txt格式转换成hdf5格式的脚本**，以及能够**读取hdf5的数据输入层**。\n\n## 源码框架\n\n### txt转hdf5的python脚本\n\n- 采用了**模板方法**的设计模式\n- **hdf5\\_npstreamsequence\\_generator.py**: 其下定义了两个类，分别是\n\t* **SequenceGenerator: **txt转hdf5类的父类，定义了获取下一个batch内容的算法框架\n\t* **HDF5SequenceWriter: **I/O类，给定一个SequenceGenerator，将其内容输出到.h5文件\n- **framefc7\\_stream\\_text\\_to\\_hdf5\\_data.py**: 子类，定义了获取最小粒度数据的方法\n\n### 读取hdf5数据的输入层\n\n- **hdf5\\_data\\_layer.cpp**\n\n### 核心调用关系(从上往下调用)\n\n- **HDF5SequenceWriter::write\\_to\\_exhaustion**\n\t* 调用write\\_batch，直到所有数据读完\n- **HDF5SequenceWriter::write\\_batch**\n\t* **input:** get\\_next\\_batch获取的batch\n\t* **output:** 将batch内容输出为.h5文件\n- **SequenceGenerator::get\\_next\\_batch**\n\t* **intput:** 流数据\n\t* **output:** batch\\[Name]\\[T]\\[N]\n- **SequenceGenerator::reset\\_stream**\n\t* **input:** 最小粒度流数据\n\t* **output:** 重设batch中第i条流为下一条输入流\n- **fc7FrameSequenceGenerator::get_streams**\n\t* **input:** txt文件\n\t* **output:** 由下段line及其对应视频的frames所生成的数据\n\n## hdf5\\_npstreamsequence\\_generator.py\n\n### HDF5SequenceWriter::write\\_to\\_exhaustion\n\n不停地调用write\\_batch，直到所有输入的流（即line）都读完\n\n```python\ndef write_to_exhaustion(self):\n    while not self.generator.streams_exhausted():\n      self.write_batch(stop_at_exhaustion=True)\n```\n\n### HDF5SequenceWriter::write\\_batch\n\n将batch的内容以hdf5的格式保存为.h5文件，hdf5的基本操作流程如下（基本复用了NumPy的表示方式），详情参考[h5py.doc](http://docs.h5py.org/en/latest/high/dataset.html#creating-datasets)\n\n```python\nh5file = h5py.File(filename, 'w')\n# return the container\ndataset = h5file.create_dataset('cont', shape=cont_indicators.shape, dtype=cont_indicators.dtype)\n# write data intot the container\ndataset[:] = cont_indicators\nh5file.close()\n```\n\n### SequenceGenerator::get\\_next\\_batch\n\n变量解释：\n\n- **batch:** batch\\[Name]\\[T]\\[N]\n\t* 存放数据内容\n\t* **Name:** 是指framefc7, intput_sentence之类的\n\t* **T:** 是时间，注意到batch的T是1000，而每一个输入的T是80，所以说在相通的N下，T=80的流跟T=81的流不是同一个流，而是相隔了N的两个流\n- **batch\\_indicators:** batch\\_indicators\\[T]\\[N]\n\t* 指示对应流的开始和结束，0是开始，1是延续\n- **self.substream\\_names:** batch中Name维的值域，即framefc7, input_sentence\n- **self.array\\_type\\_inputs:** 类型是数组的name，比如framefc7\\[num_frames]\\[4096]\n- **exhausted:** vector<bool>(N)\n\t* 指示batch中第N个流是否已经结束（80的倍数或者轮到第N个流时输入已经结束）\n- **all\\_exhausted:** true if all exahusted are true\n- **reached\\_exhaustion:** 基本同streams_exhausted()\n- **self.stream\\_indices:** 标识当前第N个流读到T几\n\n过程解释：\n\n初始化batch和batch\\_indicator的shape，以及各种辅助变量\n```python\nif not self.streams_initialized:\n\tself.init_streams()\n# format: len0: [s0, s2, num of streams, s_n]\n#         len1: [s0, s2, num of streams, s_n]\n#         len2: [s0, s2, num of streams, s_n]\nbatch_size = self.batch_num_streams * self.batch_stream_length\nbatch = {}\nbatch_indicators = np.zeros((self.batch_stream_length, self.batch_num_streams))\n# reshape batch[name] like batch_indicators\n# and set value to pad value\nfor name in self.substream_names:\n\t# if value is high dimension\n\tif name in self.array_type_inputs.keys():\n    \tdim = self.array_type_inputs[name]\n        batch[name] = self.get_pad_value(name) * np.ones((self.batch_stream_length, self.batch_num_streams, dim))\n\t# if value is 1d\n\telse:\n        # each batch[name] is a T * N * dim blob\n        batch[name] = self.get_pad_value(name) * np.ones_like(batch_indicators)\n```\n假如第i个流从来没有用过或者上一个位于i位置的流已经读完，就reset\\_stream(i)\n```python\n# never been initialized or come to the end of a stream\nif self.streams[i] is None or \\\n\tself.stream_indices[i] == len(self.streams[i][self.substream_names[0]]): \n\tself.stream_indices[i] = 0\n\t# Q: self.streams_exhausted() always return false, so the expression is meaningless?\n\t# A: derived class will override function streams_exhausted\n\t# reached_exhaustion is forever True after pass through all lines\n\treached_exhaustion = reached_exhaustion or self.streams_exhausted()\n\t# exhausted[i] indicates the end of ith stream i.e. all lines in ith stream are read\n\tif reached_exhaustion: exhausted[i] = True\n\t# Q: why reset stream i? self.streams is the same data for all stream i\n\t# A: get_streams() in reset_stream() is wrapped around\n\tif not reached_exhaustion or not truncate_at_exhaustion:\n\t\tself.reset_stream(i)\n\telse:\n\t\tcontinue\n```\n\n将各个name的t, i写到对应的batch\\[name]\\[t][i]\n```python\nfor name in self.substream_names:\n    if isinstance(self.streams[i][name], np.ndarray) and self.streams[i][name].ndim > 1:\n        batch[name].resize((batch_size, self.streams[i][name].shape[1],1))\n        batch[name][(t*self.batch_num_streams + i), :,0] = self.streams[i][name][self.stream_indices[i],:]\n    elif name in self.array_type_inputs.keys():\n        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]][0,:]\n    else:\n        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]]\n```\n\n### SequenceGenerator::reset\\_stream\n\n1. 通过get_streams()得到下一个数据流（即下一个line对应的input, framefc7 ...)\n2. 修改实例变量streams[stream_index]为下一个数据流\n\n## framefc7\\_stream\\_text\\_to\\_hdf5\\_data.py\n\n### fc7FrameSequenceGenerator::\\__init__\n\n从txt中读取数据并将数据存入以下变量\n\n- self.vid\\_framefeats[video\\_id]: 存放video_id对应的frames(frame1, frame2)的feats(4096)\n- self.lines: pair< vid, line >\n\n### fc7FrameSequenceGenerator::get_streams\n\n将下一条line对应的frames feats及其他数据规范化为MAX_WORD长度的out，out的示意如下\n\n```\n\t\t     MAX_WORD\ncont_sentence\tx x x x ... x x x x\ninput_sentence  x x x x ... x x x x \nframe_fc7\tx x x x ... x x x x\n\t\t| | | |     | | | |  \n\t\t| | | |     | | | |  4096\n\n```\n\n## 收获\n\n- 在看源码前大概**交流**一下各个函数是干嘛用的，把握整体思路\n- 在纸上画出**核心函数调用链**\n- 像python这样的弱类型语言，可以看**被调函数返回数据**的数据结构\n\n## resource\n- [含注释的hdf5\\_npstreamsequence\\_generator.py](https://github.com/meltycriss/commented_src/blob/master/s2vt_data/hdf5_npstreamsequence_generator.py)\n\n## reference\n\n- 项目地址：[Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487)\n\n","slug":"caffe_3_s2vt_data_process","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrp90005xuvt5j9j539y","content":"<h2 id=\"任务简介\"><a href=\"#任务简介\" class=\"headerlink\" title=\"任务简介\"></a>任务简介</h2><p>s2vt做的是从视频生成文字，输入端的数据量较于传统任务庞大很多，对数据流的输入速率提出了要求。传统的以txt方式保存数据的读取方式不再适用，转而使用了更大但是更快的hdf5格式。于是就需要实现两种功能，分别是将数据从<strong>txt格式转换成hdf5格式的脚本</strong>，以及能够<strong>读取hdf5的数据输入层</strong>。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"txt转hdf5的python脚本\"><a href=\"#txt转hdf5的python脚本\" class=\"headerlink\" title=\"txt转hdf5的python脚本\"></a>txt转hdf5的python脚本</h3><ul>\n<li>采用了<strong>模板方法</strong>的设计模式</li>\n<li><strong>hdf5_npstreamsequence_generator.py</strong>: 其下定义了两个类，分别是<ul>\n<li><strong>SequenceGenerator: </strong>txt转hdf5类的父类，定义了获取下一个batch内容的算法框架</li>\n<li><strong>HDF5SequenceWriter: </strong>I/O类，给定一个SequenceGenerator，将其内容输出到.h5文件</li>\n</ul>\n</li>\n<li><strong>framefc7_stream_text_to_hdf5_data.py</strong>: 子类，定义了获取最小粒度数据的方法</li>\n</ul>\n<h3 id=\"读取hdf5数据的输入层\"><a href=\"#读取hdf5数据的输入层\" class=\"headerlink\" title=\"读取hdf5数据的输入层\"></a>读取hdf5数据的输入层</h3><ul>\n<li><strong>hdf5_data_layer.cpp</strong></li>\n</ul>\n<h3 id=\"核心调用关系-从上往下调用\"><a href=\"#核心调用关系-从上往下调用\" class=\"headerlink\" title=\"核心调用关系(从上往下调用)\"></a>核心调用关系(从上往下调用)</h3><ul>\n<li><strong>HDF5SequenceWriter::write_to_exhaustion</strong><ul>\n<li>调用write_batch，直到所有数据读完</li>\n</ul>\n</li>\n<li><strong>HDF5SequenceWriter::write_batch</strong><ul>\n<li><strong>input:</strong> get_next_batch获取的batch</li>\n<li><strong>output:</strong> 将batch内容输出为.h5文件</li>\n</ul>\n</li>\n<li><strong>SequenceGenerator::get_next_batch</strong><ul>\n<li><strong>intput:</strong> 流数据</li>\n<li><strong>output:</strong> batch[Name][T][N]</li>\n</ul>\n</li>\n<li><strong>SequenceGenerator::reset_stream</strong><ul>\n<li><strong>input:</strong> 最小粒度流数据</li>\n<li><strong>output:</strong> 重设batch中第i条流为下一条输入流</li>\n</ul>\n</li>\n<li><strong>fc7FrameSequenceGenerator::get_streams</strong><ul>\n<li><strong>input:</strong> txt文件</li>\n<li><strong>output:</strong> 由下段line及其对应视频的frames所生成的数据</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"hdf5-npstreamsequence-generator-py\"><a href=\"#hdf5-npstreamsequence-generator-py\" class=\"headerlink\" title=\"hdf5_npstreamsequence_generator.py\"></a>hdf5_npstreamsequence_generator.py</h2><h3 id=\"HDF5SequenceWriter-write-to-exhaustion\"><a href=\"#HDF5SequenceWriter-write-to-exhaustion\" class=\"headerlink\" title=\"HDF5SequenceWriter::write_to_exhaustion\"></a>HDF5SequenceWriter::write_to_exhaustion</h3><p>不停地调用write_batch，直到所有输入的流（即line）都读完</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write_to_exhaustion</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> self.generator.streams_exhausted():</div><div class=\"line\">      self.write_batch(stop_at_exhaustion=<span class=\"keyword\">True</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"HDF5SequenceWriter-write-batch\"><a href=\"#HDF5SequenceWriter-write-batch\" class=\"headerlink\" title=\"HDF5SequenceWriter::write_batch\"></a>HDF5SequenceWriter::write_batch</h3><p>将batch的内容以hdf5的格式保存为.h5文件，hdf5的基本操作流程如下（基本复用了NumPy的表示方式），详情参考<a href=\"http://docs.h5py.org/en/latest/high/dataset.html#creating-datasets\" target=\"_blank\" rel=\"external\">h5py.doc</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">h5file = h5py.File(filename, <span class=\"string\">'w'</span>)</div><div class=\"line\"><span class=\"comment\"># return the container</span></div><div class=\"line\">dataset = h5file.create_dataset(<span class=\"string\">'cont'</span>, shape=cont_indicators.shape, dtype=cont_indicators.dtype)</div><div class=\"line\"><span class=\"comment\"># write data intot the container</span></div><div class=\"line\">dataset[:] = cont_indicators</div><div class=\"line\">h5file.close()</div></pre></td></tr></table></figure>\n<h3 id=\"SequenceGenerator-get-next-batch\"><a href=\"#SequenceGenerator-get-next-batch\" class=\"headerlink\" title=\"SequenceGenerator::get_next_batch\"></a>SequenceGenerator::get_next_batch</h3><p>变量解释：</p>\n<ul>\n<li><strong>batch:</strong> batch[Name][T][N]<ul>\n<li>存放数据内容</li>\n<li><strong>Name:</strong> 是指framefc7, intput_sentence之类的</li>\n<li><strong>T:</strong> 是时间，注意到batch的T是1000，而每一个输入的T是80，所以说在相通的N下，T=80的流跟T=81的流不是同一个流，而是相隔了N的两个流</li>\n</ul>\n</li>\n<li><strong>batch_indicators:</strong> batch_indicators[T][N]<ul>\n<li>指示对应流的开始和结束，0是开始，1是延续</li>\n</ul>\n</li>\n<li><strong>self.substream_names:</strong> batch中Name维的值域，即framefc7, input_sentence</li>\n<li><strong>self.array_type_inputs:</strong> 类型是数组的name，比如framefc7[num_frames][4096]</li>\n<li><strong>exhausted:</strong> vector<bool>(N)<ul>\n<li>指示batch中第N个流是否已经结束（80的倍数或者轮到第N个流时输入已经结束）</li>\n</ul>\n</bool></li>\n<li><strong>all_exhausted:</strong> true if all exahusted are true</li>\n<li><strong>reached_exhaustion:</strong> 基本同streams_exhausted()</li>\n<li><strong>self.stream_indices:</strong> 标识当前第N个流读到T几</li>\n</ul>\n<p>过程解释：</p>\n<p>初始化batch和batch_indicator的shape，以及各种辅助变量<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.streams_initialized:</div><div class=\"line\">\tself.init_streams()</div><div class=\"line\"><span class=\"comment\"># format: len0: [s0, s2, num of streams, s_n]</span></div><div class=\"line\"><span class=\"comment\">#         len1: [s0, s2, num of streams, s_n]</span></div><div class=\"line\"><span class=\"comment\">#         len2: [s0, s2, num of streams, s_n]</span></div><div class=\"line\">batch_size = self.batch_num_streams * self.batch_stream_length</div><div class=\"line\">batch = &#123;&#125;</div><div class=\"line\">batch_indicators = np.zeros((self.batch_stream_length, self.batch_num_streams))</div><div class=\"line\"><span class=\"comment\"># reshape batch[name] like batch_indicators</span></div><div class=\"line\"><span class=\"comment\"># and set value to pad value</span></div><div class=\"line\"><span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> self.substream_names:</div><div class=\"line\">\t<span class=\"comment\"># if value is high dimension</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> name <span class=\"keyword\">in</span> self.array_type_inputs.keys():</div><div class=\"line\">    \tdim = self.array_type_inputs[name]</div><div class=\"line\">        batch[name] = self.get_pad_value(name) * np.ones((self.batch_stream_length, self.batch_num_streams, dim))</div><div class=\"line\">\t<span class=\"comment\"># if value is 1d</span></div><div class=\"line\">\t<span class=\"keyword\">else</span>:</div><div class=\"line\">        <span class=\"comment\"># each batch[name] is a T * N * dim blob</span></div><div class=\"line\">        batch[name] = self.get_pad_value(name) * np.ones_like(batch_indicators)</div></pre></td></tr></table></figure></p>\n<p>假如第i个流从来没有用过或者上一个位于i位置的流已经读完，就reset_stream(i)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># never been initialized or come to the end of a stream</span></div><div class=\"line\"><span class=\"keyword\">if</span> self.streams[i] <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> \\</div><div class=\"line\">\tself.stream_indices[i] == len(self.streams[i][self.substream_names[<span class=\"number\">0</span>]]): </div><div class=\"line\">\tself.stream_indices[i] = <span class=\"number\">0</span></div><div class=\"line\">\t<span class=\"comment\"># Q: self.streams_exhausted() always return false, so the expression is meaningless?</span></div><div class=\"line\">\t<span class=\"comment\"># A: derived class will override function streams_exhausted</span></div><div class=\"line\">\t<span class=\"comment\"># reached_exhaustion is forever True after pass through all lines</span></div><div class=\"line\">\treached_exhaustion = reached_exhaustion <span class=\"keyword\">or</span> self.streams_exhausted()</div><div class=\"line\">\t<span class=\"comment\"># exhausted[i] indicates the end of ith stream i.e. all lines in ith stream are read</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> reached_exhaustion: exhausted[i] = <span class=\"keyword\">True</span></div><div class=\"line\">\t<span class=\"comment\"># Q: why reset stream i? self.streams is the same data for all stream i</span></div><div class=\"line\">\t<span class=\"comment\"># A: get_streams() in reset_stream() is wrapped around</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> reached_exhaustion <span class=\"keyword\">or</span> <span class=\"keyword\">not</span> truncate_at_exhaustion:</div><div class=\"line\">\t\tself.reset_stream(i)</div><div class=\"line\">\t<span class=\"keyword\">else</span>:</div><div class=\"line\">\t\t<span class=\"keyword\">continue</span></div></pre></td></tr></table></figure></p>\n<p>将各个name的t, i写到对应的batch[name][t][i]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> self.substream_names:</div><div class=\"line\">    <span class=\"keyword\">if</span> isinstance(self.streams[i][name], np.ndarray) <span class=\"keyword\">and</span> self.streams[i][name].ndim &gt; <span class=\"number\">1</span>:</div><div class=\"line\">        batch[name].resize((batch_size, self.streams[i][name].shape[<span class=\"number\">1</span>],<span class=\"number\">1</span>))</div><div class=\"line\">        batch[name][(t*self.batch_num_streams + i), :,<span class=\"number\">0</span>] = self.streams[i][name][self.stream_indices[i],:]</div><div class=\"line\">    <span class=\"keyword\">elif</span> name <span class=\"keyword\">in</span> self.array_type_inputs.keys():</div><div class=\"line\">        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]][<span class=\"number\">0</span>,:]</div><div class=\"line\">    <span class=\"keyword\">else</span>:</div><div class=\"line\">        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]]</div></pre></td></tr></table></figure></p>\n<h3 id=\"SequenceGenerator-reset-stream\"><a href=\"#SequenceGenerator-reset-stream\" class=\"headerlink\" title=\"SequenceGenerator::reset_stream\"></a>SequenceGenerator::reset_stream</h3><ol>\n<li>通过get_streams()得到下一个数据流（即下一个line对应的input, framefc7 …)</li>\n<li>修改实例变量streams[stream_index]为下一个数据流</li>\n</ol>\n<h2 id=\"framefc7-stream-text-to-hdf5-data-py\"><a href=\"#framefc7-stream-text-to-hdf5-data-py\" class=\"headerlink\" title=\"framefc7_stream_text_to_hdf5_data.py\"></a>framefc7_stream_text_to_hdf5_data.py</h2><h3 id=\"fc7FrameSequenceGenerator-init\"><a href=\"#fc7FrameSequenceGenerator-init\" class=\"headerlink\" title=\"fc7FrameSequenceGenerator::__init__\"></a>fc7FrameSequenceGenerator::__init__</h3><p>从txt中读取数据并将数据存入以下变量</p>\n<ul>\n<li>self.vid_framefeats[video_id]: 存放video_id对应的frames(frame1, frame2)的feats(4096)</li>\n<li>self.lines: pair&lt; vid, line &gt;</li>\n</ul>\n<h3 id=\"fc7FrameSequenceGenerator-get-streams\"><a href=\"#fc7FrameSequenceGenerator-get-streams\" class=\"headerlink\" title=\"fc7FrameSequenceGenerator::get_streams\"></a>fc7FrameSequenceGenerator::get_streams</h3><p>将下一条line对应的frames feats及其他数据规范化为MAX_WORD长度的out，out的示意如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">\t\t     MAX_WORD</div><div class=\"line\">cont_sentence\tx x x x ... x x x x</div><div class=\"line\">input_sentence  x x x x ... x x x x </div><div class=\"line\">frame_fc7\tx x x x ... x x x x</div><div class=\"line\">\t\t| | | |     | | | |  </div><div class=\"line\">\t\t| | | |     | | | |  4096</div></pre></td></tr></table></figure>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>在看源码前大概<strong>交流</strong>一下各个函数是干嘛用的，把握整体思路</li>\n<li>在纸上画出<strong>核心函数调用链</strong></li>\n<li>像python这样的弱类型语言，可以看<strong>被调函数返回数据</strong>的数据结构</li>\n</ul>\n<h2 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h2><ul>\n<li><a href=\"https://github.com/meltycriss/commented_src/blob/master/s2vt_data/hdf5_npstreamsequence_generator.py\" target=\"_blank\" rel=\"external\">含注释的hdf5_npstreamsequence_generator.py</a></li>\n</ul>\n<h2 id=\"reference\"><a href=\"#reference\" class=\"headerlink\" title=\"reference\"></a>reference</h2><ul>\n<li>项目地址：<a href=\"https://arxiv.org/abs/1505.00487\" target=\"_blank\" rel=\"external\">Sequence to Sequence – Video to Text</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"任务简介\"><a href=\"#任务简介\" class=\"headerlink\" title=\"任务简介\"></a>任务简介</h2><p>s2vt做的是从视频生成文字，输入端的数据量较于传统任务庞大很多，对数据流的输入速率提出了要求。传统的以txt方式保存数据的读取方式不再适用，转而使用了更大但是更快的hdf5格式。于是就需要实现两种功能，分别是将数据从<strong>txt格式转换成hdf5格式的脚本</strong>，以及能够<strong>读取hdf5的数据输入层</strong>。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"txt转hdf5的python脚本\"><a href=\"#txt转hdf5的python脚本\" class=\"headerlink\" title=\"txt转hdf5的python脚本\"></a>txt转hdf5的python脚本</h3><ul>\n<li>采用了<strong>模板方法</strong>的设计模式</li>\n<li><strong>hdf5_npstreamsequence_generator.py</strong>: 其下定义了两个类，分别是<ul>\n<li><strong>SequenceGenerator: </strong>txt转hdf5类的父类，定义了获取下一个batch内容的算法框架</li>\n<li><strong>HDF5SequenceWriter: </strong>I/O类，给定一个SequenceGenerator，将其内容输出到.h5文件</li>\n</ul>\n</li>\n<li><strong>framefc7_stream_text_to_hdf5_data.py</strong>: 子类，定义了获取最小粒度数据的方法</li>\n</ul>\n<h3 id=\"读取hdf5数据的输入层\"><a href=\"#读取hdf5数据的输入层\" class=\"headerlink\" title=\"读取hdf5数据的输入层\"></a>读取hdf5数据的输入层</h3><ul>\n<li><strong>hdf5_data_layer.cpp</strong></li>\n</ul>\n<h3 id=\"核心调用关系-从上往下调用\"><a href=\"#核心调用关系-从上往下调用\" class=\"headerlink\" title=\"核心调用关系(从上往下调用)\"></a>核心调用关系(从上往下调用)</h3><ul>\n<li><strong>HDF5SequenceWriter::write_to_exhaustion</strong><ul>\n<li>调用write_batch，直到所有数据读完</li>\n</ul>\n</li>\n<li><strong>HDF5SequenceWriter::write_batch</strong><ul>\n<li><strong>input:</strong> get_next_batch获取的batch</li>\n<li><strong>output:</strong> 将batch内容输出为.h5文件</li>\n</ul>\n</li>\n<li><strong>SequenceGenerator::get_next_batch</strong><ul>\n<li><strong>intput:</strong> 流数据</li>\n<li><strong>output:</strong> batch[Name][T][N]</li>\n</ul>\n</li>\n<li><strong>SequenceGenerator::reset_stream</strong><ul>\n<li><strong>input:</strong> 最小粒度流数据</li>\n<li><strong>output:</strong> 重设batch中第i条流为下一条输入流</li>\n</ul>\n</li>\n<li><strong>fc7FrameSequenceGenerator::get_streams</strong><ul>\n<li><strong>input:</strong> txt文件</li>\n<li><strong>output:</strong> 由下段line及其对应视频的frames所生成的数据</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"hdf5-npstreamsequence-generator-py\"><a href=\"#hdf5-npstreamsequence-generator-py\" class=\"headerlink\" title=\"hdf5_npstreamsequence_generator.py\"></a>hdf5_npstreamsequence_generator.py</h2><h3 id=\"HDF5SequenceWriter-write-to-exhaustion\"><a href=\"#HDF5SequenceWriter-write-to-exhaustion\" class=\"headerlink\" title=\"HDF5SequenceWriter::write_to_exhaustion\"></a>HDF5SequenceWriter::write_to_exhaustion</h3><p>不停地调用write_batch，直到所有输入的流（即line）都读完</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write_to_exhaustion</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> self.generator.streams_exhausted():</div><div class=\"line\">      self.write_batch(stop_at_exhaustion=<span class=\"keyword\">True</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"HDF5SequenceWriter-write-batch\"><a href=\"#HDF5SequenceWriter-write-batch\" class=\"headerlink\" title=\"HDF5SequenceWriter::write_batch\"></a>HDF5SequenceWriter::write_batch</h3><p>将batch的内容以hdf5的格式保存为.h5文件，hdf5的基本操作流程如下（基本复用了NumPy的表示方式），详情参考<a href=\"http://docs.h5py.org/en/latest/high/dataset.html#creating-datasets\">h5py.doc</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">h5file = h5py.File(filename, <span class=\"string\">'w'</span>)</div><div class=\"line\"><span class=\"comment\"># return the container</span></div><div class=\"line\">dataset = h5file.create_dataset(<span class=\"string\">'cont'</span>, shape=cont_indicators.shape, dtype=cont_indicators.dtype)</div><div class=\"line\"><span class=\"comment\"># write data intot the container</span></div><div class=\"line\">dataset[:] = cont_indicators</div><div class=\"line\">h5file.close()</div></pre></td></tr></table></figure>\n<h3 id=\"SequenceGenerator-get-next-batch\"><a href=\"#SequenceGenerator-get-next-batch\" class=\"headerlink\" title=\"SequenceGenerator::get_next_batch\"></a>SequenceGenerator::get_next_batch</h3><p>变量解释：</p>\n<ul>\n<li><strong>batch:</strong> batch[Name][T][N]<ul>\n<li>存放数据内容</li>\n<li><strong>Name:</strong> 是指framefc7, intput_sentence之类的</li>\n<li><strong>T:</strong> 是时间，注意到batch的T是1000，而每一个输入的T是80，所以说在相通的N下，T=80的流跟T=81的流不是同一个流，而是相隔了N的两个流</li>\n</ul>\n</li>\n<li><strong>batch_indicators:</strong> batch_indicators[T][N]<ul>\n<li>指示对应流的开始和结束，0是开始，1是延续</li>\n</ul>\n</li>\n<li><strong>self.substream_names:</strong> batch中Name维的值域，即framefc7, input_sentence</li>\n<li><strong>self.array_type_inputs:</strong> 类型是数组的name，比如framefc7[num_frames][4096]</li>\n<li><strong>exhausted:</strong> vector<bool>(N)<ul>\n<li>指示batch中第N个流是否已经结束（80的倍数或者轮到第N个流时输入已经结束）</li>\n</ul>\n</li>\n<li><strong>all_exhausted:</strong> true if all exahusted are true</li>\n<li><strong>reached_exhaustion:</strong> 基本同streams_exhausted()</li>\n<li><strong>self.stream_indices:</strong> 标识当前第N个流读到T几</li>\n</ul>\n<p>过程解释：</p>\n<p>初始化batch和batch_indicator的shape，以及各种辅助变量<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.streams_initialized:</div><div class=\"line\">\tself.init_streams()</div><div class=\"line\"><span class=\"comment\"># format: len0: [s0, s2, num of streams, s_n]</span></div><div class=\"line\"><span class=\"comment\">#         len1: [s0, s2, num of streams, s_n]</span></div><div class=\"line\"><span class=\"comment\">#         len2: [s0, s2, num of streams, s_n]</span></div><div class=\"line\">batch_size = self.batch_num_streams * self.batch_stream_length</div><div class=\"line\">batch = &#123;&#125;</div><div class=\"line\">batch_indicators = np.zeros((self.batch_stream_length, self.batch_num_streams))</div><div class=\"line\"><span class=\"comment\"># reshape batch[name] like batch_indicators</span></div><div class=\"line\"><span class=\"comment\"># and set value to pad value</span></div><div class=\"line\"><span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> self.substream_names:</div><div class=\"line\">\t<span class=\"comment\"># if value is high dimension</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> name <span class=\"keyword\">in</span> self.array_type_inputs.keys():</div><div class=\"line\">    \tdim = self.array_type_inputs[name]</div><div class=\"line\">        batch[name] = self.get_pad_value(name) * np.ones((self.batch_stream_length, self.batch_num_streams, dim))</div><div class=\"line\">\t<span class=\"comment\"># if value is 1d</span></div><div class=\"line\">\t<span class=\"keyword\">else</span>:</div><div class=\"line\">        <span class=\"comment\"># each batch[name] is a T * N * dim blob</span></div><div class=\"line\">        batch[name] = self.get_pad_value(name) * np.ones_like(batch_indicators)</div></pre></td></tr></table></figure></p>\n<p>假如第i个流从来没有用过或者上一个位于i位置的流已经读完，就reset_stream(i)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># never been initialized or come to the end of a stream</span></div><div class=\"line\"><span class=\"keyword\">if</span> self.streams[i] <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> \\</div><div class=\"line\">\tself.stream_indices[i] == len(self.streams[i][self.substream_names[<span class=\"number\">0</span>]]): </div><div class=\"line\">\tself.stream_indices[i] = <span class=\"number\">0</span></div><div class=\"line\">\t<span class=\"comment\"># Q: self.streams_exhausted() always return false, so the expression is meaningless?</span></div><div class=\"line\">\t<span class=\"comment\"># A: derived class will override function streams_exhausted</span></div><div class=\"line\">\t<span class=\"comment\"># reached_exhaustion is forever True after pass through all lines</span></div><div class=\"line\">\treached_exhaustion = reached_exhaustion <span class=\"keyword\">or</span> self.streams_exhausted()</div><div class=\"line\">\t<span class=\"comment\"># exhausted[i] indicates the end of ith stream i.e. all lines in ith stream are read</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> reached_exhaustion: exhausted[i] = <span class=\"keyword\">True</span></div><div class=\"line\">\t<span class=\"comment\"># Q: why reset stream i? self.streams is the same data for all stream i</span></div><div class=\"line\">\t<span class=\"comment\"># A: get_streams() in reset_stream() is wrapped around</span></div><div class=\"line\">\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> reached_exhaustion <span class=\"keyword\">or</span> <span class=\"keyword\">not</span> truncate_at_exhaustion:</div><div class=\"line\">\t\tself.reset_stream(i)</div><div class=\"line\">\t<span class=\"keyword\">else</span>:</div><div class=\"line\">\t\t<span class=\"keyword\">continue</span></div></pre></td></tr></table></figure></p>\n<p>将各个name的t, i写到对应的batch[name][t][i]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> self.substream_names:</div><div class=\"line\">    <span class=\"keyword\">if</span> isinstance(self.streams[i][name], np.ndarray) <span class=\"keyword\">and</span> self.streams[i][name].ndim &gt; <span class=\"number\">1</span>:</div><div class=\"line\">        batch[name].resize((batch_size, self.streams[i][name].shape[<span class=\"number\">1</span>],<span class=\"number\">1</span>))</div><div class=\"line\">        batch[name][(t*self.batch_num_streams + i), :,<span class=\"number\">0</span>] = self.streams[i][name][self.stream_indices[i],:]</div><div class=\"line\">    <span class=\"keyword\">elif</span> name <span class=\"keyword\">in</span> self.array_type_inputs.keys():</div><div class=\"line\">        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]][<span class=\"number\">0</span>,:]</div><div class=\"line\">    <span class=\"keyword\">else</span>:</div><div class=\"line\">        batch[name][t, i] = self.streams[i][name][self.stream_indices[i]]</div></pre></td></tr></table></figure></p>\n<h3 id=\"SequenceGenerator-reset-stream\"><a href=\"#SequenceGenerator-reset-stream\" class=\"headerlink\" title=\"SequenceGenerator::reset_stream\"></a>SequenceGenerator::reset_stream</h3><ol>\n<li>通过get_streams()得到下一个数据流（即下一个line对应的input, framefc7 …)</li>\n<li>修改实例变量streams[stream_index]为下一个数据流</li>\n</ol>\n<h2 id=\"framefc7-stream-text-to-hdf5-data-py\"><a href=\"#framefc7-stream-text-to-hdf5-data-py\" class=\"headerlink\" title=\"framefc7_stream_text_to_hdf5_data.py\"></a>framefc7_stream_text_to_hdf5_data.py</h2><h3 id=\"fc7FrameSequenceGenerator-init\"><a href=\"#fc7FrameSequenceGenerator-init\" class=\"headerlink\" title=\"fc7FrameSequenceGenerator::__init__\"></a>fc7FrameSequenceGenerator::__init__</h3><p>从txt中读取数据并将数据存入以下变量</p>\n<ul>\n<li>self.vid_framefeats[video_id]: 存放video_id对应的frames(frame1, frame2)的feats(4096)</li>\n<li>self.lines: pair&lt; vid, line &gt;</li>\n</ul>\n<h3 id=\"fc7FrameSequenceGenerator-get-streams\"><a href=\"#fc7FrameSequenceGenerator-get-streams\" class=\"headerlink\" title=\"fc7FrameSequenceGenerator::get_streams\"></a>fc7FrameSequenceGenerator::get_streams</h3><p>将下一条line对应的frames feats及其他数据规范化为MAX_WORD长度的out，out的示意如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">\t\t     MAX_WORD</div><div class=\"line\">cont_sentence\tx x x x ... x x x x</div><div class=\"line\">input_sentence  x x x x ... x x x x </div><div class=\"line\">frame_fc7\tx x x x ... x x x x</div><div class=\"line\">\t\t| | | |     | | | |  </div><div class=\"line\">\t\t| | | |     | | | |  4096</div></pre></td></tr></table></figure>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>在看源码前大概<strong>交流</strong>一下各个函数是干嘛用的，把握整体思路</li>\n<li>在纸上画出<strong>核心函数调用链</strong></li>\n<li>像python这样的弱类型语言，可以看<strong>被调函数返回数据</strong>的数据结构</li>\n</ul>\n<h2 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h2><ul>\n<li><a href=\"https://github.com/meltycriss/commented_src/blob/master/s2vt_data/hdf5_npstreamsequence_generator.py\">含注释的hdf5_npstreamsequence_generator.py</a></li>\n</ul>\n<h2 id=\"reference\"><a href=\"#reference\" class=\"headerlink\" title=\"reference\"></a>reference</h2><ul>\n<li>项目地址：<a href=\"https://arxiv.org/abs/1505.00487\">Sequence to Sequence – Video to Text</a></li>\n</ul>\n"},{"title":"Caffe学习：LSTM源码阅读","date":"2016-08-05T02:00:46.000Z","description":["LSTM源码阅读，主要针对几个比较tricky的点展开"],"_content":"\n## 简介\n由于之前已经有一篇RNN的源码阅读文章，这里就不再从超类讲起了，而且整体思路上LSTM跟RNN比较相似，所以本文主要是将某些比较tricky的点提出来，而不再像往常的源码阅读一样对整份源码解析。\n\n## 源码框架\n\n### 目录结构\n- **sequence_layers.hpp:**  抽象类RecurrentLayer，子类RNN和LSTM的头文件\n- **recurrent_layer.cpp:** 抽象类RecurrentLayer的定义文件\n- **lstm_layer.cpp:** 子类LSTM的定义文件\n- **lstm\\_unit\\_layer.cpp:** 子类LSTM的辅助层定义文件\n\n### 逻辑结构\n首先要说明的是这里的LSTM不是[标准LSTM](http://www.jianshu.com/p/9dc9f41f0b29/)，而是一种变种，具体参考论文[Sequence to Sequence - Video to Text](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)，最关键的区别在于x和h的处理，不再是拼接，而是求和。\n\n每个for循环生成了一个像这样的net（省略cont）\n\n```\n     |---|         |---|\n c_0 |   |   c_0   |   |  c_1\n-----|   |---------|   |-----\n h_0 |   | Wh+Wx+b |   |  h_1\n-----|   |---------|   |-----\n     |---|         |---|\n   lstm_layer  lstm_unit_layer\n\n```\n\n## num_output * 4\n可以看到在lstm\\_layer.cpp中的内积层都是将num\\_output设置成num_output * 4\n\n```cpp\n  LayerParameter hidden_param;\n  hidden_param.set_type(\"InnerProduct\");\n  hidden_param.mutable_inner_product_param()->set_num_output(num_output * 4);\n  hidden_param.mutable_inner_product_param()->set_bias_term(false);\n  hidden_param.mutable_inner_product_param()->set_axis(2);\n  hidden_param.mutable_inner_product_param()->\n      mutable_weight_filler()->CopyFrom(weight_filler);\n```\n\n而这个内积层后面用作将h全连接成一个num_output * 4维的向量\n\n```cpp\n  LayerParameter* w_param = net_param->add_layer();\n  w_param->CopyFrom(hidden_param);\n  w_param->set_name(\"transform_\" + ts);\n  w_param->add_param()->set_name(\"W_hc\");\n  w_param->add_bottom(\"h_conted_\" + tm1s);\n  w_param->add_top(\"W_hc_h_\" + tm1s);\n  w_param->mutable_inner_product_param()->set_axis(2);\n```\n\n最后又可以看到，在lstm\\_unit\\_layer.cpp中，当使用到这个num_output * 4维的向量时，是把它当作4个不同意义的向量来用的\n\n\n```cpp\n  const Dtype i = sigmoid(X[d]);\n  const Dtype f = (*flush == 0) ? 0 :\n      (*flush * sigmoid(X[1 * hidden_dim_ + d]));\n  const Dtype o = sigmoid(X[2 * hidden_dim_ + d]);\n  const Dtype g = tanh(X[3 * hidden_dim_ + d]);\n```\n\n那为什么要用一种这么不直观的方式来写呢？一般来说，有违直观理解的编码方式都是**出于提高效率**考虑的。下面讨论具体原因\n\n- 创建一个layer有**开销**\n- 一个大矩阵的优化会比拆开的几个小矩阵的优化效果好，因为\n\t* 拆开的几个小矩阵相当于用**循环**\n\t* 在有**优化矩阵计算**的情况下，直接用矩阵计算会比用循环来计算快很多[（为什么矩阵计算比循环快）](https://www.zhihu.com/question/19706331)\n\n## gate_input\n\n源码里面给到lstm\\_unit\\_layer.cpp的输入是通过h和x求完内积的gate_input，而不是比较直观地将h和x输入，然后在lstm\\_unit\\_layer.cpp里面求内积。\n\n```cpp\n // Add LSTMUnit layer to compute the cell & hidden vectors c_t and h_t.\n // Inputs: c_{t-1}, gate_input_t = (i_t, f_t, o_t, g_t), cont_t\n // Outputs: c_t, h_t\n //     [ i_t' ]\n //     [ f_t' ] := gate_input_t\n //     [ o_t' ]\n //     [ g_t' ]\n //         i_t := \\sigmoid[i_t']\n //         f_t := \\sigmoid[f_t']\n //         o_t := \\sigmoid[o_t']\n //         g_t := \\tanh[g_t']\n //         c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)\n //         h_t := o_t .* \\tanh[c_t]\n {\n   LayerParameter* lstm_unit_param = net_param->add_layer();\n   lstm_unit_param->set_type(\"LSTMUnit\");\n   lstm_unit_param->add_bottom(\"c_\" + tm1s);\n   lstm_unit_param->add_bottom(\"gate_input_\" + ts);\n   lstm_unit_param->add_bottom(\"cont_\" + ts);\n   lstm_unit_param->add_top(\"c_\" + ts);\n   lstm_unit_param->add_top(\"h_\" + ts);\n   lstm_unit_param->set_name(\"unit_\" + ts);\n }\n\n```\n\nX先在外面统一求，再通过切片的方式传给unit可以理解成为了提高效率。但是为什么要将h也在外面求完内积，和x求和后以一个统一的gate_input传给unit就是出于**实现方便**的角度考虑。\n\n- 试想将求内积的操作放在unit里面，那么在**求回传梯度**的时候，由于有一个内积层，就变得非常不好求\n- 而现在的实现方式，将所有内积操作放在unit外面，使得unit求梯度回传与内积层无关，变得简单\n\n## tanh与sigmoid\n\n之前都没发现原来tanh和sigmoid之间的关系是这样的\n\n```cpp\n  template <typename Dtype>\n  inline Dtype sigmoid(Dtype x) {\n    return 1. / (1. + exp(-x));\n  }\n\n  template <typename Dtype>\n  inline Dtype tanh(Dtype x) {\n    return 2. * sigmoid(2. * x) - 1.;\n  }\n\n```\n\n进而他们的导数为\n\n$$\nsigmoid' = sigmoid * (1 - sigmoid)\\\\\\\\\ntanh' = 1 - tanh^{2}\n$$\n\n## 对cont的处理\n\ncont为0的时候需要截断操作，具体的表现为\n\n- 作为输入的h为0\n\n```cpp\n  // Add layers to flush the hidden state when beginning a new\n  // sequence, as indicated by cont_t.\n  //     h_conted_{t-1} := cont_t * h_{t-1}\n  //\n  // Normally, cont_t is binary (i.e., 0 or 1), so:\n  //     h_conted_{t-1} := h_{t-1} if cont_t == 1\n  //                       0   otherwise\n  {\n    LayerParameter* cont_h_param = net_param->add_layer();\n    cont_h_param->CopyFrom(scalar_param);\n    cont_h_param->set_name(\"h_conted_\" + tm1s);\n    cont_h_param->add_bottom(\"h_\" + tm1s);\n    cont_h_param->add_bottom(\"cont_\" + ts);\n    cont_h_param->add_top(\"h_conted_\" + tm1s);\n  }\n```\n\n- 对c不进行遗忘操作\n\n```cpp\n    //         c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)\n    const Dtype f = (*flush == 0) ? 0 :\n        (*flush * sigmoid(X[1 * hidden_dim_ + d]));\n\n```\n\n同时也可以看出来，当$h_0$和$c_0$的初始值设为多少不会有影响，因为\n\n- 在时刻0的时候cont为0，$h_0$会被归0\n- 且由于f\\_t=0，c\\_{t-1}对c_{t}没影响\n\n## 收获\n\n- 看源码的时候先确定模型是**标准模型**还是**变种**\n- 实现有违直观逻辑时，考虑\n\t* **效率**\n\t* **实现便利度**","source":"_posts/caffe_4_lstm.md","raw":"---\ntitle: Caffe学习：LSTM源码阅读\ndate: 2016-08-05 10:00:46\ntags: \n  - lstm\ndescription:\n  - LSTM源码阅读，主要针对几个比较tricky的点展开\ncategories:\n  - caffe\n---\n\n## 简介\n由于之前已经有一篇RNN的源码阅读文章，这里就不再从超类讲起了，而且整体思路上LSTM跟RNN比较相似，所以本文主要是将某些比较tricky的点提出来，而不再像往常的源码阅读一样对整份源码解析。\n\n## 源码框架\n\n### 目录结构\n- **sequence_layers.hpp:**  抽象类RecurrentLayer，子类RNN和LSTM的头文件\n- **recurrent_layer.cpp:** 抽象类RecurrentLayer的定义文件\n- **lstm_layer.cpp:** 子类LSTM的定义文件\n- **lstm\\_unit\\_layer.cpp:** 子类LSTM的辅助层定义文件\n\n### 逻辑结构\n首先要说明的是这里的LSTM不是[标准LSTM](http://www.jianshu.com/p/9dc9f41f0b29/)，而是一种变种，具体参考论文[Sequence to Sequence - Video to Text](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)，最关键的区别在于x和h的处理，不再是拼接，而是求和。\n\n每个for循环生成了一个像这样的net（省略cont）\n\n```\n     |---|         |---|\n c_0 |   |   c_0   |   |  c_1\n-----|   |---------|   |-----\n h_0 |   | Wh+Wx+b |   |  h_1\n-----|   |---------|   |-----\n     |---|         |---|\n   lstm_layer  lstm_unit_layer\n\n```\n\n## num_output * 4\n可以看到在lstm\\_layer.cpp中的内积层都是将num\\_output设置成num_output * 4\n\n```cpp\n  LayerParameter hidden_param;\n  hidden_param.set_type(\"InnerProduct\");\n  hidden_param.mutable_inner_product_param()->set_num_output(num_output * 4);\n  hidden_param.mutable_inner_product_param()->set_bias_term(false);\n  hidden_param.mutable_inner_product_param()->set_axis(2);\n  hidden_param.mutable_inner_product_param()->\n      mutable_weight_filler()->CopyFrom(weight_filler);\n```\n\n而这个内积层后面用作将h全连接成一个num_output * 4维的向量\n\n```cpp\n  LayerParameter* w_param = net_param->add_layer();\n  w_param->CopyFrom(hidden_param);\n  w_param->set_name(\"transform_\" + ts);\n  w_param->add_param()->set_name(\"W_hc\");\n  w_param->add_bottom(\"h_conted_\" + tm1s);\n  w_param->add_top(\"W_hc_h_\" + tm1s);\n  w_param->mutable_inner_product_param()->set_axis(2);\n```\n\n最后又可以看到，在lstm\\_unit\\_layer.cpp中，当使用到这个num_output * 4维的向量时，是把它当作4个不同意义的向量来用的\n\n\n```cpp\n  const Dtype i = sigmoid(X[d]);\n  const Dtype f = (*flush == 0) ? 0 :\n      (*flush * sigmoid(X[1 * hidden_dim_ + d]));\n  const Dtype o = sigmoid(X[2 * hidden_dim_ + d]);\n  const Dtype g = tanh(X[3 * hidden_dim_ + d]);\n```\n\n那为什么要用一种这么不直观的方式来写呢？一般来说，有违直观理解的编码方式都是**出于提高效率**考虑的。下面讨论具体原因\n\n- 创建一个layer有**开销**\n- 一个大矩阵的优化会比拆开的几个小矩阵的优化效果好，因为\n\t* 拆开的几个小矩阵相当于用**循环**\n\t* 在有**优化矩阵计算**的情况下，直接用矩阵计算会比用循环来计算快很多[（为什么矩阵计算比循环快）](https://www.zhihu.com/question/19706331)\n\n## gate_input\n\n源码里面给到lstm\\_unit\\_layer.cpp的输入是通过h和x求完内积的gate_input，而不是比较直观地将h和x输入，然后在lstm\\_unit\\_layer.cpp里面求内积。\n\n```cpp\n // Add LSTMUnit layer to compute the cell & hidden vectors c_t and h_t.\n // Inputs: c_{t-1}, gate_input_t = (i_t, f_t, o_t, g_t), cont_t\n // Outputs: c_t, h_t\n //     [ i_t' ]\n //     [ f_t' ] := gate_input_t\n //     [ o_t' ]\n //     [ g_t' ]\n //         i_t := \\sigmoid[i_t']\n //         f_t := \\sigmoid[f_t']\n //         o_t := \\sigmoid[o_t']\n //         g_t := \\tanh[g_t']\n //         c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)\n //         h_t := o_t .* \\tanh[c_t]\n {\n   LayerParameter* lstm_unit_param = net_param->add_layer();\n   lstm_unit_param->set_type(\"LSTMUnit\");\n   lstm_unit_param->add_bottom(\"c_\" + tm1s);\n   lstm_unit_param->add_bottom(\"gate_input_\" + ts);\n   lstm_unit_param->add_bottom(\"cont_\" + ts);\n   lstm_unit_param->add_top(\"c_\" + ts);\n   lstm_unit_param->add_top(\"h_\" + ts);\n   lstm_unit_param->set_name(\"unit_\" + ts);\n }\n\n```\n\nX先在外面统一求，再通过切片的方式传给unit可以理解成为了提高效率。但是为什么要将h也在外面求完内积，和x求和后以一个统一的gate_input传给unit就是出于**实现方便**的角度考虑。\n\n- 试想将求内积的操作放在unit里面，那么在**求回传梯度**的时候，由于有一个内积层，就变得非常不好求\n- 而现在的实现方式，将所有内积操作放在unit外面，使得unit求梯度回传与内积层无关，变得简单\n\n## tanh与sigmoid\n\n之前都没发现原来tanh和sigmoid之间的关系是这样的\n\n```cpp\n  template <typename Dtype>\n  inline Dtype sigmoid(Dtype x) {\n    return 1. / (1. + exp(-x));\n  }\n\n  template <typename Dtype>\n  inline Dtype tanh(Dtype x) {\n    return 2. * sigmoid(2. * x) - 1.;\n  }\n\n```\n\n进而他们的导数为\n\n$$\nsigmoid' = sigmoid * (1 - sigmoid)\\\\\\\\\ntanh' = 1 - tanh^{2}\n$$\n\n## 对cont的处理\n\ncont为0的时候需要截断操作，具体的表现为\n\n- 作为输入的h为0\n\n```cpp\n  // Add layers to flush the hidden state when beginning a new\n  // sequence, as indicated by cont_t.\n  //     h_conted_{t-1} := cont_t * h_{t-1}\n  //\n  // Normally, cont_t is binary (i.e., 0 or 1), so:\n  //     h_conted_{t-1} := h_{t-1} if cont_t == 1\n  //                       0   otherwise\n  {\n    LayerParameter* cont_h_param = net_param->add_layer();\n    cont_h_param->CopyFrom(scalar_param);\n    cont_h_param->set_name(\"h_conted_\" + tm1s);\n    cont_h_param->add_bottom(\"h_\" + tm1s);\n    cont_h_param->add_bottom(\"cont_\" + ts);\n    cont_h_param->add_top(\"h_conted_\" + tm1s);\n  }\n```\n\n- 对c不进行遗忘操作\n\n```cpp\n    //         c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)\n    const Dtype f = (*flush == 0) ? 0 :\n        (*flush * sigmoid(X[1 * hidden_dim_ + d]));\n\n```\n\n同时也可以看出来，当$h_0$和$c_0$的初始值设为多少不会有影响，因为\n\n- 在时刻0的时候cont为0，$h_0$会被归0\n- 且由于f\\_t=0，c\\_{t-1}对c_{t}没影响\n\n## 收获\n\n- 看源码的时候先确定模型是**标准模型**还是**变种**\n- 实现有违直观逻辑时，考虑\n\t* **效率**\n\t* **实现便利度**","slug":"caffe_4_lstm","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrpc0006xuvtjie6lvpp","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>由于之前已经有一篇RNN的源码阅读文章，这里就不再从超类讲起了，而且整体思路上LSTM跟RNN比较相似，所以本文主要是将某些比较tricky的点提出来，而不再像往常的源码阅读一样对整份源码解析。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h3><ul>\n<li><strong>sequence_layers.hpp:</strong>  抽象类RecurrentLayer，子类RNN和LSTM的头文件</li>\n<li><strong>recurrent_layer.cpp:</strong> 抽象类RecurrentLayer的定义文件</li>\n<li><strong>lstm_layer.cpp:</strong> 子类LSTM的定义文件</li>\n<li><strong>lstm_unit_layer.cpp:</strong> 子类LSTM的辅助层定义文件</li>\n</ul>\n<h3 id=\"逻辑结构\"><a href=\"#逻辑结构\" class=\"headerlink\" title=\"逻辑结构\"></a>逻辑结构</h3><p>首先要说明的是这里的LSTM不是<a href=\"http://www.jianshu.com/p/9dc9f41f0b29/\" target=\"_blank\" rel=\"external\">标准LSTM</a>，而是一种变种，具体参考论文<a href=\"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\" target=\"_blank\" rel=\"external\">Sequence to Sequence - Video to Text</a>，最关键的区别在于x和h的处理，不再是拼接，而是求和。</p>\n<p>每个for循环生成了一个像这样的net（省略cont）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">     |---|         |---|</div><div class=\"line\"> c_0 |   |   c_0   |   |  c_1</div><div class=\"line\">-----|   |---------|   |-----</div><div class=\"line\"> h_0 |   | Wh+Wx+b |   |  h_1</div><div class=\"line\">-----|   |---------|   |-----</div><div class=\"line\">     |---|         |---|</div><div class=\"line\">   lstm_layer  lstm_unit_layer</div></pre></td></tr></table></figure>\n<h2 id=\"num-output-4\"><a href=\"#num-output-4\" class=\"headerlink\" title=\"num_output * 4\"></a>num_output * 4</h2><p>可以看到在lstm_layer.cpp中的内积层都是将num_output设置成num_output * 4</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">LayerParameter hidden_param;</div><div class=\"line\">hidden_param.set_type(<span class=\"string\">\"InnerProduct\"</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_num_output(num_output * <span class=\"number\">4</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_bias_term(<span class=\"literal\">false</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_axis(<span class=\"number\">2</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;</div><div class=\"line\">    mutable_weight_filler()-&gt;CopyFrom(weight_filler);</div></pre></td></tr></table></figure>\n<p>而这个内积层后面用作将h全连接成一个num_output * 4维的向量</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">LayerParameter* w_param = net_param-&gt;add_layer();</div><div class=\"line\">w_param-&gt;CopyFrom(hidden_param);</div><div class=\"line\">w_param-&gt;set_name(<span class=\"string\">\"transform_\"</span> + ts);</div><div class=\"line\">w_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_hc\"</span>);</div><div class=\"line\">w_param-&gt;add_bottom(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">w_param-&gt;add_top(<span class=\"string\">\"W_hc_h_\"</span> + tm1s);</div><div class=\"line\">w_param-&gt;mutable_inner_product_param()-&gt;set_axis(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure>\n<p>最后又可以看到，在lstm_unit_layer.cpp中，当使用到这个num_output * 4维的向量时，是把它当作4个不同意义的向量来用的</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> Dtype i = sigmoid(X[d]);</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype f = (*flush == <span class=\"number\">0</span>) ? <span class=\"number\">0</span> :</div><div class=\"line\">    (*flush * sigmoid(X[<span class=\"number\">1</span> * hidden_dim_ + d]));</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype o = sigmoid(X[<span class=\"number\">2</span> * hidden_dim_ + d]);</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype g = <span class=\"built_in\">tanh</span>(X[<span class=\"number\">3</span> * hidden_dim_ + d]);</div></pre></td></tr></table></figure>\n<p>那为什么要用一种这么不直观的方式来写呢？一般来说，有违直观理解的编码方式都是<strong>出于提高效率</strong>考虑的。下面讨论具体原因</p>\n<ul>\n<li>创建一个layer有<strong>开销</strong></li>\n<li>一个大矩阵的优化会比拆开的几个小矩阵的优化效果好，因为<ul>\n<li>拆开的几个小矩阵相当于用<strong>循环</strong></li>\n<li>在有<strong>优化矩阵计算</strong>的情况下，直接用矩阵计算会比用循环来计算快很多<a href=\"https://www.zhihu.com/question/19706331\" target=\"_blank\" rel=\"external\">（为什么矩阵计算比循环快）</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"gate-input\"><a href=\"#gate-input\" class=\"headerlink\" title=\"gate_input\"></a>gate_input</h2><p>源码里面给到lstm_unit_layer.cpp的输入是通过h和x求完内积的gate_input，而不是比较直观地将h和x输入，然后在lstm_unit_layer.cpp里面求内积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Add LSTMUnit layer to compute the cell &amp; hidden vectors c_t and h_t.</span></div><div class=\"line\"><span class=\"comment\">// Inputs: c_&#123;t-1&#125;, gate_input_t = (i_t, f_t, o_t, g_t), cont_t</span></div><div class=\"line\"><span class=\"comment\">// Outputs: c_t, h_t</span></div><div class=\"line\"><span class=\"comment\">//     [ i_t' ]</span></div><div class=\"line\"><span class=\"comment\">//     [ f_t' ] := gate_input_t</span></div><div class=\"line\"><span class=\"comment\">//     [ o_t' ]</span></div><div class=\"line\"><span class=\"comment\">//     [ g_t' ]</span></div><div class=\"line\"><span class=\"comment\">//         i_t := \\sigmoid[i_t']</span></div><div class=\"line\"><span class=\"comment\">//         f_t := \\sigmoid[f_t']</span></div><div class=\"line\"><span class=\"comment\">//         o_t := \\sigmoid[o_t']</span></div><div class=\"line\"><span class=\"comment\">//         g_t := \\tanh[g_t']</span></div><div class=\"line\"><span class=\"comment\">//         c_t := cont_t * (f_t .* c_&#123;t-1&#125;) + (i_t .* g_t)</span></div><div class=\"line\"><span class=\"comment\">//         h_t := o_t .* \\tanh[c_t]</span></div><div class=\"line\">&#123;</div><div class=\"line\">  LayerParameter* lstm_unit_param = net_param-&gt;add_layer();</div><div class=\"line\">  lstm_unit_param-&gt;set_type(<span class=\"string\">\"LSTMUnit\"</span>);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"c_\"</span> + tm1s);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"gate_input_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"cont_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_top(<span class=\"string\">\"c_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_top(<span class=\"string\">\"h_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;set_name(<span class=\"string\">\"unit_\"</span> + ts);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>X先在外面统一求，再通过切片的方式传给unit可以理解成为了提高效率。但是为什么要将h也在外面求完内积，和x求和后以一个统一的gate_input传给unit就是出于<strong>实现方便</strong>的角度考虑。</p>\n<ul>\n<li>试想将求内积的操作放在unit里面，那么在<strong>求回传梯度</strong>的时候，由于有一个内积层，就变得非常不好求</li>\n<li>而现在的实现方式，将所有内积操作放在unit外面，使得unit求梯度回传与内积层无关，变得简单</li>\n</ul>\n<h2 id=\"tanh与sigmoid\"><a href=\"#tanh与sigmoid\" class=\"headerlink\" title=\"tanh与sigmoid\"></a>tanh与sigmoid</h2><p>之前都没发现原来tanh和sigmoid之间的关系是这样的</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> Dtype <span class=\"title\">sigmoid</span><span class=\"params\">(Dtype x)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + <span class=\"built_in\">exp</span>(-x));</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> Dtype <span class=\"title\">tanh</span><span class=\"params\">(Dtype x)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">2.</span> * sigmoid(<span class=\"number\">2.</span> * x) - <span class=\"number\">1.</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>进而他们的导数为</p>\n<p>$$<br>sigmoid’ = sigmoid * (1 - sigmoid)\\\\<br>tanh’ = 1 - tanh^{2}<br>$$</p>\n<h2 id=\"对cont的处理\"><a href=\"#对cont的处理\" class=\"headerlink\" title=\"对cont的处理\"></a>对cont的处理</h2><p>cont为0的时候需要截断操作，具体的表现为</p>\n<ul>\n<li>作为输入的h为0</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Add layers to flush the hidden state when beginning a new</span></div><div class=\"line\"><span class=\"comment\">// sequence, as indicated by cont_t.</span></div><div class=\"line\"><span class=\"comment\">//     h_conted_&#123;t-1&#125; := cont_t * h_&#123;t-1&#125;</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// Normally, cont_t is binary (i.e., 0 or 1), so:</span></div><div class=\"line\"><span class=\"comment\">//     h_conted_&#123;t-1&#125; := h_&#123;t-1&#125; if cont_t == 1</span></div><div class=\"line\"><span class=\"comment\">//                       0   otherwise</span></div><div class=\"line\">&#123;</div><div class=\"line\">  LayerParameter* cont_h_param = net_param-&gt;add_layer();</div><div class=\"line\">  cont_h_param-&gt;CopyFrom(scalar_param);</div><div class=\"line\">  cont_h_param-&gt;set_name(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">  cont_h_param-&gt;add_bottom(<span class=\"string\">\"h_\"</span> + tm1s);</div><div class=\"line\">  cont_h_param-&gt;add_bottom(<span class=\"string\">\"cont_\"</span> + ts);</div><div class=\"line\">  cont_h_param-&gt;add_top(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>对c不进行遗忘操作</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//         c_t := cont_t * (f_t .* c_&#123;t-1&#125;) + (i_t .* g_t)</span></div><div class=\"line\"><span class=\"keyword\">const</span> Dtype f = (*flush == <span class=\"number\">0</span>) ? <span class=\"number\">0</span> :</div><div class=\"line\">    (*flush * sigmoid(X[<span class=\"number\">1</span> * hidden_dim_ + d]));</div></pre></td></tr></table></figure>\n<p>同时也可以看出来，当$h_0$和$c_0$的初始值设为多少不会有影响，因为</p>\n<ul>\n<li>在时刻0的时候cont为0，$h_0$会被归0</li>\n<li>且由于f_t=0，c_{t-1}对c_{t}没影响</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>看源码的时候先确定模型是<strong>标准模型</strong>还是<strong>变种</strong></li>\n<li>实现有违直观逻辑时，考虑<ul>\n<li><strong>效率</strong></li>\n<li><strong>实现便利度</strong></li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>由于之前已经有一篇RNN的源码阅读文章，这里就不再从超类讲起了，而且整体思路上LSTM跟RNN比较相似，所以本文主要是将某些比较tricky的点提出来，而不再像往常的源码阅读一样对整份源码解析。</p>\n<h2 id=\"源码框架\"><a href=\"#源码框架\" class=\"headerlink\" title=\"源码框架\"></a>源码框架</h2><h3 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h3><ul>\n<li><strong>sequence_layers.hpp:</strong>  抽象类RecurrentLayer，子类RNN和LSTM的头文件</li>\n<li><strong>recurrent_layer.cpp:</strong> 抽象类RecurrentLayer的定义文件</li>\n<li><strong>lstm_layer.cpp:</strong> 子类LSTM的定义文件</li>\n<li><strong>lstm_unit_layer.cpp:</strong> 子类LSTM的辅助层定义文件</li>\n</ul>\n<h3 id=\"逻辑结构\"><a href=\"#逻辑结构\" class=\"headerlink\" title=\"逻辑结构\"></a>逻辑结构</h3><p>首先要说明的是这里的LSTM不是<a href=\"http://www.jianshu.com/p/9dc9f41f0b29/\">标准LSTM</a>，而是一种变种，具体参考论文<a href=\"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\">Sequence to Sequence - Video to Text</a>，最关键的区别在于x和h的处理，不再是拼接，而是求和。</p>\n<p>每个for循环生成了一个像这样的net（省略cont）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">     |---|         |---|</div><div class=\"line\"> c_0 |   |   c_0   |   |  c_1</div><div class=\"line\">-----|   |---------|   |-----</div><div class=\"line\"> h_0 |   | Wh+Wx+b |   |  h_1</div><div class=\"line\">-----|   |---------|   |-----</div><div class=\"line\">     |---|         |---|</div><div class=\"line\">   lstm_layer  lstm_unit_layer</div></pre></td></tr></table></figure>\n<h2 id=\"num-output-4\"><a href=\"#num-output-4\" class=\"headerlink\" title=\"num_output * 4\"></a>num_output * 4</h2><p>可以看到在lstm_layer.cpp中的内积层都是将num_output设置成num_output * 4</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">LayerParameter hidden_param;</div><div class=\"line\">hidden_param.set_type(<span class=\"string\">\"InnerProduct\"</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_num_output(num_output * <span class=\"number\">4</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_bias_term(<span class=\"literal\">false</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;set_axis(<span class=\"number\">2</span>);</div><div class=\"line\">hidden_param.mutable_inner_product_param()-&gt;</div><div class=\"line\">    mutable_weight_filler()-&gt;CopyFrom(weight_filler);</div></pre></td></tr></table></figure>\n<p>而这个内积层后面用作将h全连接成一个num_output * 4维的向量</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">LayerParameter* w_param = net_param-&gt;add_layer();</div><div class=\"line\">w_param-&gt;CopyFrom(hidden_param);</div><div class=\"line\">w_param-&gt;set_name(<span class=\"string\">\"transform_\"</span> + ts);</div><div class=\"line\">w_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_hc\"</span>);</div><div class=\"line\">w_param-&gt;add_bottom(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">w_param-&gt;add_top(<span class=\"string\">\"W_hc_h_\"</span> + tm1s);</div><div class=\"line\">w_param-&gt;mutable_inner_product_param()-&gt;set_axis(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure>\n<p>最后又可以看到，在lstm_unit_layer.cpp中，当使用到这个num_output * 4维的向量时，是把它当作4个不同意义的向量来用的</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> Dtype i = sigmoid(X[d]);</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype f = (*flush == <span class=\"number\">0</span>) ? <span class=\"number\">0</span> :</div><div class=\"line\">    (*flush * sigmoid(X[<span class=\"number\">1</span> * hidden_dim_ + d]));</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype o = sigmoid(X[<span class=\"number\">2</span> * hidden_dim_ + d]);</div><div class=\"line\"><span class=\"keyword\">const</span> Dtype g = <span class=\"built_in\">tanh</span>(X[<span class=\"number\">3</span> * hidden_dim_ + d]);</div></pre></td></tr></table></figure>\n<p>那为什么要用一种这么不直观的方式来写呢？一般来说，有违直观理解的编码方式都是<strong>出于提高效率</strong>考虑的。下面讨论具体原因</p>\n<ul>\n<li>创建一个layer有<strong>开销</strong></li>\n<li>一个大矩阵的优化会比拆开的几个小矩阵的优化效果好，因为<ul>\n<li>拆开的几个小矩阵相当于用<strong>循环</strong></li>\n<li>在有<strong>优化矩阵计算</strong>的情况下，直接用矩阵计算会比用循环来计算快很多<a href=\"https://www.zhihu.com/question/19706331\">（为什么矩阵计算比循环快）</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"gate-input\"><a href=\"#gate-input\" class=\"headerlink\" title=\"gate_input\"></a>gate_input</h2><p>源码里面给到lstm_unit_layer.cpp的输入是通过h和x求完内积的gate_input，而不是比较直观地将h和x输入，然后在lstm_unit_layer.cpp里面求内积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Add LSTMUnit layer to compute the cell &amp; hidden vectors c_t and h_t.</span></div><div class=\"line\"><span class=\"comment\">// Inputs: c_&#123;t-1&#125;, gate_input_t = (i_t, f_t, o_t, g_t), cont_t</span></div><div class=\"line\"><span class=\"comment\">// Outputs: c_t, h_t</span></div><div class=\"line\"><span class=\"comment\">//     [ i_t' ]</span></div><div class=\"line\"><span class=\"comment\">//     [ f_t' ] := gate_input_t</span></div><div class=\"line\"><span class=\"comment\">//     [ o_t' ]</span></div><div class=\"line\"><span class=\"comment\">//     [ g_t' ]</span></div><div class=\"line\"><span class=\"comment\">//         i_t := \\sigmoid[i_t']</span></div><div class=\"line\"><span class=\"comment\">//         f_t := \\sigmoid[f_t']</span></div><div class=\"line\"><span class=\"comment\">//         o_t := \\sigmoid[o_t']</span></div><div class=\"line\"><span class=\"comment\">//         g_t := \\tanh[g_t']</span></div><div class=\"line\"><span class=\"comment\">//         c_t := cont_t * (f_t .* c_&#123;t-1&#125;) + (i_t .* g_t)</span></div><div class=\"line\"><span class=\"comment\">//         h_t := o_t .* \\tanh[c_t]</span></div><div class=\"line\">&#123;</div><div class=\"line\">  LayerParameter* lstm_unit_param = net_param-&gt;add_layer();</div><div class=\"line\">  lstm_unit_param-&gt;set_type(<span class=\"string\">\"LSTMUnit\"</span>);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"c_\"</span> + tm1s);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"gate_input_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_bottom(<span class=\"string\">\"cont_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_top(<span class=\"string\">\"c_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;add_top(<span class=\"string\">\"h_\"</span> + ts);</div><div class=\"line\">  lstm_unit_param-&gt;set_name(<span class=\"string\">\"unit_\"</span> + ts);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>X先在外面统一求，再通过切片的方式传给unit可以理解成为了提高效率。但是为什么要将h也在外面求完内积，和x求和后以一个统一的gate_input传给unit就是出于<strong>实现方便</strong>的角度考虑。</p>\n<ul>\n<li>试想将求内积的操作放在unit里面，那么在<strong>求回传梯度</strong>的时候，由于有一个内积层，就变得非常不好求</li>\n<li>而现在的实现方式，将所有内积操作放在unit外面，使得unit求梯度回传与内积层无关，变得简单</li>\n</ul>\n<h2 id=\"tanh与sigmoid\"><a href=\"#tanh与sigmoid\" class=\"headerlink\" title=\"tanh与sigmoid\"></a>tanh与sigmoid</h2><p>之前都没发现原来tanh和sigmoid之间的关系是这样的</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> Dtype <span class=\"title\">sigmoid</span><span class=\"params\">(Dtype x)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + <span class=\"built_in\">exp</span>(-x));</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> Dtype <span class=\"title\">tanh</span><span class=\"params\">(Dtype x)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">2.</span> * sigmoid(<span class=\"number\">2.</span> * x) - <span class=\"number\">1.</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>进而他们的导数为</p>\n<p>$$<br>sigmoid’ = sigmoid * (1 - sigmoid)\\\\<br>tanh’ = 1 - tanh^{2}<br>$$</p>\n<h2 id=\"对cont的处理\"><a href=\"#对cont的处理\" class=\"headerlink\" title=\"对cont的处理\"></a>对cont的处理</h2><p>cont为0的时候需要截断操作，具体的表现为</p>\n<ul>\n<li>作为输入的h为0</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Add layers to flush the hidden state when beginning a new</span></div><div class=\"line\"><span class=\"comment\">// sequence, as indicated by cont_t.</span></div><div class=\"line\"><span class=\"comment\">//     h_conted_&#123;t-1&#125; := cont_t * h_&#123;t-1&#125;</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// Normally, cont_t is binary (i.e., 0 or 1), so:</span></div><div class=\"line\"><span class=\"comment\">//     h_conted_&#123;t-1&#125; := h_&#123;t-1&#125; if cont_t == 1</span></div><div class=\"line\"><span class=\"comment\">//                       0   otherwise</span></div><div class=\"line\">&#123;</div><div class=\"line\">  LayerParameter* cont_h_param = net_param-&gt;add_layer();</div><div class=\"line\">  cont_h_param-&gt;CopyFrom(scalar_param);</div><div class=\"line\">  cont_h_param-&gt;set_name(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">  cont_h_param-&gt;add_bottom(<span class=\"string\">\"h_\"</span> + tm1s);</div><div class=\"line\">  cont_h_param-&gt;add_bottom(<span class=\"string\">\"cont_\"</span> + ts);</div><div class=\"line\">  cont_h_param-&gt;add_top(<span class=\"string\">\"h_conted_\"</span> + tm1s);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>对c不进行遗忘操作</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//         c_t := cont_t * (f_t .* c_&#123;t-1&#125;) + (i_t .* g_t)</span></div><div class=\"line\"><span class=\"keyword\">const</span> Dtype f = (*flush == <span class=\"number\">0</span>) ? <span class=\"number\">0</span> :</div><div class=\"line\">    (*flush * sigmoid(X[<span class=\"number\">1</span> * hidden_dim_ + d]));</div></pre></td></tr></table></figure>\n<p>同时也可以看出来，当$h_0$和$c_0$的初始值设为多少不会有影响，因为</p>\n<ul>\n<li>在时刻0的时候cont为0，$h_0$会被归0</li>\n<li>且由于f_t=0，c_{t-1}对c_{t}没影响</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>看源码的时候先确定模型是<strong>标准模型</strong>还是<strong>变种</strong></li>\n<li>实现有违直观逻辑时，考虑<ul>\n<li><strong>效率</strong></li>\n<li><strong>实现便利度</strong></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Caffe学习：s2vt_captioner.py源码阅读","date":"2016-08-17T08:15:14.000Z","description":["s2vt_captioner.py源码阅读，是LSTM进行test的python脚本"],"_content":"\n## 简介\n\ns2vt_captioner.py是使用之前训练出来的s2vt model进行caption的脚本。整体逻辑跟训练的时候差不多，但还是有一些区别，比如\n\n- decode时候LSTM2的x不再是gt，而是上一个unit生成的单词\n- 使用了python的接口调用caffe\n\n下面按照脚本的核心函数调用链来展开。\n\n## 核心函数调用链\n\n- **main:** 划分为vid chunks\n\t* **run\\_pred\\_iters:** 划分为一个个vid\n\t\t- **encode\\_video\\_frames:** 使用vid的feats进行encode\n\t\t- **run\\_pred\\_iter:** decode一个vid，返回captions（有可能多个）\n\t\t\t* **predict\\_image\\_caption:** 根据caption策略选择对应的caption方式\n\t\t\t\t- **predict\\_image\\_caption\\_beam\\_search:** 使用beam search的方式进行caption\n\t\t\t\t\t* **predict\\_single\\_word:** 生成下一个单词\n\n## main\n\n将任务划分为一个个chunk，交给run\\_pred\\_iters完成任务\n\n```py\n    fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE,\n          vocab_file, max_words=MAX_WORDS, align=aligned, shuffle=False,\n          pad=aligned, truncate=aligned)\n    video_gt_pairs = all_video_gt_pairs(fsg)\n\n```\n\n- **video\\_gt\\_pairs:** dict{vid: list[gt\\_sents]}\n\t* 假若没有给gt的话，list[gt\\_sents]为空\n\n\n```py\n    outputs = run_pred_iters(lstm_net, chunk, video_gt_pairs,\n                  fsg, strategies=STRATEGIES, display_vocab=vocab_list)\n      \n```\n\n- **outputs:** dict{vid: output_batch}\n\t* 每个vid有可能有多于一个caption，所以是output_batch\n- **output_batch:** list[dict{caption, prob, gt, source}]\n\t* **caption:** 预测的句子，list[word1, word2, word3, ...]\n\t* **prob:** 句子各单词发生的概率，list[prob\\_word1, prob\\_word2, prob\\_word3, ...]\n\t* **source:** 搜索句子的策略 e.g. sample / beam\n\n```py\n    text_out_types = to_text_output(outputs, vocab_list)\n\n```\n\n- **text\\_output\\_types:** dict{source\\_type: list[output string]}\n\t* **source\\_type:** 搜索句子的策略 e.g. sample / beam\n\n## run\\_pred\\_iters\n\n将chunk进一步划分为一个个vid，交给run\\_pre\\_iter完成任务\n\n```py\n    # get fc7 feature for the video\n    video_features = video_to_descriptor(video_id, fsg)\n\n```\n\nvideo\\_features: list[1 \\* FEAT\\_DIM]\n\n```py\n    # run lstm on all the frames of video before predicting\n    encode_video_frames(pred_net, video_features)\n    outputs[video_id] = \\\n        run_pred_iter(pred_net, pad_img_feature, display_vocab, strategies=strategies)\n\n```\n\ncaption的流程：\n\n1. 将frame_feats encode到LSTM\n2. 从encode好的LSTM中decode出caption\n\n## encode\\_video\\_frames\n\n```py\n    net.forward(frames_fc7=image_features, cont_sentence=cont, input_sentence=data_en,\n       stage_indicator=stage_ind)\n\n```\n\n将数据格式匹配输入，将frame\\_feats依次传入LSTM，并设置input\\_sentence为0\n\n## run\\_pred\\_iter\n\n```py\n    captions, probs = predict_image_caption(net, pad_image_feature, vocab_list, strategy=strategy)\n```\n\n对每种strategy都调用predict\\_image\\_caption来进行caption\n\n## predict\\_image\\_caption\n\n根据strategy选择对应的image\\_caption函数进行caption\n\n## predict\\_image\\_caption\\_beam\\_search\n\n使用beam\\_search的方式生成句子。所谓的beam\\_search实际上为启发式的BFS，使用了贪心的思想：即每次搜索完下一层后，只在下一层中取beam\\_size个结点进一步展开，而其余的结点则不要\n\n```py\n  beam_size = 1\t#每层保留结点数\n  beams = [[]]\t#beams: list[sentence], sentence: list[word]\n  beams_complete = 0\t#当前层有多少beam时已经eos了\n  beam_probs = [[]]\t#粒度为单词，对应beams中每个单词出现的概率\n  beam_log_probs = [0.]\t#粒度为beam(i.e. 句子)，对应beams中每个beam出现的概率(log w1*w2*w3...)\n  current_input_word = 0  # first input is EOS\n  while beams_complete < len(beams):\n    # expansions: append a new word to current beams\n    expansions = []\t#记录了下一层单词的信息\n\n\t  #每一个单词的信息如下\n      # extension : the new word\n      exp = {'prefix_beam_index': beam_index, 'extension': [ind],\n             'prob_extension': [prob], 'log_prob': extended_beam_log_prob}\n      expansions.append(exp)\n\n      #prefix_beam_index: 这个单词是由哪条beam生成出来的\n      #extension: 这个单词在字典中的index\n      #prob_extension: 产生的是这个单词的概率\n      #log_prob: 加上这个单词后，beam的概率log w1*w2*w3...*w_extension\n\n```\n\n了解了数据结构后，先看内层循环\n\n```py\n    #       0       p_w1, p_w2...  w1, w2...\n    for beam_index, beam_log_prob, beam in \\\n        zip(range(len(beams)), beam_log_probs, beams):\n      if beam:\n        previous_word = beam[-1]\n        if len(beam) >= max_length or previous_word == 0:\n          exp = {'prefix_beam_index': beam_index, 'extension': [],\n                 'prob_extension': [], 'log_prob': beam_log_prob}\n          expansions.append(exp)\n          # Don't expand this beam; it was already ended with an EOS,\n          # or is the max length.\n          continue\n      else:\n        previous_word = 0  # EOS is first word\n      if beam_size == 1:\n        probs = predict_single_word(net, pad_img_feature, previous_word)\n      else:\n        probs = predict_single_word_from_all_previous(net, pad_img_feature, beam)\n      assert len(probs.shape) == 1\n      assert probs.shape[0] == len(vocab_list)\n      # index of top beam_size prob words\n      expansion_inds = probs.argsort()[-beam_size:]\n      for ind in expansion_inds:\n        prob = probs[ind]\n        extended_beam_log_prob = beam_log_prob + math.log(prob)\n        # extension : the new word\n        exp = {'prefix_beam_index': beam_index, 'extension': [ind],\n               'prob_extension': [prob], 'log_prob': extended_beam_log_prob}\n        expansions.append(exp)\n\n```\n\n将每条beam生成的单词中，概率最高的beam\\_size个单词保存到expansions中。内层循环结束后，expansions中含有len(beams) * beam\\_size个单词的信息。接下来看外层循环\n\n```py\n  while beams_complete < len(beams):\n    # expansions: append a new word to current beams\n    expansions = []\n\n  \t#内层循环\n\n    # Sort expansions in decreasing order of probabilitf.\n    expansions.sort(key=lambda expansion: -1 * expansion['log_prob'])\n    # only reserve beam_size number of node in each BFS layer\n    expansions = expansions[:beam_size]\n    new_beams = \\\n        [beams[e['prefix_beam_index']] + e['extension'] for e in expansions]\n    new_beam_probs = \\\n        [beam_probs[e['prefix_beam_index']] + e['prob_extension'] for e in expansions]\n    beam_log_probs = [e['log_prob'] for e in expansions]\n    beams_complete = 0\n    for beam in new_beams:\n      if beam[-1] == 0 or len(beam) >= max_length: beams_complete += 1\n    beams, beam_probs = new_beams, new_beam_probs\n\n```\n\n保留expansions中概率最高的beam\\_size个单词，并用这bean\\_size个单词扩展beams，得到size为beam\\_size的new_beams。一直循环，直到beams中所有beam都结束\n\n## predict\\_single\\_word\n\n同理encode\\_video\\_frame，不过这里以pad作为frames_fc7的输入\n\n## 关于caffemodel参数不匹配的问题\n\n可以看到\n\n- 在使用s2vt\\_captioner.py进行**test**的时候，网络的模型是**没有展开的**，然后通过将每次的output作为下次的input来传递c和h\n- 然而，在**train**的时候，可以看到网络的模型是将LSTM**展开的**\n- 显然，展开的LSTM的参数会比不展开的**参数要多得多**\n- 那为什么还能用**train出来的caffemodel给test用呢？**\n\n原因就在于，train的时候，展开的各部分实际上是share同一份参数的。而实现参数共享的trick就在我一开始想不明白有什么用的`add_param()->set_name()`\n\n```cpp\n  //lstm_layer.cpp\n\n  // Add layer to transform all timesteps of x to the hidden state dimension.\n  //     W_xc_x = W_xc * x + b_c\n  //{\n  //  LayerParameter* x_transform_param = net_param->add_layer();\n  //  x_transform_param->CopyFrom(biased_hidden_param);\n  //  x_transform_param->set_name(\"x_transform\");\n    x_transform_param->add_param()->set_name(\"W_xc\");\n    x_transform_param->add_param()->set_name(\"b_c\");\n  //  x_transform_param->add_bottom(\"x\");\n  //  x_transform_param->add_top(\"W_xc_x\");\n  //}\n\n  // Add layer to compute\n  //     W_hc_h_{t-1} := W_hc * h_conted_{t-1}\n  //{\n  //  LayerParameter* w_param = net_param->add_layer();\n  //  w_param->CopyFrom(hidden_param);\n  //  w_param->set_name(\"transform_\" + ts);\n    w_param->add_param()->set_name(\"W_hc\");\n  //  w_param->add_bottom(\"h_conted_\" + tm1s);\n  //  w_param->add_top(\"W_hc_h_\" + tm1s);\n  //  w_param->mutable_inner_product_param()->set_axis(2);\n  //}\n\n```\n\n再从caffe.proto中看这个param参数的含义\n\n```proto\n//caffe.proto\n\nmessage LayerParameter {\n  // Specifies training parameters (multipliers on global learning constants,\n  // and the name and other settings used for weight sharing).\n  repeated ParamSpec param = 6;\n}\n\n```\n\n所以说，所有参数都是共享同一份的\n\n## 收获\n\n- 在脚本迭代更新，备份上一份脚本时，**备份命名要有意义**，不要只加个.bak就算了，回去一个周末就忘记了.bak1, .bak2, .bak3是哪个跟哪个了\n- debug的时候还需要考虑**环境变量**，比如PYTHONPATH这种，特别是两份相同的代码在两台机子上跑出不一样的结果\n","source":"_posts/caffe_5_s2vt_captioner.md","raw":"---\ntitle: Caffe学习：s2vt_captioner.py源码阅读\ndate: 2016-08-17 16:15:14\ntags: \n  - s2vt_captioner\ndescription:\n  - s2vt_captioner.py源码阅读，是LSTM进行test的python脚本\ncategories:\n  - caffe\n---\n\n## 简介\n\ns2vt_captioner.py是使用之前训练出来的s2vt model进行caption的脚本。整体逻辑跟训练的时候差不多，但还是有一些区别，比如\n\n- decode时候LSTM2的x不再是gt，而是上一个unit生成的单词\n- 使用了python的接口调用caffe\n\n下面按照脚本的核心函数调用链来展开。\n\n## 核心函数调用链\n\n- **main:** 划分为vid chunks\n\t* **run\\_pred\\_iters:** 划分为一个个vid\n\t\t- **encode\\_video\\_frames:** 使用vid的feats进行encode\n\t\t- **run\\_pred\\_iter:** decode一个vid，返回captions（有可能多个）\n\t\t\t* **predict\\_image\\_caption:** 根据caption策略选择对应的caption方式\n\t\t\t\t- **predict\\_image\\_caption\\_beam\\_search:** 使用beam search的方式进行caption\n\t\t\t\t\t* **predict\\_single\\_word:** 生成下一个单词\n\n## main\n\n将任务划分为一个个chunk，交给run\\_pred\\_iters完成任务\n\n```py\n    fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE,\n          vocab_file, max_words=MAX_WORDS, align=aligned, shuffle=False,\n          pad=aligned, truncate=aligned)\n    video_gt_pairs = all_video_gt_pairs(fsg)\n\n```\n\n- **video\\_gt\\_pairs:** dict{vid: list[gt\\_sents]}\n\t* 假若没有给gt的话，list[gt\\_sents]为空\n\n\n```py\n    outputs = run_pred_iters(lstm_net, chunk, video_gt_pairs,\n                  fsg, strategies=STRATEGIES, display_vocab=vocab_list)\n      \n```\n\n- **outputs:** dict{vid: output_batch}\n\t* 每个vid有可能有多于一个caption，所以是output_batch\n- **output_batch:** list[dict{caption, prob, gt, source}]\n\t* **caption:** 预测的句子，list[word1, word2, word3, ...]\n\t* **prob:** 句子各单词发生的概率，list[prob\\_word1, prob\\_word2, prob\\_word3, ...]\n\t* **source:** 搜索句子的策略 e.g. sample / beam\n\n```py\n    text_out_types = to_text_output(outputs, vocab_list)\n\n```\n\n- **text\\_output\\_types:** dict{source\\_type: list[output string]}\n\t* **source\\_type:** 搜索句子的策略 e.g. sample / beam\n\n## run\\_pred\\_iters\n\n将chunk进一步划分为一个个vid，交给run\\_pre\\_iter完成任务\n\n```py\n    # get fc7 feature for the video\n    video_features = video_to_descriptor(video_id, fsg)\n\n```\n\nvideo\\_features: list[1 \\* FEAT\\_DIM]\n\n```py\n    # run lstm on all the frames of video before predicting\n    encode_video_frames(pred_net, video_features)\n    outputs[video_id] = \\\n        run_pred_iter(pred_net, pad_img_feature, display_vocab, strategies=strategies)\n\n```\n\ncaption的流程：\n\n1. 将frame_feats encode到LSTM\n2. 从encode好的LSTM中decode出caption\n\n## encode\\_video\\_frames\n\n```py\n    net.forward(frames_fc7=image_features, cont_sentence=cont, input_sentence=data_en,\n       stage_indicator=stage_ind)\n\n```\n\n将数据格式匹配输入，将frame\\_feats依次传入LSTM，并设置input\\_sentence为0\n\n## run\\_pred\\_iter\n\n```py\n    captions, probs = predict_image_caption(net, pad_image_feature, vocab_list, strategy=strategy)\n```\n\n对每种strategy都调用predict\\_image\\_caption来进行caption\n\n## predict\\_image\\_caption\n\n根据strategy选择对应的image\\_caption函数进行caption\n\n## predict\\_image\\_caption\\_beam\\_search\n\n使用beam\\_search的方式生成句子。所谓的beam\\_search实际上为启发式的BFS，使用了贪心的思想：即每次搜索完下一层后，只在下一层中取beam\\_size个结点进一步展开，而其余的结点则不要\n\n```py\n  beam_size = 1\t#每层保留结点数\n  beams = [[]]\t#beams: list[sentence], sentence: list[word]\n  beams_complete = 0\t#当前层有多少beam时已经eos了\n  beam_probs = [[]]\t#粒度为单词，对应beams中每个单词出现的概率\n  beam_log_probs = [0.]\t#粒度为beam(i.e. 句子)，对应beams中每个beam出现的概率(log w1*w2*w3...)\n  current_input_word = 0  # first input is EOS\n  while beams_complete < len(beams):\n    # expansions: append a new word to current beams\n    expansions = []\t#记录了下一层单词的信息\n\n\t  #每一个单词的信息如下\n      # extension : the new word\n      exp = {'prefix_beam_index': beam_index, 'extension': [ind],\n             'prob_extension': [prob], 'log_prob': extended_beam_log_prob}\n      expansions.append(exp)\n\n      #prefix_beam_index: 这个单词是由哪条beam生成出来的\n      #extension: 这个单词在字典中的index\n      #prob_extension: 产生的是这个单词的概率\n      #log_prob: 加上这个单词后，beam的概率log w1*w2*w3...*w_extension\n\n```\n\n了解了数据结构后，先看内层循环\n\n```py\n    #       0       p_w1, p_w2...  w1, w2...\n    for beam_index, beam_log_prob, beam in \\\n        zip(range(len(beams)), beam_log_probs, beams):\n      if beam:\n        previous_word = beam[-1]\n        if len(beam) >= max_length or previous_word == 0:\n          exp = {'prefix_beam_index': beam_index, 'extension': [],\n                 'prob_extension': [], 'log_prob': beam_log_prob}\n          expansions.append(exp)\n          # Don't expand this beam; it was already ended with an EOS,\n          # or is the max length.\n          continue\n      else:\n        previous_word = 0  # EOS is first word\n      if beam_size == 1:\n        probs = predict_single_word(net, pad_img_feature, previous_word)\n      else:\n        probs = predict_single_word_from_all_previous(net, pad_img_feature, beam)\n      assert len(probs.shape) == 1\n      assert probs.shape[0] == len(vocab_list)\n      # index of top beam_size prob words\n      expansion_inds = probs.argsort()[-beam_size:]\n      for ind in expansion_inds:\n        prob = probs[ind]\n        extended_beam_log_prob = beam_log_prob + math.log(prob)\n        # extension : the new word\n        exp = {'prefix_beam_index': beam_index, 'extension': [ind],\n               'prob_extension': [prob], 'log_prob': extended_beam_log_prob}\n        expansions.append(exp)\n\n```\n\n将每条beam生成的单词中，概率最高的beam\\_size个单词保存到expansions中。内层循环结束后，expansions中含有len(beams) * beam\\_size个单词的信息。接下来看外层循环\n\n```py\n  while beams_complete < len(beams):\n    # expansions: append a new word to current beams\n    expansions = []\n\n  \t#内层循环\n\n    # Sort expansions in decreasing order of probabilitf.\n    expansions.sort(key=lambda expansion: -1 * expansion['log_prob'])\n    # only reserve beam_size number of node in each BFS layer\n    expansions = expansions[:beam_size]\n    new_beams = \\\n        [beams[e['prefix_beam_index']] + e['extension'] for e in expansions]\n    new_beam_probs = \\\n        [beam_probs[e['prefix_beam_index']] + e['prob_extension'] for e in expansions]\n    beam_log_probs = [e['log_prob'] for e in expansions]\n    beams_complete = 0\n    for beam in new_beams:\n      if beam[-1] == 0 or len(beam) >= max_length: beams_complete += 1\n    beams, beam_probs = new_beams, new_beam_probs\n\n```\n\n保留expansions中概率最高的beam\\_size个单词，并用这bean\\_size个单词扩展beams，得到size为beam\\_size的new_beams。一直循环，直到beams中所有beam都结束\n\n## predict\\_single\\_word\n\n同理encode\\_video\\_frame，不过这里以pad作为frames_fc7的输入\n\n## 关于caffemodel参数不匹配的问题\n\n可以看到\n\n- 在使用s2vt\\_captioner.py进行**test**的时候，网络的模型是**没有展开的**，然后通过将每次的output作为下次的input来传递c和h\n- 然而，在**train**的时候，可以看到网络的模型是将LSTM**展开的**\n- 显然，展开的LSTM的参数会比不展开的**参数要多得多**\n- 那为什么还能用**train出来的caffemodel给test用呢？**\n\n原因就在于，train的时候，展开的各部分实际上是share同一份参数的。而实现参数共享的trick就在我一开始想不明白有什么用的`add_param()->set_name()`\n\n```cpp\n  //lstm_layer.cpp\n\n  // Add layer to transform all timesteps of x to the hidden state dimension.\n  //     W_xc_x = W_xc * x + b_c\n  //{\n  //  LayerParameter* x_transform_param = net_param->add_layer();\n  //  x_transform_param->CopyFrom(biased_hidden_param);\n  //  x_transform_param->set_name(\"x_transform\");\n    x_transform_param->add_param()->set_name(\"W_xc\");\n    x_transform_param->add_param()->set_name(\"b_c\");\n  //  x_transform_param->add_bottom(\"x\");\n  //  x_transform_param->add_top(\"W_xc_x\");\n  //}\n\n  // Add layer to compute\n  //     W_hc_h_{t-1} := W_hc * h_conted_{t-1}\n  //{\n  //  LayerParameter* w_param = net_param->add_layer();\n  //  w_param->CopyFrom(hidden_param);\n  //  w_param->set_name(\"transform_\" + ts);\n    w_param->add_param()->set_name(\"W_hc\");\n  //  w_param->add_bottom(\"h_conted_\" + tm1s);\n  //  w_param->add_top(\"W_hc_h_\" + tm1s);\n  //  w_param->mutable_inner_product_param()->set_axis(2);\n  //}\n\n```\n\n再从caffe.proto中看这个param参数的含义\n\n```proto\n//caffe.proto\n\nmessage LayerParameter {\n  // Specifies training parameters (multipliers on global learning constants,\n  // and the name and other settings used for weight sharing).\n  repeated ParamSpec param = 6;\n}\n\n```\n\n所以说，所有参数都是共享同一份的\n\n## 收获\n\n- 在脚本迭代更新，备份上一份脚本时，**备份命名要有意义**，不要只加个.bak就算了，回去一个周末就忘记了.bak1, .bak2, .bak3是哪个跟哪个了\n- debug的时候还需要考虑**环境变量**，比如PYTHONPATH这种，特别是两份相同的代码在两台机子上跑出不一样的结果\n","slug":"caffe_5_s2vt_captioner","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrpf000axuvt92ohs6dt","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>s2vt_captioner.py是使用之前训练出来的s2vt model进行caption的脚本。整体逻辑跟训练的时候差不多，但还是有一些区别，比如</p>\n<ul>\n<li>decode时候LSTM2的x不再是gt，而是上一个unit生成的单词</li>\n<li>使用了python的接口调用caffe</li>\n</ul>\n<p>下面按照脚本的核心函数调用链来展开。</p>\n<h2 id=\"核心函数调用链\"><a href=\"#核心函数调用链\" class=\"headerlink\" title=\"核心函数调用链\"></a>核心函数调用链</h2><ul>\n<li><strong>main:</strong> 划分为vid chunks<ul>\n<li><strong>run_pred_iters:</strong> 划分为一个个vid<ul>\n<li><strong>encode_video_frames:</strong> 使用vid的feats进行encode</li>\n<li><strong>run_pred_iter:</strong> decode一个vid，返回captions（有可能多个）<ul>\n<li><strong>predict_image_caption:</strong> 根据caption策略选择对应的caption方式<ul>\n<li><strong>predict_image_caption_beam_search:</strong> 使用beam search的方式进行caption<ul>\n<li><strong>predict_single_word:</strong> 生成下一个单词</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"main\"><a href=\"#main\" class=\"headerlink\" title=\"main\"></a>main</h2><p>将任务划分为一个个chunk，交给run_pred_iters完成任务</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE,</div><div class=\"line\">      vocab_file, max_words=MAX_WORDS, align=aligned, shuffle=<span class=\"keyword\">False</span>,</div><div class=\"line\">      pad=aligned, truncate=aligned)</div><div class=\"line\">video_gt_pairs = all_video_gt_pairs(fsg)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>video_gt_pairs:</strong> dict{vid: list[gt_sents]}<ul>\n<li>假若没有给gt的话，list[gt_sents]为空</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">outputs = run_pred_iters(lstm_net, chunk, video_gt_pairs,</div><div class=\"line\">              fsg, strategies=STRATEGIES, display_vocab=vocab_list)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>outputs:</strong> dict{vid: output_batch}<ul>\n<li>每个vid有可能有多于一个caption，所以是output_batch</li>\n</ul>\n</li>\n<li><strong>output_batch:</strong> list[dict{caption, prob, gt, source}]<ul>\n<li><strong>caption:</strong> 预测的句子，list[word1, word2, word3, …]</li>\n<li><strong>prob:</strong> 句子各单词发生的概率，list[prob_word1, prob_word2, prob_word3, …]</li>\n<li><strong>source:</strong> 搜索句子的策略 e.g. sample / beam</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">text_out_types = to_text_output(outputs, vocab_list)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>text_output_types:</strong> dict{source_type: list[output string]}<ul>\n<li><strong>source_type:</strong> 搜索句子的策略 e.g. sample / beam</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"run-pred-iters\"><a href=\"#run-pred-iters\" class=\"headerlink\" title=\"run_pred_iters\"></a>run_pred_iters</h2><p>将chunk进一步划分为一个个vid，交给run_pre_iter完成任务</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># get fc7 feature for the video</span></div><div class=\"line\">video_features = video_to_descriptor(video_id, fsg)</div></pre></td></tr></table></figure>\n<p>video_features: list[1 * FEAT_DIM]</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># run lstm on all the frames of video before predicting</span></div><div class=\"line\">encode_video_frames(pred_net, video_features)</div><div class=\"line\">outputs[video_id] = \\</div><div class=\"line\">    run_pred_iter(pred_net, pad_img_feature, display_vocab, strategies=strategies)</div></pre></td></tr></table></figure>\n<p>caption的流程：</p>\n<ol>\n<li>将frame_feats encode到LSTM</li>\n<li>从encode好的LSTM中decode出caption</li>\n</ol>\n<h2 id=\"encode-video-frames\"><a href=\"#encode-video-frames\" class=\"headerlink\" title=\"encode_video_frames\"></a>encode_video_frames</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.forward(frames_fc7=image_features, cont_sentence=cont, input_sentence=data_en,</div><div class=\"line\">   stage_indicator=stage_ind)</div></pre></td></tr></table></figure>\n<p>将数据格式匹配输入，将frame_feats依次传入LSTM，并设置input_sentence为0</p>\n<h2 id=\"run-pred-iter\"><a href=\"#run-pred-iter\" class=\"headerlink\" title=\"run_pred_iter\"></a>run_pred_iter</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">captions, probs = predict_image_caption(net, pad_image_feature, vocab_list, strategy=strategy)</div></pre></td></tr></table></figure>\n<p>对每种strategy都调用predict_image_caption来进行caption</p>\n<h2 id=\"predict-image-caption\"><a href=\"#predict-image-caption\" class=\"headerlink\" title=\"predict_image_caption\"></a>predict_image_caption</h2><p>根据strategy选择对应的image_caption函数进行caption</p>\n<h2 id=\"predict-image-caption-beam-search\"><a href=\"#predict-image-caption-beam-search\" class=\"headerlink\" title=\"predict_image_caption_beam_search\"></a>predict_image_caption_beam_search</h2><p>使用beam_search的方式生成句子。所谓的beam_search实际上为启发式的BFS，使用了贪心的思想：即每次搜索完下一层后，只在下一层中取beam_size个结点进一步展开，而其余的结点则不要</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">beam_size = <span class=\"number\">1</span>\t<span class=\"comment\">#每层保留结点数</span></div><div class=\"line\">beams = [[]]\t<span class=\"comment\">#beams: list[sentence], sentence: list[word]</span></div><div class=\"line\">beams_complete = <span class=\"number\">0</span>\t<span class=\"comment\">#当前层有多少beam时已经eos了</span></div><div class=\"line\">beam_probs = [[]]\t<span class=\"comment\">#粒度为单词，对应beams中每个单词出现的概率</span></div><div class=\"line\">beam_log_probs = [<span class=\"number\">0.</span>]\t<span class=\"comment\">#粒度为beam(i.e. 句子)，对应beams中每个beam出现的概率(log w1*w2*w3...)</span></div><div class=\"line\">current_input_word = <span class=\"number\">0</span>  <span class=\"comment\"># first input is EOS</span></div><div class=\"line\"><span class=\"keyword\">while</span> beams_complete &lt; len(beams):</div><div class=\"line\">  <span class=\"comment\"># expansions: append a new word to current beams</span></div><div class=\"line\">  expansions = []\t<span class=\"comment\">#记录了下一层单词的信息</span></div><div class=\"line\"></div><div class=\"line\"> <span class=\"comment\">#每一个单词的信息如下</span></div><div class=\"line\">    <span class=\"comment\"># extension : the new word</span></div><div class=\"line\">    exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [ind],</div><div class=\"line\">           <span class=\"string\">'prob_extension'</span>: [prob], <span class=\"string\">'log_prob'</span>: extended_beam_log_prob&#125;</div><div class=\"line\">    expansions.append(exp)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">#prefix_beam_index: 这个单词是由哪条beam生成出来的</span></div><div class=\"line\">    <span class=\"comment\">#extension: 这个单词在字典中的index</span></div><div class=\"line\">    <span class=\"comment\">#prob_extension: 产生的是这个单词的概率</span></div><div class=\"line\">    <span class=\"comment\">#log_prob: 加上这个单词后，beam的概率log w1*w2*w3...*w_extension</span></div></pre></td></tr></table></figure>\n<p>了解了数据结构后，先看内层循环</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">#       0       p_w1, p_w2...  w1, w2...</span></div><div class=\"line\"><span class=\"keyword\">for</span> beam_index, beam_log_prob, beam <span class=\"keyword\">in</span> \\</div><div class=\"line\">    zip(range(len(beams)), beam_log_probs, beams):</div><div class=\"line\">  <span class=\"keyword\">if</span> beam:</div><div class=\"line\">    previous_word = beam[<span class=\"number\">-1</span>]</div><div class=\"line\">    <span class=\"keyword\">if</span> len(beam) &gt;= max_length <span class=\"keyword\">or</span> previous_word == <span class=\"number\">0</span>:</div><div class=\"line\">      exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [],</div><div class=\"line\">             <span class=\"string\">'prob_extension'</span>: [], <span class=\"string\">'log_prob'</span>: beam_log_prob&#125;</div><div class=\"line\">      expansions.append(exp)</div><div class=\"line\">      <span class=\"comment\"># Don't expand this beam; it was already ended with an EOS,</span></div><div class=\"line\">      <span class=\"comment\"># or is the max length.</span></div><div class=\"line\">      <span class=\"keyword\">continue</span></div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    previous_word = <span class=\"number\">0</span>  <span class=\"comment\"># EOS is first word</span></div><div class=\"line\">  <span class=\"keyword\">if</span> beam_size == <span class=\"number\">1</span>:</div><div class=\"line\">    probs = predict_single_word(net, pad_img_feature, previous_word)</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    probs = predict_single_word_from_all_previous(net, pad_img_feature, beam)</div><div class=\"line\">  <span class=\"keyword\">assert</span> len(probs.shape) == <span class=\"number\">1</span></div><div class=\"line\">  <span class=\"keyword\">assert</span> probs.shape[<span class=\"number\">0</span>] == len(vocab_list)</div><div class=\"line\">  <span class=\"comment\"># index of top beam_size prob words</span></div><div class=\"line\">  expansion_inds = probs.argsort()[-beam_size:]</div><div class=\"line\">  <span class=\"keyword\">for</span> ind <span class=\"keyword\">in</span> expansion_inds:</div><div class=\"line\">    prob = probs[ind]</div><div class=\"line\">    extended_beam_log_prob = beam_log_prob + math.log(prob)</div><div class=\"line\">    <span class=\"comment\"># extension : the new word</span></div><div class=\"line\">    exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [ind],</div><div class=\"line\">           <span class=\"string\">'prob_extension'</span>: [prob], <span class=\"string\">'log_prob'</span>: extended_beam_log_prob&#125;</div><div class=\"line\">    expansions.append(exp)</div></pre></td></tr></table></figure>\n<p>将每条beam生成的单词中，概率最高的beam_size个单词保存到expansions中。内层循环结束后，expansions中含有len(beams) * beam_size个单词的信息。接下来看外层循环</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> beams_complete &lt; len(beams):</div><div class=\"line\">  <span class=\"comment\"># expansions: append a new word to current beams</span></div><div class=\"line\">  expansions = []</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">#内层循环</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\"># Sort expansions in decreasing order of probabilitf.</span></div><div class=\"line\">  expansions.sort(key=<span class=\"keyword\">lambda</span> expansion: <span class=\"number\">-1</span> * expansion[<span class=\"string\">'log_prob'</span>])</div><div class=\"line\">  <span class=\"comment\"># only reserve beam_size number of node in each BFS layer</span></div><div class=\"line\">  expansions = expansions[:beam_size]</div><div class=\"line\">  new_beams = \\</div><div class=\"line\">      [beams[e[<span class=\"string\">'prefix_beam_index'</span>]] + e[<span class=\"string\">'extension'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  new_beam_probs = \\</div><div class=\"line\">      [beam_probs[e[<span class=\"string\">'prefix_beam_index'</span>]] + e[<span class=\"string\">'prob_extension'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  beam_log_probs = [e[<span class=\"string\">'log_prob'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  beams_complete = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">for</span> beam <span class=\"keyword\">in</span> new_beams:</div><div class=\"line\">    <span class=\"keyword\">if</span> beam[<span class=\"number\">-1</span>] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> len(beam) &gt;= max_length: beams_complete += <span class=\"number\">1</span></div><div class=\"line\">  beams, beam_probs = new_beams, new_beam_probs</div></pre></td></tr></table></figure>\n<p>保留expansions中概率最高的beam_size个单词，并用这bean_size个单词扩展beams，得到size为beam_size的new_beams。一直循环，直到beams中所有beam都结束</p>\n<h2 id=\"predict-single-word\"><a href=\"#predict-single-word\" class=\"headerlink\" title=\"predict_single_word\"></a>predict_single_word</h2><p>同理encode_video_frame，不过这里以pad作为frames_fc7的输入</p>\n<h2 id=\"关于caffemodel参数不匹配的问题\"><a href=\"#关于caffemodel参数不匹配的问题\" class=\"headerlink\" title=\"关于caffemodel参数不匹配的问题\"></a>关于caffemodel参数不匹配的问题</h2><p>可以看到</p>\n<ul>\n<li>在使用s2vt_captioner.py进行<strong>test</strong>的时候，网络的模型是<strong>没有展开的</strong>，然后通过将每次的output作为下次的input来传递c和h</li>\n<li>然而，在<strong>train</strong>的时候，可以看到网络的模型是将LSTM<strong>展开的</strong></li>\n<li>显然，展开的LSTM的参数会比不展开的<strong>参数要多得多</strong></li>\n<li>那为什么还能用<strong>train出来的caffemodel给test用呢？</strong></li>\n</ul>\n<p>原因就在于，train的时候，展开的各部分实际上是share同一份参数的。而实现参数共享的trick就在我一开始想不明白有什么用的<code>add_param()-&gt;set_name()</code></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//lstm_layer.cpp</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Add layer to transform all timesteps of x to the hidden state dimension.</span></div><div class=\"line\"><span class=\"comment\">//     W_xc_x = W_xc * x + b_c</span></div><div class=\"line\"><span class=\"comment\">//&#123;</span></div><div class=\"line\"><span class=\"comment\">//  LayerParameter* x_transform_param = net_param-&gt;add_layer();</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;CopyFrom(biased_hidden_param);</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;set_name(\"x_transform\");</span></div><div class=\"line\">  x_transform_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_xc\"</span>);</div><div class=\"line\">  x_transform_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"b_c\"</span>);</div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;add_bottom(\"x\");</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;add_top(\"W_xc_x\");</span></div><div class=\"line\"><span class=\"comment\">//&#125;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Add layer to compute</span></div><div class=\"line\"><span class=\"comment\">//     W_hc_h_&#123;t-1&#125; := W_hc * h_conted_&#123;t-1&#125;</span></div><div class=\"line\"><span class=\"comment\">//&#123;</span></div><div class=\"line\"><span class=\"comment\">//  LayerParameter* w_param = net_param-&gt;add_layer();</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;CopyFrom(hidden_param);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;set_name(\"transform_\" + ts);</span></div><div class=\"line\">  w_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_hc\"</span>);</div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;add_bottom(\"h_conted_\" + tm1s);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;add_top(\"W_hc_h_\" + tm1s);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;mutable_inner_product_param()-&gt;set_axis(2);</span></div><div class=\"line\"><span class=\"comment\">//&#125;</span></div></pre></td></tr></table></figure>\n<p>再从caffe.proto中看这个param参数的含义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">//caffe.proto</div><div class=\"line\"></div><div class=\"line\">message LayerParameter &#123;</div><div class=\"line\">  // Specifies training parameters (multipliers on global learning constants,</div><div class=\"line\">  // and the name and other settings used for weight sharing).</div><div class=\"line\">  repeated ParamSpec param = 6;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>所以说，所有参数都是共享同一份的</p>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>在脚本迭代更新，备份上一份脚本时，<strong>备份命名要有意义</strong>，不要只加个.bak就算了，回去一个周末就忘记了.bak1, .bak2, .bak3是哪个跟哪个了</li>\n<li>debug的时候还需要考虑<strong>环境变量</strong>，比如PYTHONPATH这种，特别是两份相同的代码在两台机子上跑出不一样的结果</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>s2vt_captioner.py是使用之前训练出来的s2vt model进行caption的脚本。整体逻辑跟训练的时候差不多，但还是有一些区别，比如</p>\n<ul>\n<li>decode时候LSTM2的x不再是gt，而是上一个unit生成的单词</li>\n<li>使用了python的接口调用caffe</li>\n</ul>\n<p>下面按照脚本的核心函数调用链来展开。</p>\n<h2 id=\"核心函数调用链\"><a href=\"#核心函数调用链\" class=\"headerlink\" title=\"核心函数调用链\"></a>核心函数调用链</h2><ul>\n<li><strong>main:</strong> 划分为vid chunks<ul>\n<li><strong>run_pred_iters:</strong> 划分为一个个vid<ul>\n<li><strong>encode_video_frames:</strong> 使用vid的feats进行encode</li>\n<li><strong>run_pred_iter:</strong> decode一个vid，返回captions（有可能多个）<ul>\n<li><strong>predict_image_caption:</strong> 根据caption策略选择对应的caption方式<ul>\n<li><strong>predict_image_caption_beam_search:</strong> 使用beam search的方式进行caption<ul>\n<li><strong>predict_single_word:</strong> 生成下一个单词</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"main\"><a href=\"#main\" class=\"headerlink\" title=\"main\"></a>main</h2><p>将任务划分为一个个chunk，交给run_pred_iters完成任务</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE,</div><div class=\"line\">      vocab_file, max_words=MAX_WORDS, align=aligned, shuffle=<span class=\"keyword\">False</span>,</div><div class=\"line\">      pad=aligned, truncate=aligned)</div><div class=\"line\">video_gt_pairs = all_video_gt_pairs(fsg)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>video_gt_pairs:</strong> dict{vid: list[gt_sents]}<ul>\n<li>假若没有给gt的话，list[gt_sents]为空</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">outputs = run_pred_iters(lstm_net, chunk, video_gt_pairs,</div><div class=\"line\">              fsg, strategies=STRATEGIES, display_vocab=vocab_list)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>outputs:</strong> dict{vid: output_batch}<ul>\n<li>每个vid有可能有多于一个caption，所以是output_batch</li>\n</ul>\n</li>\n<li><strong>output_batch:</strong> list[dict{caption, prob, gt, source}]<ul>\n<li><strong>caption:</strong> 预测的句子，list[word1, word2, word3, …]</li>\n<li><strong>prob:</strong> 句子各单词发生的概率，list[prob_word1, prob_word2, prob_word3, …]</li>\n<li><strong>source:</strong> 搜索句子的策略 e.g. sample / beam</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">text_out_types = to_text_output(outputs, vocab_list)</div></pre></td></tr></table></figure>\n<ul>\n<li><strong>text_output_types:</strong> dict{source_type: list[output string]}<ul>\n<li><strong>source_type:</strong> 搜索句子的策略 e.g. sample / beam</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"run-pred-iters\"><a href=\"#run-pred-iters\" class=\"headerlink\" title=\"run_pred_iters\"></a>run_pred_iters</h2><p>将chunk进一步划分为一个个vid，交给run_pre_iter完成任务</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># get fc7 feature for the video</span></div><div class=\"line\">video_features = video_to_descriptor(video_id, fsg)</div></pre></td></tr></table></figure>\n<p>video_features: list[1 * FEAT_DIM]</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># run lstm on all the frames of video before predicting</span></div><div class=\"line\">encode_video_frames(pred_net, video_features)</div><div class=\"line\">outputs[video_id] = \\</div><div class=\"line\">    run_pred_iter(pred_net, pad_img_feature, display_vocab, strategies=strategies)</div></pre></td></tr></table></figure>\n<p>caption的流程：</p>\n<ol>\n<li>将frame_feats encode到LSTM</li>\n<li>从encode好的LSTM中decode出caption</li>\n</ol>\n<h2 id=\"encode-video-frames\"><a href=\"#encode-video-frames\" class=\"headerlink\" title=\"encode_video_frames\"></a>encode_video_frames</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.forward(frames_fc7=image_features, cont_sentence=cont, input_sentence=data_en,</div><div class=\"line\">   stage_indicator=stage_ind)</div></pre></td></tr></table></figure>\n<p>将数据格式匹配输入，将frame_feats依次传入LSTM，并设置input_sentence为0</p>\n<h2 id=\"run-pred-iter\"><a href=\"#run-pred-iter\" class=\"headerlink\" title=\"run_pred_iter\"></a>run_pred_iter</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">captions, probs = predict_image_caption(net, pad_image_feature, vocab_list, strategy=strategy)</div></pre></td></tr></table></figure>\n<p>对每种strategy都调用predict_image_caption来进行caption</p>\n<h2 id=\"predict-image-caption\"><a href=\"#predict-image-caption\" class=\"headerlink\" title=\"predict_image_caption\"></a>predict_image_caption</h2><p>根据strategy选择对应的image_caption函数进行caption</p>\n<h2 id=\"predict-image-caption-beam-search\"><a href=\"#predict-image-caption-beam-search\" class=\"headerlink\" title=\"predict_image_caption_beam_search\"></a>predict_image_caption_beam_search</h2><p>使用beam_search的方式生成句子。所谓的beam_search实际上为启发式的BFS，使用了贪心的思想：即每次搜索完下一层后，只在下一层中取beam_size个结点进一步展开，而其余的结点则不要</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">beam_size = <span class=\"number\">1</span>\t<span class=\"comment\">#每层保留结点数</span></div><div class=\"line\">beams = [[]]\t<span class=\"comment\">#beams: list[sentence], sentence: list[word]</span></div><div class=\"line\">beams_complete = <span class=\"number\">0</span>\t<span class=\"comment\">#当前层有多少beam时已经eos了</span></div><div class=\"line\">beam_probs = [[]]\t<span class=\"comment\">#粒度为单词，对应beams中每个单词出现的概率</span></div><div class=\"line\">beam_log_probs = [<span class=\"number\">0.</span>]\t<span class=\"comment\">#粒度为beam(i.e. 句子)，对应beams中每个beam出现的概率(log w1*w2*w3...)</span></div><div class=\"line\">current_input_word = <span class=\"number\">0</span>  <span class=\"comment\"># first input is EOS</span></div><div class=\"line\"><span class=\"keyword\">while</span> beams_complete &lt; len(beams):</div><div class=\"line\">  <span class=\"comment\"># expansions: append a new word to current beams</span></div><div class=\"line\">  expansions = []\t<span class=\"comment\">#记录了下一层单词的信息</span></div><div class=\"line\"></div><div class=\"line\"> <span class=\"comment\">#每一个单词的信息如下</span></div><div class=\"line\">    <span class=\"comment\"># extension : the new word</span></div><div class=\"line\">    exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [ind],</div><div class=\"line\">           <span class=\"string\">'prob_extension'</span>: [prob], <span class=\"string\">'log_prob'</span>: extended_beam_log_prob&#125;</div><div class=\"line\">    expansions.append(exp)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">#prefix_beam_index: 这个单词是由哪条beam生成出来的</span></div><div class=\"line\">    <span class=\"comment\">#extension: 这个单词在字典中的index</span></div><div class=\"line\">    <span class=\"comment\">#prob_extension: 产生的是这个单词的概率</span></div><div class=\"line\">    <span class=\"comment\">#log_prob: 加上这个单词后，beam的概率log w1*w2*w3...*w_extension</span></div></pre></td></tr></table></figure>\n<p>了解了数据结构后，先看内层循环</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">#       0       p_w1, p_w2...  w1, w2...</span></div><div class=\"line\"><span class=\"keyword\">for</span> beam_index, beam_log_prob, beam <span class=\"keyword\">in</span> \\</div><div class=\"line\">    zip(range(len(beams)), beam_log_probs, beams):</div><div class=\"line\">  <span class=\"keyword\">if</span> beam:</div><div class=\"line\">    previous_word = beam[<span class=\"number\">-1</span>]</div><div class=\"line\">    <span class=\"keyword\">if</span> len(beam) &gt;= max_length <span class=\"keyword\">or</span> previous_word == <span class=\"number\">0</span>:</div><div class=\"line\">      exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [],</div><div class=\"line\">             <span class=\"string\">'prob_extension'</span>: [], <span class=\"string\">'log_prob'</span>: beam_log_prob&#125;</div><div class=\"line\">      expansions.append(exp)</div><div class=\"line\">      <span class=\"comment\"># Don't expand this beam; it was already ended with an EOS,</span></div><div class=\"line\">      <span class=\"comment\"># or is the max length.</span></div><div class=\"line\">      <span class=\"keyword\">continue</span></div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    previous_word = <span class=\"number\">0</span>  <span class=\"comment\"># EOS is first word</span></div><div class=\"line\">  <span class=\"keyword\">if</span> beam_size == <span class=\"number\">1</span>:</div><div class=\"line\">    probs = predict_single_word(net, pad_img_feature, previous_word)</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    probs = predict_single_word_from_all_previous(net, pad_img_feature, beam)</div><div class=\"line\">  <span class=\"keyword\">assert</span> len(probs.shape) == <span class=\"number\">1</span></div><div class=\"line\">  <span class=\"keyword\">assert</span> probs.shape[<span class=\"number\">0</span>] == len(vocab_list)</div><div class=\"line\">  <span class=\"comment\"># index of top beam_size prob words</span></div><div class=\"line\">  expansion_inds = probs.argsort()[-beam_size:]</div><div class=\"line\">  <span class=\"keyword\">for</span> ind <span class=\"keyword\">in</span> expansion_inds:</div><div class=\"line\">    prob = probs[ind]</div><div class=\"line\">    extended_beam_log_prob = beam_log_prob + math.log(prob)</div><div class=\"line\">    <span class=\"comment\"># extension : the new word</span></div><div class=\"line\">    exp = &#123;<span class=\"string\">'prefix_beam_index'</span>: beam_index, <span class=\"string\">'extension'</span>: [ind],</div><div class=\"line\">           <span class=\"string\">'prob_extension'</span>: [prob], <span class=\"string\">'log_prob'</span>: extended_beam_log_prob&#125;</div><div class=\"line\">    expansions.append(exp)</div></pre></td></tr></table></figure>\n<p>将每条beam生成的单词中，概率最高的beam_size个单词保存到expansions中。内层循环结束后，expansions中含有len(beams) * beam_size个单词的信息。接下来看外层循环</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> beams_complete &lt; len(beams):</div><div class=\"line\">  <span class=\"comment\"># expansions: append a new word to current beams</span></div><div class=\"line\">  expansions = []</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">#内层循环</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\"># Sort expansions in decreasing order of probabilitf.</span></div><div class=\"line\">  expansions.sort(key=<span class=\"keyword\">lambda</span> expansion: <span class=\"number\">-1</span> * expansion[<span class=\"string\">'log_prob'</span>])</div><div class=\"line\">  <span class=\"comment\"># only reserve beam_size number of node in each BFS layer</span></div><div class=\"line\">  expansions = expansions[:beam_size]</div><div class=\"line\">  new_beams = \\</div><div class=\"line\">      [beams[e[<span class=\"string\">'prefix_beam_index'</span>]] + e[<span class=\"string\">'extension'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  new_beam_probs = \\</div><div class=\"line\">      [beam_probs[e[<span class=\"string\">'prefix_beam_index'</span>]] + e[<span class=\"string\">'prob_extension'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  beam_log_probs = [e[<span class=\"string\">'log_prob'</span>] <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> expansions]</div><div class=\"line\">  beams_complete = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">for</span> beam <span class=\"keyword\">in</span> new_beams:</div><div class=\"line\">    <span class=\"keyword\">if</span> beam[<span class=\"number\">-1</span>] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> len(beam) &gt;= max_length: beams_complete += <span class=\"number\">1</span></div><div class=\"line\">  beams, beam_probs = new_beams, new_beam_probs</div></pre></td></tr></table></figure>\n<p>保留expansions中概率最高的beam_size个单词，并用这bean_size个单词扩展beams，得到size为beam_size的new_beams。一直循环，直到beams中所有beam都结束</p>\n<h2 id=\"predict-single-word\"><a href=\"#predict-single-word\" class=\"headerlink\" title=\"predict_single_word\"></a>predict_single_word</h2><p>同理encode_video_frame，不过这里以pad作为frames_fc7的输入</p>\n<h2 id=\"关于caffemodel参数不匹配的问题\"><a href=\"#关于caffemodel参数不匹配的问题\" class=\"headerlink\" title=\"关于caffemodel参数不匹配的问题\"></a>关于caffemodel参数不匹配的问题</h2><p>可以看到</p>\n<ul>\n<li>在使用s2vt_captioner.py进行<strong>test</strong>的时候，网络的模型是<strong>没有展开的</strong>，然后通过将每次的output作为下次的input来传递c和h</li>\n<li>然而，在<strong>train</strong>的时候，可以看到网络的模型是将LSTM<strong>展开的</strong></li>\n<li>显然，展开的LSTM的参数会比不展开的<strong>参数要多得多</strong></li>\n<li>那为什么还能用<strong>train出来的caffemodel给test用呢？</strong></li>\n</ul>\n<p>原因就在于，train的时候，展开的各部分实际上是share同一份参数的。而实现参数共享的trick就在我一开始想不明白有什么用的<code>add_param()-&gt;set_name()</code></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//lstm_layer.cpp</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Add layer to transform all timesteps of x to the hidden state dimension.</span></div><div class=\"line\"><span class=\"comment\">//     W_xc_x = W_xc * x + b_c</span></div><div class=\"line\"><span class=\"comment\">//&#123;</span></div><div class=\"line\"><span class=\"comment\">//  LayerParameter* x_transform_param = net_param-&gt;add_layer();</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;CopyFrom(biased_hidden_param);</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;set_name(\"x_transform\");</span></div><div class=\"line\">  x_transform_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_xc\"</span>);</div><div class=\"line\">  x_transform_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"b_c\"</span>);</div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;add_bottom(\"x\");</span></div><div class=\"line\"><span class=\"comment\">//  x_transform_param-&gt;add_top(\"W_xc_x\");</span></div><div class=\"line\"><span class=\"comment\">//&#125;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Add layer to compute</span></div><div class=\"line\"><span class=\"comment\">//     W_hc_h_&#123;t-1&#125; := W_hc * h_conted_&#123;t-1&#125;</span></div><div class=\"line\"><span class=\"comment\">//&#123;</span></div><div class=\"line\"><span class=\"comment\">//  LayerParameter* w_param = net_param-&gt;add_layer();</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;CopyFrom(hidden_param);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;set_name(\"transform_\" + ts);</span></div><div class=\"line\">  w_param-&gt;add_param()-&gt;set_name(<span class=\"string\">\"W_hc\"</span>);</div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;add_bottom(\"h_conted_\" + tm1s);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;add_top(\"W_hc_h_\" + tm1s);</span></div><div class=\"line\"><span class=\"comment\">//  w_param-&gt;mutable_inner_product_param()-&gt;set_axis(2);</span></div><div class=\"line\"><span class=\"comment\">//&#125;</span></div></pre></td></tr></table></figure>\n<p>再从caffe.proto中看这个param参数的含义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">//caffe.proto</div><div class=\"line\"></div><div class=\"line\">message LayerParameter &#123;</div><div class=\"line\">  // Specifies training parameters (multipliers on global learning constants,</div><div class=\"line\">  // and the name and other settings used for weight sharing).</div><div class=\"line\">  repeated ParamSpec param = 6;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>所以说，所有参数都是共享同一份的</p>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>在脚本迭代更新，备份上一份脚本时，<strong>备份命名要有意义</strong>，不要只加个.bak就算了，回去一个周末就忘记了.bak1, .bak2, .bak3是哪个跟哪个了</li>\n<li>debug的时候还需要考虑<strong>环境变量</strong>，比如PYTHONPATH这种，特别是两份相同的代码在两台机子上跑出不一样的结果</li>\n</ul>\n"},{"title":"论文笔记《ImageNet Classification with Deep Convolutional Neural Networks》","date":"2016-07-01T11:10:02.000Z","description":["AlexNet介绍，包括其架构，如何加快收敛，怎样加强泛化能力以及如何评估模型"],"_content":"\n第一次阅读论文，没有什么阅读技巧，再加上对CNN[(CNN解析)](http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi)没有什么概念，读了几次才大概明白讲的是什么。\n\n## The Architecture\n\n1. **ReLU Nonlinearity**\n\t- **目的：** 加快收敛速度\n\t- **手段：** 使用Rectified Linear Units(ReLUs)作为神经元。即采用$$f(x) = max(0, x)$$作为激活函数\n\t- **原理：** 一般使用的激活函数$$f(x) = tanh(x) \\\\\\\\ f(x) = \\frac{1}{1 + e^{-x} }$$具有饱和(saturating)的特性，即导数在越接近目标的时候越小[(原理参考)](http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/)。当使用梯度下降的方法时，步长由学习率以及导数决定，导数太小会导致步长太小，收敛会慢。ReLU对于大于0部分导数恒为1，所以能加快收敛。\n\n\n2. **Training on Multiple GPUs**\n\t- **目的：** 加快训练速度\n\n3. **Local Response Normalization**\n\t- **目的：** 加强泛化能力\n\t- **手段：** 神经元受附近神经元抑制，各神经元的输出为$$b_i = \\frac{a_i}{ {(k+\\alpha\\sum{a_j^2})}^\\beta}$$其中$b_i$为神经元i的输出，$a_i$为神经元i的输入\n\t- **原理：** 模拟神经元间的抑制作用\n\n4. **Overlapping Pooling**\n\t- **目的：** 加强泛化能力\n\t- **手段：** pooling的时候stride < filter size，重叠地pooling\n\t- **原理：** 实验性结果\n\n## Reduce Overfitting\n\n1. **Data Augmentation**\n\t- **目的：** 加强泛化能力\n\t- **手段1：** 随机裁剪，将原来256×256的图片随机裁剪为224×224,并且允许水平翻转，则增加了$(256-224)^2*2$倍的样本量。（裁剪的时候一定是要连片的，不能是随机取的）\n\t- **手段2：** PCA，这个部分不太懂原理。[(原理参考)](http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/)\n\t- **原理：** 增大样本数增强泛化能力。裁剪，旋转，滤镜并不影响图片内容，但是对于机器来说就是完全不同的样本\n\n2. **Dropout**\n\t- **目的：** 加强泛化能力\n\t- **手段：** 随机disable神经元\n\t- **原理：** 隐式多模型，迫使模型要学习更robust的feature，抓住核心特征\n\n## Qulitative Evaluations\n\n1. **准确性：** 对于每张图片，给出CNN分类结果前5的类别及概率\n2. **一致性：** 对于每张照片，给出CNN提取特征相似的照片 e.g. 图片$i_1$提取的特征为$f_1$，找特征跟$f_1$相似的$i_k$，看下是不是同一个物体\n","source":"_posts/imagenet.md","raw":"---\ntitle: 论文笔记《ImageNet Classification with Deep Convolutional Neural Networks》\ndate: 2016-07-01 19:10:02\ntags: \n  - AlexNet\ndescription:\n  - AlexNet介绍，包括其架构，如何加快收敛，怎样加强泛化能力以及如何评估模型\ncategories:\n  - 论文笔记\n---\n\n第一次阅读论文，没有什么阅读技巧，再加上对CNN[(CNN解析)](http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi)没有什么概念，读了几次才大概明白讲的是什么。\n\n## The Architecture\n\n1. **ReLU Nonlinearity**\n\t- **目的：** 加快收敛速度\n\t- **手段：** 使用Rectified Linear Units(ReLUs)作为神经元。即采用$$f(x) = max(0, x)$$作为激活函数\n\t- **原理：** 一般使用的激活函数$$f(x) = tanh(x) \\\\\\\\ f(x) = \\frac{1}{1 + e^{-x} }$$具有饱和(saturating)的特性，即导数在越接近目标的时候越小[(原理参考)](http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/)。当使用梯度下降的方法时，步长由学习率以及导数决定，导数太小会导致步长太小，收敛会慢。ReLU对于大于0部分导数恒为1，所以能加快收敛。\n\n\n2. **Training on Multiple GPUs**\n\t- **目的：** 加快训练速度\n\n3. **Local Response Normalization**\n\t- **目的：** 加强泛化能力\n\t- **手段：** 神经元受附近神经元抑制，各神经元的输出为$$b_i = \\frac{a_i}{ {(k+\\alpha\\sum{a_j^2})}^\\beta}$$其中$b_i$为神经元i的输出，$a_i$为神经元i的输入\n\t- **原理：** 模拟神经元间的抑制作用\n\n4. **Overlapping Pooling**\n\t- **目的：** 加强泛化能力\n\t- **手段：** pooling的时候stride < filter size，重叠地pooling\n\t- **原理：** 实验性结果\n\n## Reduce Overfitting\n\n1. **Data Augmentation**\n\t- **目的：** 加强泛化能力\n\t- **手段1：** 随机裁剪，将原来256×256的图片随机裁剪为224×224,并且允许水平翻转，则增加了$(256-224)^2*2$倍的样本量。（裁剪的时候一定是要连片的，不能是随机取的）\n\t- **手段2：** PCA，这个部分不太懂原理。[(原理参考)](http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/)\n\t- **原理：** 增大样本数增强泛化能力。裁剪，旋转，滤镜并不影响图片内容，但是对于机器来说就是完全不同的样本\n\n2. **Dropout**\n\t- **目的：** 加强泛化能力\n\t- **手段：** 随机disable神经元\n\t- **原理：** 隐式多模型，迫使模型要学习更robust的feature，抓住核心特征\n\n## Qulitative Evaluations\n\n1. **准确性：** 对于每张图片，给出CNN分类结果前5的类别及概率\n2. **一致性：** 对于每张照片，给出CNN提取特征相似的照片 e.g. 图片$i_1$提取的特征为$f_1$，找特征跟$f_1$相似的$i_k$，看下是不是同一个物体\n","slug":"imagenet","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrph000cxuvtt1vfairk","content":"<p>第一次阅读论文，没有什么阅读技巧，再加上对CNN<a href=\"http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi\" target=\"_blank\" rel=\"external\">(CNN解析)</a>没有什么概念，读了几次才大概明白讲的是什么。</p>\n<h2 id=\"The-Architecture\"><a href=\"#The-Architecture\" class=\"headerlink\" title=\"The Architecture\"></a>The Architecture</h2><ol>\n<li><strong>ReLU Nonlinearity</strong><ul>\n<li><strong>目的：</strong> 加快收敛速度</li>\n<li><strong>手段：</strong> 使用Rectified Linear Units(ReLUs)作为神经元。即采用$$f(x) = max(0, x)$$作为激活函数</li>\n<li><strong>原理：</strong> 一般使用的激活函数$$f(x) = tanh(x) \\\\ f(x) = \\frac{1}{1 + e^{-x} }$$具有饱和(saturating)的特性，即导数在越接近目标的时候越小<a href=\"http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/\" target=\"_blank\" rel=\"external\">(原理参考)</a>。当使用梯度下降的方法时，步长由学习率以及导数决定，导数太小会导致步长太小，收敛会慢。ReLU对于大于0部分导数恒为1，所以能加快收敛。</li>\n</ul>\n</li>\n</ol>\n<ol>\n<li><p><strong>Training on Multiple GPUs</strong></p>\n<ul>\n<li><strong>目的：</strong> 加快训练速度</li>\n</ul>\n</li>\n<li><p><strong>Local Response Normalization</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> 神经元受附近神经元抑制，各神经元的输出为$$b_i = \\frac{a_i}{ {(k+\\alpha\\sum{a_j^2})}^\\beta}$$其中$b_i$为神经元i的输出，$a_i$为神经元i的输入</li>\n<li><strong>原理：</strong> 模拟神经元间的抑制作用</li>\n</ul>\n</li>\n<li><p><strong>Overlapping Pooling</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> pooling的时候stride &lt; filter size，重叠地pooling</li>\n<li><strong>原理：</strong> 实验性结果</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Reduce-Overfitting\"><a href=\"#Reduce-Overfitting\" class=\"headerlink\" title=\"Reduce Overfitting\"></a>Reduce Overfitting</h2><ol>\n<li><p><strong>Data Augmentation</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段1：</strong> 随机裁剪，将原来256×256的图片随机裁剪为224×224,并且允许水平翻转，则增加了$(256-224)^2*2$倍的样本量。（裁剪的时候一定是要连片的，不能是随机取的）</li>\n<li><strong>手段2：</strong> PCA，这个部分不太懂原理。<a href=\"http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/\" target=\"_blank\" rel=\"external\">(原理参考)</a></li>\n<li><strong>原理：</strong> 增大样本数增强泛化能力。裁剪，旋转，滤镜并不影响图片内容，但是对于机器来说就是完全不同的样本</li>\n</ul>\n</li>\n<li><p><strong>Dropout</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> 随机disable神经元</li>\n<li><strong>原理：</strong> 隐式多模型，迫使模型要学习更robust的feature，抓住核心特征</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Qulitative-Evaluations\"><a href=\"#Qulitative-Evaluations\" class=\"headerlink\" title=\"Qulitative Evaluations\"></a>Qulitative Evaluations</h2><ol>\n<li><strong>准确性：</strong> 对于每张图片，给出CNN分类结果前5的类别及概率</li>\n<li><strong>一致性：</strong> 对于每张照片，给出CNN提取特征相似的照片 e.g. 图片$i_1$提取的特征为$f_1$，找特征跟$f_1$相似的$i_k$，看下是不是同一个物体</li>\n</ol>\n","excerpt":"","more":"<p>第一次阅读论文，没有什么阅读技巧，再加上对CNN<a href=\"http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi\">(CNN解析)</a>没有什么概念，读了几次才大概明白讲的是什么。</p>\n<h2 id=\"The-Architecture\"><a href=\"#The-Architecture\" class=\"headerlink\" title=\"The Architecture\"></a>The Architecture</h2><ol>\n<li><strong>ReLU Nonlinearity</strong><ul>\n<li><strong>目的：</strong> 加快收敛速度</li>\n<li><strong>手段：</strong> 使用Rectified Linear Units(ReLUs)作为神经元。即采用$$f(x) = max(0, x)$$作为激活函数</li>\n<li><strong>原理：</strong> 一般使用的激活函数$$f(x) = tanh(x) \\\\ f(x) = \\frac{1}{1 + e^{-x} }$$具有饱和(saturating)的特性，即导数在越接近目标的时候越小<a href=\"http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/\">(原理参考)</a>。当使用梯度下降的方法时，步长由学习率以及导数决定，导数太小会导致步长太小，收敛会慢。ReLU对于大于0部分导数恒为1，所以能加快收敛。</li>\n</ul>\n</li>\n</ol>\n<ol>\n<li><p><strong>Training on Multiple GPUs</strong></p>\n<ul>\n<li><strong>目的：</strong> 加快训练速度</li>\n</ul>\n</li>\n<li><p><strong>Local Response Normalization</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> 神经元受附近神经元抑制，各神经元的输出为$$b_i = \\frac{a_i}{ {(k+\\alpha\\sum{a_j^2})}^\\beta}$$其中$b_i$为神经元i的输出，$a_i$为神经元i的输入</li>\n<li><strong>原理：</strong> 模拟神经元间的抑制作用</li>\n</ul>\n</li>\n<li><p><strong>Overlapping Pooling</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> pooling的时候stride &lt; filter size，重叠地pooling</li>\n<li><strong>原理：</strong> 实验性结果</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Reduce-Overfitting\"><a href=\"#Reduce-Overfitting\" class=\"headerlink\" title=\"Reduce Overfitting\"></a>Reduce Overfitting</h2><ol>\n<li><p><strong>Data Augmentation</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段1：</strong> 随机裁剪，将原来256×256的图片随机裁剪为224×224,并且允许水平翻转，则增加了$(256-224)^2*2$倍的样本量。（裁剪的时候一定是要连片的，不能是随机取的）</li>\n<li><strong>手段2：</strong> PCA，这个部分不太懂原理。<a href=\"http://zhangliliang.com/2014/07/01/paper-note-alexnet-nips2012/\">(原理参考)</a></li>\n<li><strong>原理：</strong> 增大样本数增强泛化能力。裁剪，旋转，滤镜并不影响图片内容，但是对于机器来说就是完全不同的样本</li>\n</ul>\n</li>\n<li><p><strong>Dropout</strong></p>\n<ul>\n<li><strong>目的：</strong> 加强泛化能力</li>\n<li><strong>手段：</strong> 随机disable神经元</li>\n<li><strong>原理：</strong> 隐式多模型，迫使模型要学习更robust的feature，抓住核心特征</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Qulitative-Evaluations\"><a href=\"#Qulitative-Evaluations\" class=\"headerlink\" title=\"Qulitative Evaluations\"></a>Qulitative Evaluations</h2><ol>\n<li><strong>准确性：</strong> 对于每张图片，给出CNN分类结果前5的类别及概率</li>\n<li><strong>一致性：</strong> 对于每张照片，给出CNN提取特征相似的照片 e.g. 图片$i_1$提取的特征为$f_1$，找特征跟$f_1$相似的$i_k$，看下是不是同一个物体</li>\n</ol>\n"},{"title":"思维导图《沟通的艺术》","date":"2016-08-29T14:19:14.000Z","description":["实用性很强的一本关于沟通的书目"],"_content":"\n作为[豆瓣](https://book.douban.com/subject/26275861/)8.6评分的一本书，《沟通的艺术》不仅阐述了沟通的各种概念，还给出了实实在在的方法去提升沟通的质量，包括\n\n- 校准认知和事实\n- 理解行为背后的原因\n- 监视自我情绪\n- 成为一个更好的聆听者\n- 营造良好的沟通氛围\n- 处理冲突\n\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF.png)\n\n","source":"_posts/mindmap-goutongdeyishu.md","raw":"---\ntitle: 思维导图《沟通的艺术》\ndate: 2016-08-29 22:19:14\ntags:\n\t- 沟通的艺术\ndescription:\n\t- 实用性很强的一本关于沟通的书目\ncategories:\n\t- 思维导图\n---\n\n作为[豆瓣](https://book.douban.com/subject/26275861/)8.6评分的一本书，《沟通的艺术》不仅阐述了沟通的各种概念，还给出了实实在在的方法去提升沟通的质量，包括\n\n- 校准认知和事实\n- 理解行为背后的原因\n- 监视自我情绪\n- 成为一个更好的聆听者\n- 营造良好的沟通氛围\n- 处理冲突\n\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF.png)\n\n","slug":"mindmap-goutongdeyishu","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrpm000fxuvt7vxd3scc","content":"<p>作为<a href=\"https://book.douban.com/subject/26275861/\" target=\"_blank\" rel=\"external\">豆瓣</a>8.6评分的一本书，《沟通的艺术》不仅阐述了沟通的各种概念，还给出了实实在在的方法去提升沟通的质量，包括</p>\n<ul>\n<li>校准认知和事实</li>\n<li>理解行为背后的原因</li>\n<li>监视自我情绪</li>\n<li>成为一个更好的聆听者</li>\n<li>营造良好的沟通氛围</li>\n<li>处理冲突</li>\n</ul>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF.png\" alt=\"\"></p>\n","excerpt":"","more":"<p>作为<a href=\"https://book.douban.com/subject/26275861/\">豆瓣</a>8.6评分的一本书，《沟通的艺术》不仅阐述了沟通的各种概念，还给出了实实在在的方法去提升沟通的质量，包括</p>\n<ul>\n<li>校准认知和事实</li>\n<li>理解行为背后的原因</li>\n<li>监视自我情绪</li>\n<li>成为一个更好的聆听者</li>\n<li>营造良好的沟通氛围</li>\n<li>处理冲突</li>\n</ul>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF.png\" alt=\"\"></p>\n"},{"title":"思维导图《精进》","date":"2016-07-07T11:36:25.000Z","description":["知乎大牛采铜的《精进》，干货满满"],"_content":"\n采铜阐述了自己在时间、选择、行动、学习、思维、努力和成功7个方面的思考与观点，内容不愧[豆瓣](https://book.douban.com/subject/26761696/)8.1的评分。\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E7%B2%BE%E8%BF%9B.svg)\n","source":"_posts/mindmap-jingjin.md","raw":"---\ntitle: 思维导图《精进》\ndate: 2016-07-07 19:36:25\ntags:\n\t- 精进\ndescription:\n\t- 知乎大牛采铜的《精进》，干货满满\ncategories:\n\t- 思维导图\n---\n\n采铜阐述了自己在时间、选择、行动、学习、思维、努力和成功7个方面的思考与观点，内容不愧[豆瓣](https://book.douban.com/subject/26761696/)8.1的评分。\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E7%B2%BE%E8%BF%9B.svg)\n","slug":"mindmap-jingjin","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrpo000ixuvt4ztmt213","content":"<p>采铜阐述了自己在时间、选择、行动、学习、思维、努力和成功7个方面的思考与观点，内容不愧<a href=\"https://book.douban.com/subject/26761696/\" target=\"_blank\" rel=\"external\">豆瓣</a>8.1的评分。</p>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E7%B2%BE%E8%BF%9B.svg\" alt=\"\"></p>\n","excerpt":"","more":"<p>采铜阐述了自己在时间、选择、行动、学习、思维、努力和成功7个方面的思考与观点，内容不愧<a href=\"https://book.douban.com/subject/26761696/\">豆瓣</a>8.1的评分。</p>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E7%B2%BE%E8%BF%9B.svg\" alt=\"\"></p>\n"},{"title":"思维导图《哲学家们都干了些什么》","date":"2016-09-02T12:34:46.000Z","description":["据说是最严谨又最不严肃的哲学史文普读物"],"_content":"\n[豆瓣](https://book.douban.com/subject/26390842/)8.7的评分，在评论区甚至有人称赞这是最好的哲学史入门读物。\n\n全书阅读起来比较轻松愉快，除了纯粹的历史以外，还穿插了不少作者的思考，包括该哲学思想对我们平时的日常生活能够有什么帮助。但是美中不足的是可能为了趣味性，在解释深度上面有些妥协，感觉有些观点没有解释清楚，不过对于一个哲学小白来说，收获还是很丰富的。\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E5%93%B2%E5%AD%A6%E5%AE%B6%E4%BB%AC%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88.png)\n","source":"_posts/mindmap-zhexuejiamendouganlexieshenme.md","raw":"---\ntitle: 思维导图《哲学家们都干了些什么》\ndate: 2016-09-02 20:34:46\ntags:\n\t- 哲学家们都干了些什么\ndescription:\n\t- 据说是最严谨又最不严肃的哲学史文普读物\ncategories:\n\t- 思维导图\n---\n\n[豆瓣](https://book.douban.com/subject/26390842/)8.7的评分，在评论区甚至有人称赞这是最好的哲学史入门读物。\n\n全书阅读起来比较轻松愉快，除了纯粹的历史以外，还穿插了不少作者的思考，包括该哲学思想对我们平时的日常生活能够有什么帮助。但是美中不足的是可能为了趣味性，在解释深度上面有些妥协，感觉有些观点没有解释清楚，不过对于一个哲学小白来说，收获还是很丰富的。\n\n![](http://o9xzp7efk.bkt.clouddn.com/%E5%93%B2%E5%AD%A6%E5%AE%B6%E4%BB%AC%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88.png)\n","slug":"mindmap-zhexuejiamendouganlexieshenme","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrpr000mxuvtg1qdehwe","content":"<p><a href=\"https://book.douban.com/subject/26390842/\" target=\"_blank\" rel=\"external\">豆瓣</a>8.7的评分，在评论区甚至有人称赞这是最好的哲学史入门读物。</p>\n<p>全书阅读起来比较轻松愉快，除了纯粹的历史以外，还穿插了不少作者的思考，包括该哲学思想对我们平时的日常生活能够有什么帮助。但是美中不足的是可能为了趣味性，在解释深度上面有些妥协，感觉有些观点没有解释清楚，不过对于一个哲学小白来说，收获还是很丰富的。</p>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E5%93%B2%E5%AD%A6%E5%AE%B6%E4%BB%AC%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88.png\" alt=\"\"></p>\n","excerpt":"","more":"<p><a href=\"https://book.douban.com/subject/26390842/\">豆瓣</a>8.7的评分，在评论区甚至有人称赞这是最好的哲学史入门读物。</p>\n<p>全书阅读起来比较轻松愉快，除了纯粹的历史以外，还穿插了不少作者的思考，包括该哲学思想对我们平时的日常生活能够有什么帮助。但是美中不足的是可能为了趣味性，在解释深度上面有些妥协，感觉有些观点没有解释清楚，不过对于一个哲学小白来说，收获还是很丰富的。</p>\n<p><img src=\"http://o9xzp7efk.bkt.clouddn.com/%E5%93%B2%E5%AD%A6%E5%AE%B6%E4%BB%AC%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88.png\" alt=\"\"></p>\n"},{"title":"课程笔记《线性代数的本质》","description":["形象、直观地理解线性代数"],"date":"2017-09-09T03:15:46.000Z","_content":"\n## 向量\n\n### 不同视角下的向量\n\n- physics：箭头\n- cs：数字列表（e.g. $ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $）\n- mathematician： $ \\vec{\\bf{v}} $\n\t* 抽象理解，任何相加和数乘有意义的东西\n\t* physics和cs都只是其中一种实例，还可以是其他任意满足性质的东西（e.g. 函数）\n\n### physics view与cs view的联系\n\n- 桥梁：坐标系\n- 对偶性（独立推导、一一对应）：表示、运算（加法、数乘）\n- 作用\n\t* physics→cs：已知想要的图形变换效果，使用$ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $与计算机进行交流 \n\t* cs→physics：已知$ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $相关的等式，使用图形直观理解物理意义\n\n## 线性组合、张成的空间与基\n\n为什么称$a \\vec{\\bf{v}} + b \\vec{\\bf{w}}$是$\\vec{\\bf{v}}$与$\\vec{\\bf{w}}$的**线性**组合：固定$\\vec{\\bf{v}}$的条件下，任意取$\\vec{\\bf{w}}$，得到的是一条线\n\n## 矩阵与线性变换\n\n### Linear Transformation\n\n- transformation\n\t* 相当于function（输入、输出）\n\t* 但之所以不用function，是因为transformation更有动感（由一个地方运动到另外一个地方）\n\t* 用网格线可视化（i.e. 画出网格线变换前后的位置）的时候，直观感受是空间发生变化\n- linear\n\t* 直线仍然是直线、原点位置不变（i.e. 保持网格线平行并等距分布）\n\n### 如何表示这个Linear Transformation\n\n- 一般来说，描述一个变换，我们需要记下所有的输入、输出pair\n- 但由于线性变换具有保持网格线平行且等距分布的性质\n- 发现假如原向量$\\vec{\\bf{v}}$是$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合（伸缩），那么新向量$\\vec{\\bf{v}}\\_{new}$会是$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$相同的线性组合（伸缩）\n- 因此仅需记录$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$即有**足够信息**表示该线性变换（假设知道输入向量关于原向量$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合，通过$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$即可以求得输出向量）\n- 【以上全部都只用到了图形化的解释，没有涉及坐标这个概念（i.e. 保存这个信息可以不用坐标，而是直接把新的箭头画出来，然后做箭头的加法和数乘）】\n- 【下面为了用数学来表示，开始引入坐标的概念】\n- 为了用坐标表示，需要选择一个基，于是选择原来的$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$作为基，之后的所有坐标都基于这个基\n- 引入坐标后，只需记录$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$的坐标，就有足够的信息表示该线性变换\n- 该坐标可以表示成矩阵的形式，因此**矩阵与线性变换具有对偶性**\n\n### 线性组合、线性变换、向量的表示\n\n- 注意，下面三者是独立的：\n\t* 向量的线性组合：$\\vec{\\bf{v}}$如何由$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$通过加法和数乘得到\n\t* 线性变换：具有保持网格线平行且等距分布性质的mapping\n\t* 向量的表示：引入一组基，使用一个数字列表表示沿各基的伸缩\n- e.g.\n\t+ 向量的线性组合：$\\bf{v} = 2\\bf{i} + 3\\bf{j}$\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img combination.png 向量的线性组合 %}\n</div>\n\t+ 线性变换\n\t\t* 通过变换后有${\\bf{v}}\\_{new}$、${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$\n\t\t* ${\\bf{v}}\\_{new}$关于${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$的线性组合仍然为2和3\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img transformation.png 线性变换 %}\n</div>\n\t+ 向量的表示：\n\t\t* $\\bf{i}$和$\\bf{j}$的坐标可以基于任意基S\n\t\t* ${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$的坐标可以基于任意基T\n\t\t* 最终${\\bf{v}}\\_{new}$的坐标基于基T\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img representation.png 向量的表示 %}\n</div>\n- 一般情况下，对于向量的表示，都直接选取$\\bf{i}$和$\\bf{j}$作为基，这样做的好处是\n\t+ 可以直接从$\\bf{v}$的坐标知道线性组合\n\t+ 不用再为${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$选取新的基\n\t+ 最终的${\\bf{v}}\\_{new}$的坐标与$\\bf{v}$基于相同的基\n- 核心在于三点：\n\t+ 追踪用于表示$\\vec{v}$的向量\n\t+ 线性组合\n\t+ 向量的表示\n\n### $A\\vec{\\bf{x}}$的两种理解\n\n- 结果上来看，就是得到一个新的向量（箭头）\n- 由于该箭头使用坐标的形式表示的，那么肯定跟基相关\n- 过程可以有两个理解（主要是针对坐标x的理解不同）：\n\t* 把A看成是线性变换的表示【有发生线性变换，向量的表示没变】：那么$A\\vec{\\bf{x}}$就可以理解为将在基1下的坐标x经过线性变换得到的在基1下的新坐标\n\t* 把A看成是基2【没有发生线性变换，只是向量的表示发生了变化】：那么$A\\vec{\\bf{x}}$就可以理解为在基2下的坐标x在基1下的坐标\n\n## 矩阵乘法与线性变换复合\n\n- 两个矩阵相乘：一个结合两种变换的变换\n- 复合矩阵求解：追踪${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$\n- 直观理解解释：\n\t+ 交换律（$AB \\neq BA$）不成立\n\t+ 结合律（$(AB)C = A(BC)$）成立：复合变换AB等价于顺序进行变换B、A\n\n## 行列式\n\n- 动机：衡量线性变换使得空间伸缩了多少\n- 含义：\n\t+ 大小表示：单位正方形/立方体的面积/体积伸缩的比例\n\t+ 正负表示：定向（右手法则）\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img det.png 行列式的含义 %}\n</div>\n- 等价：\n\t+ 行列式等于0\n\t+ 降维\n- 直观解释$det(AB) = det(A)det(B)$：\n\t+ $det(AB)$的含义是求解复合变换$AB$使得体积伸缩了几倍\n\t+ 复合变换$AB$首先进行变换$B$使得空间伸缩了$det(B)$，然后进行变换$A$使得空间伸缩了$det(A)$倍\n\t+ 所以体积总共伸缩了$det(A)det(B)$倍\n\n## 逆矩阵、列空间与零空间\n\n### 矩阵的用途：求解线性方程组\n\n- $A\\vec{\\bf{x}} = \\vec{\\bf{b}}$：找一个经过变换后等于$\\vec{\\bf{b}}$的向量\n- $det(A) \\neq 0$：有唯一解，一一对应\n\t+ 假如有一个变换，能使得$\\vec{\\bf{b}}$变到$\\vec{\\bf{x}}$\n\t+ 那么只要对$\\vec{\\bf{b}}$进行该变换即可得到要求的$\\vec{\\bf{x}}$\n\t+ 因为对于$\\vec{\\bf{x}}$来说，先进行变换$A$后再进行该变换等于没变\n\t+ 所以称该变换为$A$的逆\n\t+ 由此含义可知，以下等价\n\t\t* 逆存在\n\t\t* 没有降维\n\t\t* 行列式不等于0\n- $det(A) = 0$：不一定有解，除非$\\vec{\\bf{b}}$恰好在降维所在空间（列空间），此时有无限多个解（e.g. 两个共线向量表示该方向上的向量）\n\t+ 行列式等于0\n\t+ 降维（e.g. 空间由面变成线）\n\t+ 逆不存在\n\t\t* 假如逆存在\n\t\t* 存在一个变换（函数）由线map到面\n\t\t* 矛盾（违反函数的定义）\n\n### Rank\n\n- 动机：衡量降维的程度（降到点/线/面）\n- 含义：降维后的维度\n\n### 零空间\n\n- 动机：衡量有多少向量被映射到零向量（$A\\vec{\\bf{x}}=\\vec{\\bf{0}}$）\n- 假如没有降维，只有零向量会被映射到零向量\n- 假如降维，会有其他向量被映射到零向量\n- 被映射到零向量的向量组成零空间\n\t+ 零空间有非零向量\n\t+ 降维\n\t+ 行列式等于0\n\n## 非方阵\n\n- $\\begin{bmatrix} 1 & 2 \\\\\\\\ 2 & 2 \\\\\\\\ 3 & 3 \\end{bmatrix}$：\n\t+ $\\begin{bmatrix} １ \\\\\\\\ ０\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\\\\\ 2  \\\\\\\\ 3 \\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} ２ \\\\\\\\ 2  \\\\\\\\ 3 \\end{bmatrix}$\n- $\\begin{bmatrix} 1 & 2 & 3\\\\\\\\ 2 & 2 & 3 \\end{bmatrix}$：\n\t+ $\\begin{bmatrix} １ \\\\\\\\ ０ \\\\\\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ 1 \\\\\\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 2 \\\\\\\\ 2\\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ ０ \\\\\\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} 3 \\\\\\\\ 3\\end{bmatrix}$\n\n## 点积与对偶性\n\n- 证明点积与顺序无关思路：\n\t+ 先证等长时顺序无关\n\t+ 再证不等长时顺序无关（标量相乘）\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img dot_product_order.png 点积顺序无关 %}\n</div>\n- 对偶性：一一对应\n- 点积：\n\t+ 物理含义：投影长度再乘以被投影向量长度\n\t+ 数学含义\n- 证明对偶型思路：\n\t+ 验证单位向量点积：\n\t\t+ 定义投影到单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix}$的投影长度这种线性变换\n\t\t+ 得到投影长度这种线性变换对应的矩阵为$\\begin{bmatrix} x & y\\end{bmatrix}$\n\t\t+ 运算结果恰好等于与一个单位向量点积的数学形式\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img dot_product_projection.png 单位向量点积等价于投影长度这种线性变换 %}\n</div>\n\t+ 验证非单位向量点积：\n\t\t+ 对于一个任意长度的向量$\\vec{\\bf{u}}=k\\vec{\\bf{\\hat{u}}}$\n\t\t+ 定义投影到其单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix}$的投影长度乘以其长度k这种线性变换\n\t\t+ 得到投影长度这种线性变换对应的矩阵为$k\\begin{bmatrix} x & y\\end{bmatrix}$\n\t\t+ 运算结果恰好等于与一个非单位向量$\\vec{\\bf{u}}$点积的数学形式\n- 将一个向量倒过来：投影到这个向量的投影长度乘以这个向量的长度这种线性变换对应的矩阵\n\n## 叉乘\n\n- 叉乘：\n\t+ 向量：大小为两向量平行四边形面积，方向为右手定则的向量\n\t+ 数学含义\n\t\t* 二维：$det(\\begin{bmatrix} \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix})$\n\t\t* 三维：$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}} & \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix})$\n- 证明思路\n\t+ 根据叉乘的定义，他是由$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}} & \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix}) = Xi + Yj +Zk$得到的向量$\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$\n\t+ 要证明$\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$的大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则\n\t\t* $Xi + Yj +Zk$的大小的物理意义有两个（i.e. 所找向量必须同时满足以下两个性质）\n\t\t\t+ $\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$与$\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}$的点积\n\t\t\t+ $\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}$、$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行六面体的体积\n\t\t* 当$Xi + Yj +Zk$大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则时同时成立\n\n## 基变换\n\n### 向量表示（坐标）的变换\n\n- 从基于A系的坐标得到基于B系的坐标\n\t+ 对于一个坐标为$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$的向量\n\t+ 我们可以假设这个坐标是基于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的（i.e. 2和3是向量关于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的线性组合）\n\t+ 那么假如想要得到这个向量基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示，只需得到$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示（i.e. 从基于A系的坐标得到基于B系的坐标）\n\t+ 从之前关于矩阵的两个理解可知该变换是个矩阵，记为M\n- 根据上述得到的矩阵M直接取得B系到A系的坐标变换\n\t+ 假设B系到A系的坐标变换矩阵为M'\n\t+ 那么对于A系下坐标为$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$的向量\n\t+ 先进行M变换将该坐标转换为B系下的坐标，再进行M'变换将B系下坐标转换为A系下坐标，得到的结果必然还是$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$\n\t+ 由此可得M'M是一个什么都不干的变换，所以M'是M的逆\n\n### 变换表示（矩阵）的变换\n\n- 对于同一个线性变换，基于不同的系有不同的表示\n- 假设线性变换在标准系下的表示为M，要求该线性变化在系A下的表示\n\t* 一般来说，在表示一个变换的时候，希望输入的表示和输出的表示是基于同一个系的\n\t* 输入的表示取决于基\n\t* 输出的表示取决于变换后基的表示\n\t* 所以为了输入输出基于相同系，变换后基的表示也取表示输入的基\n\t* 因此矩阵里面的列向量要与输入基于相同的基\n\t* 所以下面第一步是将基统一\n- A：将A系下的表示转换为标准系下的表示\n- MA：在标准系下进行转换\n- A'MA：通过A'将标准系下的向量转换为A系下的表示\n- 因此A'MA也称M的【相似矩阵】\n\n## 特征向量与特征值\n\n- eigen vector：变换后方向不变的向量\n- eigen value：变换后在对应eigen vector的伸缩量\n- eigen basis：\n\t* 有可能不够eigen vector来张成整个空间\n\t* 假如有的话，好处是：\n\t\t+ 在该变换下，基只发生了伸缩\n\t\t+ 用该基表示变换的话可以是对角矩阵，因为变换后基只与变换前基的一项相关，其余全是0\n\t\t+ 对角矩阵算幂很爽\n\t* 应用：算A的幂\n\t\t+ A是标准系下某个线性变换的表示\n\t\t+ 假如A有eigen basis（i.e.　足够多的方向不变向量）\n\t\t+ 求A对应的线性变换在eigen　basis下的表示（形式为B'AB），且该表示必然为对角矩阵【矩阵的对角化】\n\t\t+ 在eigen basis下进行幂运算\n\t\t+ 最后再把向量在eigen basis下的表示转回在标准系下的表示\n\n## 抽象向量空间\n\n- 基本就是1中所说的mathematician view（i.e.　满足一些性质（定义了加法、数乘，且运算封闭等等）的东西，physics view和cs　view等等都是向量的具体化）\n- 这里举了一个例子：\n\t+ 向量：多项式函数\n\t+ 线性变换：求导\n\n## 收获\n\n- 对线性代数有了更加直观、深刻的理解\n\t+ 理清了原先混淆的概念\n\t+　知道了为什么一些操作的物理含义是这样的\n- 对偶性的idea很美\n- 视频开始的句子很美\n\n## References\n\n- [线性代数的本质](http://space.bilibili.com/88461692#!/channel/detail?cid=9450)\n","source":"_posts/note-essence-of-linear-algrbra.md","raw":"---\ntitle: 课程笔记《线性代数的本质》\ntags:\n  - 线性代数的本质\ndescription:\n  - 形象、直观地理解线性代数\ncategories:\n  - 课程笔记\ndate: 2017-09-9 11:15:46\n---\n\n## 向量\n\n### 不同视角下的向量\n\n- physics：箭头\n- cs：数字列表（e.g. $ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $）\n- mathematician： $ \\vec{\\bf{v}} $\n\t* 抽象理解，任何相加和数乘有意义的东西\n\t* physics和cs都只是其中一种实例，还可以是其他任意满足性质的东西（e.g. 函数）\n\n### physics view与cs view的联系\n\n- 桥梁：坐标系\n- 对偶性（独立推导、一一对应）：表示、运算（加法、数乘）\n- 作用\n\t* physics→cs：已知想要的图形变换效果，使用$ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $与计算机进行交流 \n\t* cs→physics：已知$ \\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix} $相关的等式，使用图形直观理解物理意义\n\n## 线性组合、张成的空间与基\n\n为什么称$a \\vec{\\bf{v}} + b \\vec{\\bf{w}}$是$\\vec{\\bf{v}}$与$\\vec{\\bf{w}}$的**线性**组合：固定$\\vec{\\bf{v}}$的条件下，任意取$\\vec{\\bf{w}}$，得到的是一条线\n\n## 矩阵与线性变换\n\n### Linear Transformation\n\n- transformation\n\t* 相当于function（输入、输出）\n\t* 但之所以不用function，是因为transformation更有动感（由一个地方运动到另外一个地方）\n\t* 用网格线可视化（i.e. 画出网格线变换前后的位置）的时候，直观感受是空间发生变化\n- linear\n\t* 直线仍然是直线、原点位置不变（i.e. 保持网格线平行并等距分布）\n\n### 如何表示这个Linear Transformation\n\n- 一般来说，描述一个变换，我们需要记下所有的输入、输出pair\n- 但由于线性变换具有保持网格线平行且等距分布的性质\n- 发现假如原向量$\\vec{\\bf{v}}$是$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合（伸缩），那么新向量$\\vec{\\bf{v}}\\_{new}$会是$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$相同的线性组合（伸缩）\n- 因此仅需记录$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$即有**足够信息**表示该线性变换（假设知道输入向量关于原向量$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合，通过$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$即可以求得输出向量）\n- 【以上全部都只用到了图形化的解释，没有涉及坐标这个概念（i.e. 保存这个信息可以不用坐标，而是直接把新的箭头画出来，然后做箭头的加法和数乘）】\n- 【下面为了用数学来表示，开始引入坐标的概念】\n- 为了用坐标表示，需要选择一个基，于是选择原来的$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$作为基，之后的所有坐标都基于这个基\n- 引入坐标后，只需记录$\\hat{\\bf{i}}\\_{new}$和$\\hat{\\bf{j}}\\_{new}$的坐标，就有足够的信息表示该线性变换\n- 该坐标可以表示成矩阵的形式，因此**矩阵与线性变换具有对偶性**\n\n### 线性组合、线性变换、向量的表示\n\n- 注意，下面三者是独立的：\n\t* 向量的线性组合：$\\vec{\\bf{v}}$如何由$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$通过加法和数乘得到\n\t* 线性变换：具有保持网格线平行且等距分布性质的mapping\n\t* 向量的表示：引入一组基，使用一个数字列表表示沿各基的伸缩\n- e.g.\n\t+ 向量的线性组合：$\\bf{v} = 2\\bf{i} + 3\\bf{j}$\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img combination.png 向量的线性组合 %}\n</div>\n\t+ 线性变换\n\t\t* 通过变换后有${\\bf{v}}\\_{new}$、${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$\n\t\t* ${\\bf{v}}\\_{new}$关于${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$的线性组合仍然为2和3\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img transformation.png 线性变换 %}\n</div>\n\t+ 向量的表示：\n\t\t* $\\bf{i}$和$\\bf{j}$的坐标可以基于任意基S\n\t\t* ${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$的坐标可以基于任意基T\n\t\t* 最终${\\bf{v}}\\_{new}$的坐标基于基T\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img representation.png 向量的表示 %}\n</div>\n- 一般情况下，对于向量的表示，都直接选取$\\bf{i}$和$\\bf{j}$作为基，这样做的好处是\n\t+ 可以直接从$\\bf{v}$的坐标知道线性组合\n\t+ 不用再为${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$选取新的基\n\t+ 最终的${\\bf{v}}\\_{new}$的坐标与$\\bf{v}$基于相同的基\n- 核心在于三点：\n\t+ 追踪用于表示$\\vec{v}$的向量\n\t+ 线性组合\n\t+ 向量的表示\n\n### $A\\vec{\\bf{x}}$的两种理解\n\n- 结果上来看，就是得到一个新的向量（箭头）\n- 由于该箭头使用坐标的形式表示的，那么肯定跟基相关\n- 过程可以有两个理解（主要是针对坐标x的理解不同）：\n\t* 把A看成是线性变换的表示【有发生线性变换，向量的表示没变】：那么$A\\vec{\\bf{x}}$就可以理解为将在基1下的坐标x经过线性变换得到的在基1下的新坐标\n\t* 把A看成是基2【没有发生线性变换，只是向量的表示发生了变化】：那么$A\\vec{\\bf{x}}$就可以理解为在基2下的坐标x在基1下的坐标\n\n## 矩阵乘法与线性变换复合\n\n- 两个矩阵相乘：一个结合两种变换的变换\n- 复合矩阵求解：追踪${\\bf{i}}\\_{new}$和${\\bf{j}}\\_{new}$\n- 直观理解解释：\n\t+ 交换律（$AB \\neq BA$）不成立\n\t+ 结合律（$(AB)C = A(BC)$）成立：复合变换AB等价于顺序进行变换B、A\n\n## 行列式\n\n- 动机：衡量线性变换使得空间伸缩了多少\n- 含义：\n\t+ 大小表示：单位正方形/立方体的面积/体积伸缩的比例\n\t+ 正负表示：定向（右手法则）\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img det.png 行列式的含义 %}\n</div>\n- 等价：\n\t+ 行列式等于0\n\t+ 降维\n- 直观解释$det(AB) = det(A)det(B)$：\n\t+ $det(AB)$的含义是求解复合变换$AB$使得体积伸缩了几倍\n\t+ 复合变换$AB$首先进行变换$B$使得空间伸缩了$det(B)$，然后进行变换$A$使得空间伸缩了$det(A)$倍\n\t+ 所以体积总共伸缩了$det(A)det(B)$倍\n\n## 逆矩阵、列空间与零空间\n\n### 矩阵的用途：求解线性方程组\n\n- $A\\vec{\\bf{x}} = \\vec{\\bf{b}}$：找一个经过变换后等于$\\vec{\\bf{b}}$的向量\n- $det(A) \\neq 0$：有唯一解，一一对应\n\t+ 假如有一个变换，能使得$\\vec{\\bf{b}}$变到$\\vec{\\bf{x}}$\n\t+ 那么只要对$\\vec{\\bf{b}}$进行该变换即可得到要求的$\\vec{\\bf{x}}$\n\t+ 因为对于$\\vec{\\bf{x}}$来说，先进行变换$A$后再进行该变换等于没变\n\t+ 所以称该变换为$A$的逆\n\t+ 由此含义可知，以下等价\n\t\t* 逆存在\n\t\t* 没有降维\n\t\t* 行列式不等于0\n- $det(A) = 0$：不一定有解，除非$\\vec{\\bf{b}}$恰好在降维所在空间（列空间），此时有无限多个解（e.g. 两个共线向量表示该方向上的向量）\n\t+ 行列式等于0\n\t+ 降维（e.g. 空间由面变成线）\n\t+ 逆不存在\n\t\t* 假如逆存在\n\t\t* 存在一个变换（函数）由线map到面\n\t\t* 矛盾（违反函数的定义）\n\n### Rank\n\n- 动机：衡量降维的程度（降到点/线/面）\n- 含义：降维后的维度\n\n### 零空间\n\n- 动机：衡量有多少向量被映射到零向量（$A\\vec{\\bf{x}}=\\vec{\\bf{0}}$）\n- 假如没有降维，只有零向量会被映射到零向量\n- 假如降维，会有其他向量被映射到零向量\n- 被映射到零向量的向量组成零空间\n\t+ 零空间有非零向量\n\t+ 降维\n\t+ 行列式等于0\n\n## 非方阵\n\n- $\\begin{bmatrix} 1 & 2 \\\\\\\\ 2 & 2 \\\\\\\\ 3 & 3 \\end{bmatrix}$：\n\t+ $\\begin{bmatrix} １ \\\\\\\\ ０\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\\\\\ 2  \\\\\\\\ 3 \\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} ２ \\\\\\\\ 2  \\\\\\\\ 3 \\end{bmatrix}$\n- $\\begin{bmatrix} 1 & 2 & 3\\\\\\\\ 2 & 2 & 3 \\end{bmatrix}$：\n\t+ $\\begin{bmatrix} １ \\\\\\\\ ０ \\\\\\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\\\\\ 2\\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ 1 \\\\\\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 2 \\\\\\\\ 2\\end{bmatrix}$\n\t+ $\\begin{bmatrix} 0 \\\\\\\\ ０ \\\\\\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} 3 \\\\\\\\ 3\\end{bmatrix}$\n\n## 点积与对偶性\n\n- 证明点积与顺序无关思路：\n\t+ 先证等长时顺序无关\n\t+ 再证不等长时顺序无关（标量相乘）\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img dot_product_order.png 点积顺序无关 %}\n</div>\n- 对偶性：一一对应\n- 点积：\n\t+ 物理含义：投影长度再乘以被投影向量长度\n\t+ 数学含义\n- 证明对偶型思路：\n\t+ 验证单位向量点积：\n\t\t+ 定义投影到单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix}$的投影长度这种线性变换\n\t\t+ 得到投影长度这种线性变换对应的矩阵为$\\begin{bmatrix} x & y\\end{bmatrix}$\n\t\t+ 运算结果恰好等于与一个单位向量点积的数学形式\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img dot_product_projection.png 单位向量点积等价于投影长度这种线性变换 %}\n</div>\n\t+ 验证非单位向量点积：\n\t\t+ 对于一个任意长度的向量$\\vec{\\bf{u}}=k\\vec{\\bf{\\hat{u}}}$\n\t\t+ 定义投影到其单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix}$的投影长度乘以其长度k这种线性变换\n\t\t+ 得到投影长度这种线性变换对应的矩阵为$k\\begin{bmatrix} x & y\\end{bmatrix}$\n\t\t+ 运算结果恰好等于与一个非单位向量$\\vec{\\bf{u}}$点积的数学形式\n- 将一个向量倒过来：投影到这个向量的投影长度乘以这个向量的长度这种线性变换对应的矩阵\n\n## 叉乘\n\n- 叉乘：\n\t+ 向量：大小为两向量平行四边形面积，方向为右手定则的向量\n\t+ 数学含义\n\t\t* 二维：$det(\\begin{bmatrix} \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix})$\n\t\t* 三维：$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}} & \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix})$\n- 证明思路\n\t+ 根据叉乘的定义，他是由$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}} & \\vec{\\bf{u}} & \\vec{\\bf{v}} \\end{bmatrix}) = Xi + Yj +Zk$得到的向量$\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$\n\t+ 要证明$\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$的大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则\n\t\t* $Xi + Yj +Zk$的大小的物理意义有两个（i.e. 所找向量必须同时满足以下两个性质）\n\t\t\t+ $\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z \\end{bmatrix}$与$\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}$的点积\n\t\t\t+ $\\begin{bmatrix} i \\\\\\\\ j \\\\\\\\ k \\end{bmatrix}$、$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行六面体的体积\n\t\t* 当$Xi + Yj +Zk$大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则时同时成立\n\n## 基变换\n\n### 向量表示（坐标）的变换\n\n- 从基于A系的坐标得到基于B系的坐标\n\t+ 对于一个坐标为$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$的向量\n\t+ 我们可以假设这个坐标是基于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的（i.e. 2和3是向量关于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的线性组合）\n\t+ 那么假如想要得到这个向量基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示，只需得到$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示（i.e. 从基于A系的坐标得到基于B系的坐标）\n\t+ 从之前关于矩阵的两个理解可知该变换是个矩阵，记为M\n- 根据上述得到的矩阵M直接取得B系到A系的坐标变换\n\t+ 假设B系到A系的坐标变换矩阵为M'\n\t+ 那么对于A系下坐标为$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$的向量\n\t+ 先进行M变换将该坐标转换为B系下的坐标，再进行M'变换将B系下坐标转换为A系下坐标，得到的结果必然还是$\\begin{bmatrix} 2 \\\\\\\\ 3\\end{bmatrix}$\n\t+ 由此可得M'M是一个什么都不干的变换，所以M'是M的逆\n\n### 变换表示（矩阵）的变换\n\n- 对于同一个线性变换，基于不同的系有不同的表示\n- 假设线性变换在标准系下的表示为M，要求该线性变化在系A下的表示\n\t* 一般来说，在表示一个变换的时候，希望输入的表示和输出的表示是基于同一个系的\n\t* 输入的表示取决于基\n\t* 输出的表示取决于变换后基的表示\n\t* 所以为了输入输出基于相同系，变换后基的表示也取表示输入的基\n\t* 因此矩阵里面的列向量要与输入基于相同的基\n\t* 所以下面第一步是将基统一\n- A：将A系下的表示转换为标准系下的表示\n- MA：在标准系下进行转换\n- A'MA：通过A'将标准系下的向量转换为A系下的表示\n- 因此A'MA也称M的【相似矩阵】\n\n## 特征向量与特征值\n\n- eigen vector：变换后方向不变的向量\n- eigen value：变换后在对应eigen vector的伸缩量\n- eigen basis：\n\t* 有可能不够eigen vector来张成整个空间\n\t* 假如有的话，好处是：\n\t\t+ 在该变换下，基只发生了伸缩\n\t\t+ 用该基表示变换的话可以是对角矩阵，因为变换后基只与变换前基的一项相关，其余全是0\n\t\t+ 对角矩阵算幂很爽\n\t* 应用：算A的幂\n\t\t+ A是标准系下某个线性变换的表示\n\t\t+ 假如A有eigen basis（i.e.　足够多的方向不变向量）\n\t\t+ 求A对应的线性变换在eigen　basis下的表示（形式为B'AB），且该表示必然为对角矩阵【矩阵的对角化】\n\t\t+ 在eigen basis下进行幂运算\n\t\t+ 最后再把向量在eigen basis下的表示转回在标准系下的表示\n\n## 抽象向量空间\n\n- 基本就是1中所说的mathematician view（i.e.　满足一些性质（定义了加法、数乘，且运算封闭等等）的东西，physics view和cs　view等等都是向量的具体化）\n- 这里举了一个例子：\n\t+ 向量：多项式函数\n\t+ 线性变换：求导\n\n## 收获\n\n- 对线性代数有了更加直观、深刻的理解\n\t+ 理清了原先混淆的概念\n\t+　知道了为什么一些操作的物理含义是这样的\n- 对偶性的idea很美\n- 视频开始的句子很美\n\n## References\n\n- [线性代数的本质](http://space.bilibili.com/88461692#!/channel/detail?cid=9450)\n","slug":"note-essence-of-linear-algrbra","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrq2000oxuvttfe0colc","content":"<h2 id=\"向量\"><a href=\"#向量\" class=\"headerlink\" title=\"向量\"></a>向量</h2><h3 id=\"不同视角下的向量\"><a href=\"#不同视角下的向量\" class=\"headerlink\" title=\"不同视角下的向量\"></a>不同视角下的向量</h3><ul>\n<li>physics：箭头</li>\n<li>cs：数字列表（e.g. $ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $）</li>\n<li>mathematician： $ \\vec{\\bf{v}} $<ul>\n<li>抽象理解，任何相加和数乘有意义的东西</li>\n<li>physics和cs都只是其中一种实例，还可以是其他任意满足性质的东西（e.g. 函数）</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"physics-view与cs-view的联系\"><a href=\"#physics-view与cs-view的联系\" class=\"headerlink\" title=\"physics view与cs view的联系\"></a>physics view与cs view的联系</h3><ul>\n<li>桥梁：坐标系</li>\n<li>对偶性（独立推导、一一对应）：表示、运算（加法、数乘）</li>\n<li>作用<ul>\n<li>physics→cs：已知想要的图形变换效果，使用$ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $与计算机进行交流 </li>\n<li>cs→physics：已知$ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $相关的等式，使用图形直观理解物理意义</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"线性组合、张成的空间与基\"><a href=\"#线性组合、张成的空间与基\" class=\"headerlink\" title=\"线性组合、张成的空间与基\"></a>线性组合、张成的空间与基</h2><p>为什么称$a \\vec{\\bf{v}} + b \\vec{\\bf{w}}$是$\\vec{\\bf{v}}$与$\\vec{\\bf{w}}$的<strong>线性</strong>组合：固定$\\vec{\\bf{v}}$的条件下，任意取$\\vec{\\bf{w}}$，得到的是一条线</p>\n<h2 id=\"矩阵与线性变换\"><a href=\"#矩阵与线性变换\" class=\"headerlink\" title=\"矩阵与线性变换\"></a>矩阵与线性变换</h2><h3 id=\"Linear-Transformation\"><a href=\"#Linear-Transformation\" class=\"headerlink\" title=\"Linear Transformation\"></a>Linear Transformation</h3><ul>\n<li>transformation<ul>\n<li>相当于function（输入、输出）</li>\n<li>但之所以不用function，是因为transformation更有动感（由一个地方运动到另外一个地方）</li>\n<li>用网格线可视化（i.e. 画出网格线变换前后的位置）的时候，直观感受是空间发生变化</li>\n</ul>\n</li>\n<li>linear<ul>\n<li>直线仍然是直线、原点位置不变（i.e. 保持网格线平行并等距分布）</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"如何表示这个Linear-Transformation\"><a href=\"#如何表示这个Linear-Transformation\" class=\"headerlink\" title=\"如何表示这个Linear Transformation\"></a>如何表示这个Linear Transformation</h3><ul>\n<li>一般来说，描述一个变换，我们需要记下所有的输入、输出pair</li>\n<li>但由于线性变换具有保持网格线平行且等距分布的性质</li>\n<li>发现假如原向量$\\vec{\\bf{v}}$是$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合（伸缩），那么新向量$\\vec{\\bf{v}}_{new}$会是$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$相同的线性组合（伸缩）</li>\n<li>因此仅需记录$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$即有<strong>足够信息</strong>表示该线性变换（假设知道输入向量关于原向量$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合，通过$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$即可以求得输出向量）</li>\n<li>【以上全部都只用到了图形化的解释，没有涉及坐标这个概念（i.e. 保存这个信息可以不用坐标，而是直接把新的箭头画出来，然后做箭头的加法和数乘）】</li>\n<li>【下面为了用数学来表示，开始引入坐标的概念】</li>\n<li>为了用坐标表示，需要选择一个基，于是选择原来的$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$作为基，之后的所有坐标都基于这个基</li>\n<li>引入坐标后，只需记录$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$的坐标，就有足够的信息表示该线性变换</li>\n<li>该坐标可以表示成矩阵的形式，因此<strong>矩阵与线性变换具有对偶性</strong></li>\n</ul>\n<h3 id=\"线性组合、线性变换、向量的表示\"><a href=\"#线性组合、线性变换、向量的表示\" class=\"headerlink\" title=\"线性组合、线性变换、向量的表示\"></a>线性组合、线性变换、向量的表示</h3><ul>\n<li>注意，下面三者是独立的：<ul>\n<li>向量的线性组合：$\\vec{\\bf{v}}$如何由$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$通过加法和数乘得到</li>\n<li>线性变换：具有保持网格线平行且等距分布性质的mapping</li>\n<li>向量的表示：引入一组基，使用一个数字列表表示沿各基的伸缩</li>\n</ul>\n</li>\n<li>e.g.<ul>\n<li>向量的线性组合：$\\bf{v} = 2\\bf{i} + 3\\bf{j}$<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/combination.png\" alt=\"向量的线性组合\" title=\"向量的线性组合\"><br></div></li>\n<li>线性变换<ul>\n<li>通过变换后有${\\bf{v}}_{new}$、${\\bf{i}}_{new}$和${\\bf{j}}_{new}$</li>\n<li>${\\bf{v}}_{new}$关于${\\bf{i}}_{new}$和${\\bf{j}}_{new}$的线性组合仍然为2和3<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/transformation.png\" alt=\"线性变换\" title=\"线性变换\"><br></div></li>\n</ul>\n</li>\n<li>向量的表示：<ul>\n<li>$\\bf{i}$和$\\bf{j}$的坐标可以基于任意基S</li>\n<li>${\\bf{i}}_{new}$和${\\bf{j}}_{new}$的坐标可以基于任意基T</li>\n<li>最终${\\bf{v}}_{new}$的坐标基于基T<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/representation.png\" alt=\"向量的表示\" title=\"向量的表示\"><br></div></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>一般情况下，对于向量的表示，都直接选取$\\bf{i}$和$\\bf{j}$作为基，这样做的好处是<ul>\n<li>可以直接从$\\bf{v}$的坐标知道线性组合</li>\n<li>不用再为${\\bf{i}}_{new}$和${\\bf{j}}_{new}$选取新的基</li>\n<li>最终的${\\bf{v}}_{new}$的坐标与$\\bf{v}$基于相同的基</li>\n</ul>\n</li>\n<li>核心在于三点：<ul>\n<li>追踪用于表示$\\vec{v}$的向量</li>\n<li>线性组合</li>\n<li>向量的表示</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"A-vec-bf-x-的两种理解\"><a href=\"#A-vec-bf-x-的两种理解\" class=\"headerlink\" title=\"$A\\vec{\\bf{x}}$的两种理解\"></a>$A\\vec{\\bf{x}}$的两种理解</h3><ul>\n<li>结果上来看，就是得到一个新的向量（箭头）</li>\n<li>由于该箭头使用坐标的形式表示的，那么肯定跟基相关</li>\n<li>过程可以有两个理解（主要是针对坐标x的理解不同）：<ul>\n<li>把A看成是线性变换的表示【有发生线性变换，向量的表示没变】：那么$A\\vec{\\bf{x}}$就可以理解为将在基1下的坐标x经过线性变换得到的在基1下的新坐标</li>\n<li>把A看成是基2【没有发生线性变换，只是向量的表示发生了变化】：那么$A\\vec{\\bf{x}}$就可以理解为在基2下的坐标x在基1下的坐标</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"矩阵乘法与线性变换复合\"><a href=\"#矩阵乘法与线性变换复合\" class=\"headerlink\" title=\"矩阵乘法与线性变换复合\"></a>矩阵乘法与线性变换复合</h2><ul>\n<li>两个矩阵相乘：一个结合两种变换的变换</li>\n<li>复合矩阵求解：追踪${\\bf{i}}_{new}$和${\\bf{j}}_{new}$</li>\n<li>直观理解解释：<ul>\n<li>交换律（$AB \\neq BA$）不成立</li>\n<li>结合律（$(AB)C = A(BC)$）成立：复合变换AB等价于顺序进行变换B、A</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul>\n<li>动机：衡量线性变换使得空间伸缩了多少</li>\n<li>含义：<ul>\n<li>大小表示：单位正方形/立方体的面积/体积伸缩的比例</li>\n<li>正负表示：定向（右手法则）<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/det.png\" alt=\"行列式的含义\" title=\"行列式的含义\"><br></div></li>\n</ul>\n</li>\n<li>等价：<ul>\n<li>行列式等于0</li>\n<li>降维</li>\n</ul>\n</li>\n<li>直观解释$det(AB) = det(A)det(B)$：<ul>\n<li>$det(AB)$的含义是求解复合变换$AB$使得体积伸缩了几倍</li>\n<li>复合变换$AB$首先进行变换$B$使得空间伸缩了$det(B)$，然后进行变换$A$使得空间伸缩了$det(A)$倍</li>\n<li>所以体积总共伸缩了$det(A)det(B)$倍</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"逆矩阵、列空间与零空间\"><a href=\"#逆矩阵、列空间与零空间\" class=\"headerlink\" title=\"逆矩阵、列空间与零空间\"></a>逆矩阵、列空间与零空间</h2><h3 id=\"矩阵的用途：求解线性方程组\"><a href=\"#矩阵的用途：求解线性方程组\" class=\"headerlink\" title=\"矩阵的用途：求解线性方程组\"></a>矩阵的用途：求解线性方程组</h3><ul>\n<li>$A\\vec{\\bf{x}} = \\vec{\\bf{b}}$：找一个经过变换后等于$\\vec{\\bf{b}}$的向量</li>\n<li>$det(A) \\neq 0$：有唯一解，一一对应<ul>\n<li>假如有一个变换，能使得$\\vec{\\bf{b}}$变到$\\vec{\\bf{x}}$</li>\n<li>那么只要对$\\vec{\\bf{b}}$进行该变换即可得到要求的$\\vec{\\bf{x}}$</li>\n<li>因为对于$\\vec{\\bf{x}}$来说，先进行变换$A$后再进行该变换等于没变</li>\n<li>所以称该变换为$A$的逆</li>\n<li>由此含义可知，以下等价<ul>\n<li>逆存在</li>\n<li>没有降维</li>\n<li>行列式不等于0</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$det(A) = 0$：不一定有解，除非$\\vec{\\bf{b}}$恰好在降维所在空间（列空间），此时有无限多个解（e.g. 两个共线向量表示该方向上的向量）<ul>\n<li>行列式等于0</li>\n<li>降维（e.g. 空间由面变成线）</li>\n<li>逆不存在<ul>\n<li>假如逆存在</li>\n<li>存在一个变换（函数）由线map到面</li>\n<li>矛盾（违反函数的定义）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Rank\"><a href=\"#Rank\" class=\"headerlink\" title=\"Rank\"></a>Rank</h3><ul>\n<li>动机：衡量降维的程度（降到点/线/面）</li>\n<li>含义：降维后的维度</li>\n</ul>\n<h3 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h3><ul>\n<li>动机：衡量有多少向量被映射到零向量（$A\\vec{\\bf{x}}=\\vec{\\bf{0}}$）</li>\n<li>假如没有降维，只有零向量会被映射到零向量</li>\n<li>假如降维，会有其他向量被映射到零向量</li>\n<li>被映射到零向量的向量组成零空间<ul>\n<li>零空间有非零向量</li>\n<li>降维</li>\n<li>行列式等于0</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"非方阵\"><a href=\"#非方阵\" class=\"headerlink\" title=\"非方阵\"></a>非方阵</h2><ul>\n<li>$\\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 2 \\\\ 3 &amp; 3 \\end{bmatrix}$：<ul>\n<li>$\\begin{bmatrix} １ \\\\ ０\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\ 2  \\\\ 3 \\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} ２ \\\\ 2  \\\\ 3 \\end{bmatrix}$</li>\n</ul>\n</li>\n<li>$\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 2 &amp; 3 \\end{bmatrix}$：<ul>\n<li>$\\begin{bmatrix} １ \\\\ ０ \\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 2 \\\\ 2\\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ ０ \\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} 3 \\\\ 3\\end{bmatrix}$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"点积与对偶性\"><a href=\"#点积与对偶性\" class=\"headerlink\" title=\"点积与对偶性\"></a>点积与对偶性</h2><ul>\n<li>证明点积与顺序无关思路：<ul>\n<li>先证等长时顺序无关</li>\n<li>再证不等长时顺序无关（标量相乘）<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/dot_product_order.png\" alt=\"点积顺序无关\" title=\"点积顺序无关\"><br></div></li>\n</ul>\n</li>\n<li>对偶性：一一对应</li>\n<li>点积：<ul>\n<li>物理含义：投影长度再乘以被投影向量长度</li>\n<li>数学含义</li>\n</ul>\n</li>\n<li>证明对偶型思路：<ul>\n<li>验证单位向量点积：<ul>\n<li>定义投影到单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\ y \\end{bmatrix}$的投影长度这种线性变换</li>\n<li>得到投影长度这种线性变换对应的矩阵为$\\begin{bmatrix} x &amp; y\\end{bmatrix}$</li>\n<li>运算结果恰好等于与一个单位向量点积的数学形式<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/dot_product_projection.png\" alt=\"单位向量点积等价于投影长度这种线性变换\" title=\"单位向量点积等价于投影长度这种线性变换\"><br></div></li>\n</ul>\n</li>\n<li>验证非单位向量点积：<ul>\n<li>对于一个任意长度的向量$\\vec{\\bf{u}}=k\\vec{\\bf{\\hat{u}}}$</li>\n<li>定义投影到其单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\ y \\end{bmatrix}$的投影长度乘以其长度k这种线性变换</li>\n<li>得到投影长度这种线性变换对应的矩阵为$k\\begin{bmatrix} x &amp; y\\end{bmatrix}$</li>\n<li>运算结果恰好等于与一个非单位向量$\\vec{\\bf{u}}$点积的数学形式</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>将一个向量倒过来：投影到这个向量的投影长度乘以这个向量的长度这种线性变换对应的矩阵</li>\n</ul>\n<h2 id=\"叉乘\"><a href=\"#叉乘\" class=\"headerlink\" title=\"叉乘\"></a>叉乘</h2><ul>\n<li>叉乘：<ul>\n<li>向量：大小为两向量平行四边形面积，方向为右手定则的向量</li>\n<li>数学含义<ul>\n<li>二维：$det(\\begin{bmatrix} \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix})$</li>\n<li>三维：$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}} &amp; \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix})$</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>证明思路<ul>\n<li>根据叉乘的定义，他是由$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}} &amp; \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix}) = Xi + Yj +Zk$得到的向量$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$</li>\n<li>要证明$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$的大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则<ul>\n<li>$Xi + Yj +Zk$的大小的物理意义有两个（i.e. 所找向量必须同时满足以下两个性质）<ul>\n<li>$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$与$\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}$的点积</li>\n<li>$\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}$、$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行六面体的体积</li>\n</ul>\n</li>\n<li>当$Xi + Yj +Zk$大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则时同时成立</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"基变换\"><a href=\"#基变换\" class=\"headerlink\" title=\"基变换\"></a>基变换</h2><h3 id=\"向量表示（坐标）的变换\"><a href=\"#向量表示（坐标）的变换\" class=\"headerlink\" title=\"向量表示（坐标）的变换\"></a>向量表示（坐标）的变换</h3><ul>\n<li>从基于A系的坐标得到基于B系的坐标<ul>\n<li>对于一个坐标为$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$的向量</li>\n<li>我们可以假设这个坐标是基于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的（i.e. 2和3是向量关于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的线性组合）</li>\n<li>那么假如想要得到这个向量基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示，只需得到$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示（i.e. 从基于A系的坐标得到基于B系的坐标）</li>\n<li>从之前关于矩阵的两个理解可知该变换是个矩阵，记为M</li>\n</ul>\n</li>\n<li>根据上述得到的矩阵M直接取得B系到A系的坐标变换<ul>\n<li>假设B系到A系的坐标变换矩阵为M’</li>\n<li>那么对于A系下坐标为$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$的向量</li>\n<li>先进行M变换将该坐标转换为B系下的坐标，再进行M’变换将B系下坐标转换为A系下坐标，得到的结果必然还是$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$</li>\n<li>由此可得M’M是一个什么都不干的变换，所以M’是M的逆</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"变换表示（矩阵）的变换\"><a href=\"#变换表示（矩阵）的变换\" class=\"headerlink\" title=\"变换表示（矩阵）的变换\"></a>变换表示（矩阵）的变换</h3><ul>\n<li>对于同一个线性变换，基于不同的系有不同的表示</li>\n<li>假设线性变换在标准系下的表示为M，要求该线性变化在系A下的表示<ul>\n<li>一般来说，在表示一个变换的时候，希望输入的表示和输出的表示是基于同一个系的</li>\n<li>输入的表示取决于基</li>\n<li>输出的表示取决于变换后基的表示</li>\n<li>所以为了输入输出基于相同系，变换后基的表示也取表示输入的基</li>\n<li>因此矩阵里面的列向量要与输入基于相同的基</li>\n<li>所以下面第一步是将基统一</li>\n</ul>\n</li>\n<li>A：将A系下的表示转换为标准系下的表示</li>\n<li>MA：在标准系下进行转换</li>\n<li>A’MA：通过A’将标准系下的向量转换为A系下的表示</li>\n<li>因此A’MA也称M的【相似矩阵】</li>\n</ul>\n<h2 id=\"特征向量与特征值\"><a href=\"#特征向量与特征值\" class=\"headerlink\" title=\"特征向量与特征值\"></a>特征向量与特征值</h2><ul>\n<li>eigen vector：变换后方向不变的向量</li>\n<li>eigen value：变换后在对应eigen vector的伸缩量</li>\n<li>eigen basis：<ul>\n<li>有可能不够eigen vector来张成整个空间</li>\n<li>假如有的话，好处是：<ul>\n<li>在该变换下，基只发生了伸缩</li>\n<li>用该基表示变换的话可以是对角矩阵，因为变换后基只与变换前基的一项相关，其余全是0</li>\n<li>对角矩阵算幂很爽</li>\n</ul>\n</li>\n<li>应用：算A的幂<ul>\n<li>A是标准系下某个线性变换的表示</li>\n<li>假如A有eigen basis（i.e.　足够多的方向不变向量）</li>\n<li>求A对应的线性变换在eigen　basis下的表示（形式为B’AB），且该表示必然为对角矩阵【矩阵的对角化】</li>\n<li>在eigen basis下进行幂运算</li>\n<li>最后再把向量在eigen basis下的表示转回在标准系下的表示</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"抽象向量空间\"><a href=\"#抽象向量空间\" class=\"headerlink\" title=\"抽象向量空间\"></a>抽象向量空间</h2><ul>\n<li>基本就是1中所说的mathematician view（i.e.　满足一些性质（定义了加法、数乘，且运算封闭等等）的东西，physics view和cs　view等等都是向量的具体化）</li>\n<li>这里举了一个例子：<ul>\n<li>向量：多项式函数</li>\n<li>线性变换：求导</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>对线性代数有了更加直观、深刻的理解<ul>\n<li>理清了原先混淆的概念<br>+　知道了为什么一些操作的物理含义是这样的</li>\n</ul>\n</li>\n<li>对偶性的idea很美</li>\n<li>视频开始的句子很美</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://space.bilibili.com/88461692#!/channel/detail?cid=9450\" target=\"_blank\" rel=\"external\">线性代数的本质</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"向量\"><a href=\"#向量\" class=\"headerlink\" title=\"向量\"></a>向量</h2><h3 id=\"不同视角下的向量\"><a href=\"#不同视角下的向量\" class=\"headerlink\" title=\"不同视角下的向量\"></a>不同视角下的向量</h3><ul>\n<li>physics：箭头</li>\n<li>cs：数字列表（e.g. $ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $）</li>\n<li>mathematician： $ \\vec{\\bf{v}} $<ul>\n<li>抽象理解，任何相加和数乘有意义的东西</li>\n<li>physics和cs都只是其中一种实例，还可以是其他任意满足性质的东西（e.g. 函数）</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"physics-view与cs-view的联系\"><a href=\"#physics-view与cs-view的联系\" class=\"headerlink\" title=\"physics view与cs view的联系\"></a>physics view与cs view的联系</h3><ul>\n<li>桥梁：坐标系</li>\n<li>对偶性（独立推导、一一对应）：表示、运算（加法、数乘）</li>\n<li>作用<ul>\n<li>physics→cs：已知想要的图形变换效果，使用$ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $与计算机进行交流 </li>\n<li>cs→physics：已知$ \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} $相关的等式，使用图形直观理解物理意义</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"线性组合、张成的空间与基\"><a href=\"#线性组合、张成的空间与基\" class=\"headerlink\" title=\"线性组合、张成的空间与基\"></a>线性组合、张成的空间与基</h2><p>为什么称$a \\vec{\\bf{v}} + b \\vec{\\bf{w}}$是$\\vec{\\bf{v}}$与$\\vec{\\bf{w}}$的<strong>线性</strong>组合：固定$\\vec{\\bf{v}}$的条件下，任意取$\\vec{\\bf{w}}$，得到的是一条线</p>\n<h2 id=\"矩阵与线性变换\"><a href=\"#矩阵与线性变换\" class=\"headerlink\" title=\"矩阵与线性变换\"></a>矩阵与线性变换</h2><h3 id=\"Linear-Transformation\"><a href=\"#Linear-Transformation\" class=\"headerlink\" title=\"Linear Transformation\"></a>Linear Transformation</h3><ul>\n<li>transformation<ul>\n<li>相当于function（输入、输出）</li>\n<li>但之所以不用function，是因为transformation更有动感（由一个地方运动到另外一个地方）</li>\n<li>用网格线可视化（i.e. 画出网格线变换前后的位置）的时候，直观感受是空间发生变化</li>\n</ul>\n</li>\n<li>linear<ul>\n<li>直线仍然是直线、原点位置不变（i.e. 保持网格线平行并等距分布）</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"如何表示这个Linear-Transformation\"><a href=\"#如何表示这个Linear-Transformation\" class=\"headerlink\" title=\"如何表示这个Linear Transformation\"></a>如何表示这个Linear Transformation</h3><ul>\n<li>一般来说，描述一个变换，我们需要记下所有的输入、输出pair</li>\n<li>但由于线性变换具有保持网格线平行且等距分布的性质</li>\n<li>发现假如原向量$\\vec{\\bf{v}}$是$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合（伸缩），那么新向量$\\vec{\\bf{v}}_{new}$会是$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$相同的线性组合（伸缩）</li>\n<li>因此仅需记录$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$即有<strong>足够信息</strong>表示该线性变换（假设知道输入向量关于原向量$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$的线性组合，通过$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$即可以求得输出向量）</li>\n<li>【以上全部都只用到了图形化的解释，没有涉及坐标这个概念（i.e. 保存这个信息可以不用坐标，而是直接把新的箭头画出来，然后做箭头的加法和数乘）】</li>\n<li>【下面为了用数学来表示，开始引入坐标的概念】</li>\n<li>为了用坐标表示，需要选择一个基，于是选择原来的$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$作为基，之后的所有坐标都基于这个基</li>\n<li>引入坐标后，只需记录$\\hat{\\bf{i}}_{new}$和$\\hat{\\bf{j}}_{new}$的坐标，就有足够的信息表示该线性变换</li>\n<li>该坐标可以表示成矩阵的形式，因此<strong>矩阵与线性变换具有对偶性</strong></li>\n</ul>\n<h3 id=\"线性组合、线性变换、向量的表示\"><a href=\"#线性组合、线性变换、向量的表示\" class=\"headerlink\" title=\"线性组合、线性变换、向量的表示\"></a>线性组合、线性变换、向量的表示</h3><ul>\n<li>注意，下面三者是独立的：<ul>\n<li>向量的线性组合：$\\vec{\\bf{v}}$如何由$\\hat{\\bf{i}}$和$\\hat{\\bf{j}}$通过加法和数乘得到</li>\n<li>线性变换：具有保持网格线平行且等距分布性质的mapping</li>\n<li>向量的表示：引入一组基，使用一个数字列表表示沿各基的伸缩</li>\n</ul>\n</li>\n<li>e.g.<ul>\n<li>向量的线性组合：$\\bf{v} = 2\\bf{i} + 3\\bf{j}$<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/combination.png\" alt=\"向量的线性组合\" title=\"向量的线性组合\"><br></div></li>\n<li>线性变换<ul>\n<li>通过变换后有${\\bf{v}}_{new}$、${\\bf{i}}_{new}$和${\\bf{j}}_{new}$</li>\n<li>${\\bf{v}}_{new}$关于${\\bf{i}}_{new}$和${\\bf{j}}_{new}$的线性组合仍然为2和3<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/transformation.png\" alt=\"线性变换\" title=\"线性变换\"><br></div></li>\n</ul>\n</li>\n<li>向量的表示：<ul>\n<li>$\\bf{i}$和$\\bf{j}$的坐标可以基于任意基S</li>\n<li>${\\bf{i}}_{new}$和${\\bf{j}}_{new}$的坐标可以基于任意基T</li>\n<li>最终${\\bf{v}}_{new}$的坐标基于基T<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/representation.png\" alt=\"向量的表示\" title=\"向量的表示\"><br></div></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>一般情况下，对于向量的表示，都直接选取$\\bf{i}$和$\\bf{j}$作为基，这样做的好处是<ul>\n<li>可以直接从$\\bf{v}$的坐标知道线性组合</li>\n<li>不用再为${\\bf{i}}_{new}$和${\\bf{j}}_{new}$选取新的基</li>\n<li>最终的${\\bf{v}}_{new}$的坐标与$\\bf{v}$基于相同的基</li>\n</ul>\n</li>\n<li>核心在于三点：<ul>\n<li>追踪用于表示$\\vec{v}$的向量</li>\n<li>线性组合</li>\n<li>向量的表示</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"A-vec-bf-x-的两种理解\"><a href=\"#A-vec-bf-x-的两种理解\" class=\"headerlink\" title=\"$A\\vec{\\bf{x}}$的两种理解\"></a>$A\\vec{\\bf{x}}$的两种理解</h3><ul>\n<li>结果上来看，就是得到一个新的向量（箭头）</li>\n<li>由于该箭头使用坐标的形式表示的，那么肯定跟基相关</li>\n<li>过程可以有两个理解（主要是针对坐标x的理解不同）：<ul>\n<li>把A看成是线性变换的表示【有发生线性变换，向量的表示没变】：那么$A\\vec{\\bf{x}}$就可以理解为将在基1下的坐标x经过线性变换得到的在基1下的新坐标</li>\n<li>把A看成是基2【没有发生线性变换，只是向量的表示发生了变化】：那么$A\\vec{\\bf{x}}$就可以理解为在基2下的坐标x在基1下的坐标</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"矩阵乘法与线性变换复合\"><a href=\"#矩阵乘法与线性变换复合\" class=\"headerlink\" title=\"矩阵乘法与线性变换复合\"></a>矩阵乘法与线性变换复合</h2><ul>\n<li>两个矩阵相乘：一个结合两种变换的变换</li>\n<li>复合矩阵求解：追踪${\\bf{i}}_{new}$和${\\bf{j}}_{new}$</li>\n<li>直观理解解释：<ul>\n<li>交换律（$AB \\neq BA$）不成立</li>\n<li>结合律（$(AB)C = A(BC)$）成立：复合变换AB等价于顺序进行变换B、A</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul>\n<li>动机：衡量线性变换使得空间伸缩了多少</li>\n<li>含义：<ul>\n<li>大小表示：单位正方形/立方体的面积/体积伸缩的比例</li>\n<li>正负表示：定向（右手法则）<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/det.png\" alt=\"行列式的含义\" title=\"行列式的含义\"><br></div></li>\n</ul>\n</li>\n<li>等价：<ul>\n<li>行列式等于0</li>\n<li>降维</li>\n</ul>\n</li>\n<li>直观解释$det(AB) = det(A)det(B)$：<ul>\n<li>$det(AB)$的含义是求解复合变换$AB$使得体积伸缩了几倍</li>\n<li>复合变换$AB$首先进行变换$B$使得空间伸缩了$det(B)$，然后进行变换$A$使得空间伸缩了$det(A)$倍</li>\n<li>所以体积总共伸缩了$det(A)det(B)$倍</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"逆矩阵、列空间与零空间\"><a href=\"#逆矩阵、列空间与零空间\" class=\"headerlink\" title=\"逆矩阵、列空间与零空间\"></a>逆矩阵、列空间与零空间</h2><h3 id=\"矩阵的用途：求解线性方程组\"><a href=\"#矩阵的用途：求解线性方程组\" class=\"headerlink\" title=\"矩阵的用途：求解线性方程组\"></a>矩阵的用途：求解线性方程组</h3><ul>\n<li>$A\\vec{\\bf{x}} = \\vec{\\bf{b}}$：找一个经过变换后等于$\\vec{\\bf{b}}$的向量</li>\n<li>$det(A) \\neq 0$：有唯一解，一一对应<ul>\n<li>假如有一个变换，能使得$\\vec{\\bf{b}}$变到$\\vec{\\bf{x}}$</li>\n<li>那么只要对$\\vec{\\bf{b}}$进行该变换即可得到要求的$\\vec{\\bf{x}}$</li>\n<li>因为对于$\\vec{\\bf{x}}$来说，先进行变换$A$后再进行该变换等于没变</li>\n<li>所以称该变换为$A$的逆</li>\n<li>由此含义可知，以下等价<ul>\n<li>逆存在</li>\n<li>没有降维</li>\n<li>行列式不等于0</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$det(A) = 0$：不一定有解，除非$\\vec{\\bf{b}}$恰好在降维所在空间（列空间），此时有无限多个解（e.g. 两个共线向量表示该方向上的向量）<ul>\n<li>行列式等于0</li>\n<li>降维（e.g. 空间由面变成线）</li>\n<li>逆不存在<ul>\n<li>假如逆存在</li>\n<li>存在一个变换（函数）由线map到面</li>\n<li>矛盾（违反函数的定义）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Rank\"><a href=\"#Rank\" class=\"headerlink\" title=\"Rank\"></a>Rank</h3><ul>\n<li>动机：衡量降维的程度（降到点/线/面）</li>\n<li>含义：降维后的维度</li>\n</ul>\n<h3 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h3><ul>\n<li>动机：衡量有多少向量被映射到零向量（$A\\vec{\\bf{x}}=\\vec{\\bf{0}}$）</li>\n<li>假如没有降维，只有零向量会被映射到零向量</li>\n<li>假如降维，会有其他向量被映射到零向量</li>\n<li>被映射到零向量的向量组成零空间<ul>\n<li>零空间有非零向量</li>\n<li>降维</li>\n<li>行列式等于0</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"非方阵\"><a href=\"#非方阵\" class=\"headerlink\" title=\"非方阵\"></a>非方阵</h2><ul>\n<li>$\\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 2 \\\\ 3 &amp; 3 \\end{bmatrix}$：<ul>\n<li>$\\begin{bmatrix} １ \\\\ ０\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\ 2  \\\\ 3 \\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} ２ \\\\ 2  \\\\ 3 \\end{bmatrix}$</li>\n</ul>\n</li>\n<li>$\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 2 &amp; 3 \\end{bmatrix}$：<ul>\n<li>$\\begin{bmatrix} １ \\\\ ０ \\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}$ →　$\\begin{bmatrix} 2 \\\\ 2\\end{bmatrix}$</li>\n<li>$\\begin{bmatrix} 0 \\\\ ０ \\\\ 1\\end{bmatrix}$ →　$\\begin{bmatrix} 3 \\\\ 3\\end{bmatrix}$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"点积与对偶性\"><a href=\"#点积与对偶性\" class=\"headerlink\" title=\"点积与对偶性\"></a>点积与对偶性</h2><ul>\n<li>证明点积与顺序无关思路：<ul>\n<li>先证等长时顺序无关</li>\n<li>再证不等长时顺序无关（标量相乘）<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/dot_product_order.png\" alt=\"点积顺序无关\" title=\"点积顺序无关\"><br></div></li>\n</ul>\n</li>\n<li>对偶性：一一对应</li>\n<li>点积：<ul>\n<li>物理含义：投影长度再乘以被投影向量长度</li>\n<li>数学含义</li>\n</ul>\n</li>\n<li>证明对偶型思路：<ul>\n<li>验证单位向量点积：<ul>\n<li>定义投影到单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\ y \\end{bmatrix}$的投影长度这种线性变换</li>\n<li>得到投影长度这种线性变换对应的矩阵为$\\begin{bmatrix} x &amp; y\\end{bmatrix}$</li>\n<li>运算结果恰好等于与一个单位向量点积的数学形式<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br><img src=\"/2017/09/09/note-essence-of-linear-algrbra/dot_product_projection.png\" alt=\"单位向量点积等价于投影长度这种线性变换\" title=\"单位向量点积等价于投影长度这种线性变换\"><br></div></li>\n</ul>\n</li>\n<li>验证非单位向量点积：<ul>\n<li>对于一个任意长度的向量$\\vec{\\bf{u}}=k\\vec{\\bf{\\hat{u}}}$</li>\n<li>定义投影到其单位向量$\\vec{\\bf{\\hat{u}}}=\\begin{bmatrix} x \\\\ y \\end{bmatrix}$的投影长度乘以其长度k这种线性变换</li>\n<li>得到投影长度这种线性变换对应的矩阵为$k\\begin{bmatrix} x &amp; y\\end{bmatrix}$</li>\n<li>运算结果恰好等于与一个非单位向量$\\vec{\\bf{u}}$点积的数学形式</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>将一个向量倒过来：投影到这个向量的投影长度乘以这个向量的长度这种线性变换对应的矩阵</li>\n</ul>\n<h2 id=\"叉乘\"><a href=\"#叉乘\" class=\"headerlink\" title=\"叉乘\"></a>叉乘</h2><ul>\n<li>叉乘：<ul>\n<li>向量：大小为两向量平行四边形面积，方向为右手定则的向量</li>\n<li>数学含义<ul>\n<li>二维：$det(\\begin{bmatrix} \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix})$</li>\n<li>三维：$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}} &amp; \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix})$</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>证明思路<ul>\n<li>根据叉乘的定义，他是由$det(\\begin{bmatrix} {\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}} &amp; \\vec{\\bf{u}} &amp; \\vec{\\bf{v}} \\end{bmatrix}) = Xi + Yj +Zk$得到的向量$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$</li>\n<li>要证明$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$的大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则<ul>\n<li>$Xi + Yj +Zk$的大小的物理意义有两个（i.e. 所找向量必须同时满足以下两个性质）<ul>\n<li>$\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$与$\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}$的点积</li>\n<li>$\\begin{bmatrix} i \\\\ j \\\\ k \\end{bmatrix}$、$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行六面体的体积</li>\n</ul>\n</li>\n<li>当$Xi + Yj +Zk$大小为$\\vec{\\bf{u}}$和 $\\vec{\\bf{v}}$构成的平行四边型的面积，方向为右手定则时同时成立</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"基变换\"><a href=\"#基变换\" class=\"headerlink\" title=\"基变换\"></a>基变换</h2><h3 id=\"向量表示（坐标）的变换\"><a href=\"#向量表示（坐标）的变换\" class=\"headerlink\" title=\"向量表示（坐标）的变换\"></a>向量表示（坐标）的变换</h3><ul>\n<li>从基于A系的坐标得到基于B系的坐标<ul>\n<li>对于一个坐标为$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$的向量</li>\n<li>我们可以假设这个坐标是基于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的（i.e. 2和3是向量关于$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$的线性组合）</li>\n<li>那么假如想要得到这个向量基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示，只需得到$\\vec{\\bf{u}}$和$\\vec{\\bf{v}}$基于$\\vec{\\bf{i}}$和$\\vec{\\bf{j}}$的表示（i.e. 从基于A系的坐标得到基于B系的坐标）</li>\n<li>从之前关于矩阵的两个理解可知该变换是个矩阵，记为M</li>\n</ul>\n</li>\n<li>根据上述得到的矩阵M直接取得B系到A系的坐标变换<ul>\n<li>假设B系到A系的坐标变换矩阵为M’</li>\n<li>那么对于A系下坐标为$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$的向量</li>\n<li>先进行M变换将该坐标转换为B系下的坐标，再进行M’变换将B系下坐标转换为A系下坐标，得到的结果必然还是$\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$</li>\n<li>由此可得M’M是一个什么都不干的变换，所以M’是M的逆</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"变换表示（矩阵）的变换\"><a href=\"#变换表示（矩阵）的变换\" class=\"headerlink\" title=\"变换表示（矩阵）的变换\"></a>变换表示（矩阵）的变换</h3><ul>\n<li>对于同一个线性变换，基于不同的系有不同的表示</li>\n<li>假设线性变换在标准系下的表示为M，要求该线性变化在系A下的表示<ul>\n<li>一般来说，在表示一个变换的时候，希望输入的表示和输出的表示是基于同一个系的</li>\n<li>输入的表示取决于基</li>\n<li>输出的表示取决于变换后基的表示</li>\n<li>所以为了输入输出基于相同系，变换后基的表示也取表示输入的基</li>\n<li>因此矩阵里面的列向量要与输入基于相同的基</li>\n<li>所以下面第一步是将基统一</li>\n</ul>\n</li>\n<li>A：将A系下的表示转换为标准系下的表示</li>\n<li>MA：在标准系下进行转换</li>\n<li>A’MA：通过A’将标准系下的向量转换为A系下的表示</li>\n<li>因此A’MA也称M的【相似矩阵】</li>\n</ul>\n<h2 id=\"特征向量与特征值\"><a href=\"#特征向量与特征值\" class=\"headerlink\" title=\"特征向量与特征值\"></a>特征向量与特征值</h2><ul>\n<li>eigen vector：变换后方向不变的向量</li>\n<li>eigen value：变换后在对应eigen vector的伸缩量</li>\n<li>eigen basis：<ul>\n<li>有可能不够eigen vector来张成整个空间</li>\n<li>假如有的话，好处是：<ul>\n<li>在该变换下，基只发生了伸缩</li>\n<li>用该基表示变换的话可以是对角矩阵，因为变换后基只与变换前基的一项相关，其余全是0</li>\n<li>对角矩阵算幂很爽</li>\n</ul>\n</li>\n<li>应用：算A的幂<ul>\n<li>A是标准系下某个线性变换的表示</li>\n<li>假如A有eigen basis（i.e.　足够多的方向不变向量）</li>\n<li>求A对应的线性变换在eigen　basis下的表示（形式为B’AB），且该表示必然为对角矩阵【矩阵的对角化】</li>\n<li>在eigen basis下进行幂运算</li>\n<li>最后再把向量在eigen basis下的表示转回在标准系下的表示</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"抽象向量空间\"><a href=\"#抽象向量空间\" class=\"headerlink\" title=\"抽象向量空间\"></a>抽象向量空间</h2><ul>\n<li>基本就是1中所说的mathematician view（i.e.　满足一些性质（定义了加法、数乘，且运算封闭等等）的东西，physics view和cs　view等等都是向量的具体化）</li>\n<li>这里举了一个例子：<ul>\n<li>向量：多项式函数</li>\n<li>线性变换：求导</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>对线性代数有了更加直观、深刻的理解<ul>\n<li>理清了原先混淆的概念<br>+　知道了为什么一些操作的物理含义是这样的</li>\n</ul>\n</li>\n<li>对偶性的idea很美</li>\n<li>视频开始的句子很美</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://space.bilibili.com/88461692#!/channel/detail?cid=9450\">线性代数的本质</a></li>\n</ul>\n"},{"title":"课程笔记《Learning How to Learn》","description":["关于学习方法的一门课程"],"date":"2017-09-09T07:16:28.000Z","_content":"\n## 思维导图\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img learning_how_to_learn.png Learning How to Learn思维导图 %}\n</div>\n\n## References\n\n- [Learning How to Learn](https://www.coursera.org/learn/learning-how-to-learn)","source":"_posts/note-learning-how-to-learn.md","raw":"---\ntitle: 课程笔记《Learning How to Learn》\ntags:\n  - 学习方法\ndescription:\n  - 关于学习方法的一门课程\ncategories:\n  - 课程笔记\ndate: 2017-09-09 15:16:28\n---\n\n## 思维导图\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img learning_how_to_learn.png Learning How to Learn思维导图 %}\n</div>\n\n## References\n\n- [Learning How to Learn](https://www.coursera.org/learn/learning-how-to-learn)","slug":"note-learning-how-to-learn","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrq4000sxuvtjbc6k0j5","content":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-learning-how-to-learn/learning_how_to_learn.png\" alt=\"Learning How to Learn思维导图\" title=\"Learning How to Learn思维导图\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.coursera.org/learn/learning-how-to-learn\" target=\"_blank\" rel=\"external\">Learning How to Learn</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-learning-how-to-learn/learning_how_to_learn.png\" alt=\"Learning How to Learn思维导图\" title=\"Learning How to Learn思维导图\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.coursera.org/learn/learning-how-to-learn\">Learning How to Learn</a></li>\n</ul>\n"},{"title":"课程笔记《MIT18.06线性代数》","description":["MIT Gilbert Strang深入浅出的线性代数课程"],"date":"2017-09-09T07:09:17.000Z","_content":"\n## 思维导图\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img linear_algebra.png MIT18.06线性代数思维导图 %}\n</div>\n\n## References\n\n- [MIT18.06线性代数](http://open.163.com/special/opencourse/daishu.html)","source":"_posts/note-linear-algebra.md","raw":"---\ntitle: 课程笔记《MIT18.06线性代数》\ntags:\n  - 线性代数\ndescription:\n  - MIT Gilbert Strang深入浅出的线性代数课程\ncategories:\n  - 课程笔记\ndate: 2017-09-09 15:09:17\n---\n\n## 思维导图\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img linear_algebra.png MIT18.06线性代数思维导图 %}\n</div>\n\n## References\n\n- [MIT18.06线性代数](http://open.163.com/special/opencourse/daishu.html)","slug":"note-linear-algebra","published":1,"updated":"2018-01-24T03:42:48.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrq6000uxuvtp42ukqns","content":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-linear-algebra/linear_algebra.png\" alt=\"MIT18.06线性代数思维导图\" title=\"MIT18.06线性代数思维导图\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://open.163.com/special/opencourse/daishu.html\" target=\"_blank\" rel=\"external\">MIT18.06线性代数</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-linear-algebra/linear_algebra.png\" alt=\"MIT18.06线性代数思维导图\" title=\"MIT18.06线性代数思维导图\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://open.163.com/special/opencourse/daishu.html\">MIT18.06线性代数</a></li>\n</ul>\n"},{"title":"课程笔记《UCL强化学习》","description":["UCL David Silver的强化学习课程"],"date":"2017-09-09T07:21:42.000Z","_content":"\n## 思维导图\n\n### Intro to RL\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec1_Intro_to_RL.png Intro to RL %}\n</div>\n\n### MDP\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec2_MDP.png MDP %}\n</div>\n\n### Planning by DP\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec3_Planning_by_DP.png Planning by DP %}\n</div>\n\n### Model-Free Prediction\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec4_Model_Free_Prediction.png Model-Free Prediction %}\n</div>\n\n### Model-Free Control\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec5_Model_Free_Control.png Model-Free Control %}\n</div>\n\n### Value Function Approximation\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec6_Value_Function_Approximation.png Value Function Approximation %}\n</div>\n\n### Policy Gradient\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec7_Policy_Gradient.png Policy Gradient %}\n</div>\n\n### Integrating Learning and Planning\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec8_Integrating_Learning_and_Planning.png Integrating Learning and Planning %}\n</div>\n\n### Exploration and Exploitation\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec9_Exploration_and_Exploitation.png Exploration and Exploitation %}\n</div>\n\n## References\n\n- [强化学习课程](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)\n- [强化学习教材](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf)","source":"_posts/note-reinforcement-learning.md","raw":"---\ntitle: 课程笔记《UCL强化学习》\ntags:\n  - 强化学习\ndescription:\n  - UCL David Silver的强化学习课程\ncategories:\n  - 课程笔记\ndate: 2017-09-09 15:21:42\n---\n\n## 思维导图\n\n### Intro to RL\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec1_Intro_to_RL.png Intro to RL %}\n</div>\n\n### MDP\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec2_MDP.png MDP %}\n</div>\n\n### Planning by DP\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec3_Planning_by_DP.png Planning by DP %}\n</div>\n\n### Model-Free Prediction\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec4_Model_Free_Prediction.png Model-Free Prediction %}\n</div>\n\n### Model-Free Control\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec5_Model_Free_Control.png Model-Free Control %}\n</div>\n\n### Value Function Approximation\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec6_Value_Function_Approximation.png Value Function Approximation %}\n</div>\n\n### Policy Gradient\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec7_Policy_Gradient.png Policy Gradient %}\n</div>\n\n### Integrating Learning and Planning\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec8_Integrating_Learning_and_Planning.png Integrating Learning and Planning %}\n</div>\n\n### Exploration and Exploitation\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img lec9_Exploration_and_Exploitation.png Exploration and Exploitation %}\n</div>\n\n## References\n\n- [强化学习课程](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)\n- [强化学习教材](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf)","slug":"note-reinforcement-learning","published":1,"updated":"2018-01-24T03:42:48.492Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrq7000xxuvt9y2qp03n","content":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><h3 id=\"Intro-to-RL\"><a href=\"#Intro-to-RL\" class=\"headerlink\" title=\"Intro to RL\"></a>Intro to RL</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec1_Intro_to_RL.png\" alt=\"Intro to RL\" title=\"Intro to RL\"><br></div>\n\n<h3 id=\"MDP\"><a href=\"#MDP\" class=\"headerlink\" title=\"MDP\"></a>MDP</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec2_MDP.png\" alt=\"MDP\" title=\"MDP\"><br></div>\n\n<h3 id=\"Planning-by-DP\"><a href=\"#Planning-by-DP\" class=\"headerlink\" title=\"Planning by DP\"></a>Planning by DP</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec3_Planning_by_DP.png\" alt=\"Planning by DP\" title=\"Planning by DP\"><br></div>\n\n<h3 id=\"Model-Free-Prediction\"><a href=\"#Model-Free-Prediction\" class=\"headerlink\" title=\"Model-Free Prediction\"></a>Model-Free Prediction</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec4_Model_Free_Prediction.png\" alt=\"Model-Free Prediction\" title=\"Model-Free Prediction\"><br></div>\n\n<h3 id=\"Model-Free-Control\"><a href=\"#Model-Free-Control\" class=\"headerlink\" title=\"Model-Free Control\"></a>Model-Free Control</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec5_Model_Free_Control.png\" alt=\"Model-Free Control\" title=\"Model-Free Control\"><br></div>\n\n<h3 id=\"Value-Function-Approximation\"><a href=\"#Value-Function-Approximation\" class=\"headerlink\" title=\"Value Function Approximation\"></a>Value Function Approximation</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec6_Value_Function_Approximation.png\" alt=\"Value Function Approximation\" title=\"Value Function Approximation\"><br></div>\n\n<h3 id=\"Policy-Gradient\"><a href=\"#Policy-Gradient\" class=\"headerlink\" title=\"Policy Gradient\"></a>Policy Gradient</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec7_Policy_Gradient.png\" alt=\"Policy Gradient\" title=\"Policy Gradient\"><br></div>\n\n<h3 id=\"Integrating-Learning-and-Planning\"><a href=\"#Integrating-Learning-and-Planning\" class=\"headerlink\" title=\"Integrating Learning and Planning\"></a>Integrating Learning and Planning</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec8_Integrating_Learning_and_Planning.png\" alt=\"Integrating Learning and Planning\" title=\"Integrating Learning and Planning\"><br></div>\n\n<h3 id=\"Exploration-and-Exploitation\"><a href=\"#Exploration-and-Exploitation\" class=\"headerlink\" title=\"Exploration and Exploitation\"></a>Exploration and Exploitation</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec9_Exploration_and_Exploitation.png\" alt=\"Exploration and Exploitation\" title=\"Exploration and Exploitation\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html\" target=\"_blank\" rel=\"external\">强化学习课程</a></li>\n<li><a href=\"http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\" target=\"_blank\" rel=\"external\">强化学习教材</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"思维导图\"><a href=\"#思维导图\" class=\"headerlink\" title=\"思维导图\"></a>思维导图</h2><h3 id=\"Intro-to-RL\"><a href=\"#Intro-to-RL\" class=\"headerlink\" title=\"Intro to RL\"></a>Intro to RL</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec1_Intro_to_RL.png\" alt=\"Intro to RL\" title=\"Intro to RL\"><br></div>\n\n<h3 id=\"MDP\"><a href=\"#MDP\" class=\"headerlink\" title=\"MDP\"></a>MDP</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec2_MDP.png\" alt=\"MDP\" title=\"MDP\"><br></div>\n\n<h3 id=\"Planning-by-DP\"><a href=\"#Planning-by-DP\" class=\"headerlink\" title=\"Planning by DP\"></a>Planning by DP</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec3_Planning_by_DP.png\" alt=\"Planning by DP\" title=\"Planning by DP\"><br></div>\n\n<h3 id=\"Model-Free-Prediction\"><a href=\"#Model-Free-Prediction\" class=\"headerlink\" title=\"Model-Free Prediction\"></a>Model-Free Prediction</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec4_Model_Free_Prediction.png\" alt=\"Model-Free Prediction\" title=\"Model-Free Prediction\"><br></div>\n\n<h3 id=\"Model-Free-Control\"><a href=\"#Model-Free-Control\" class=\"headerlink\" title=\"Model-Free Control\"></a>Model-Free Control</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec5_Model_Free_Control.png\" alt=\"Model-Free Control\" title=\"Model-Free Control\"><br></div>\n\n<h3 id=\"Value-Function-Approximation\"><a href=\"#Value-Function-Approximation\" class=\"headerlink\" title=\"Value Function Approximation\"></a>Value Function Approximation</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec6_Value_Function_Approximation.png\" alt=\"Value Function Approximation\" title=\"Value Function Approximation\"><br></div>\n\n<h3 id=\"Policy-Gradient\"><a href=\"#Policy-Gradient\" class=\"headerlink\" title=\"Policy Gradient\"></a>Policy Gradient</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec7_Policy_Gradient.png\" alt=\"Policy Gradient\" title=\"Policy Gradient\"><br></div>\n\n<h3 id=\"Integrating-Learning-and-Planning\"><a href=\"#Integrating-Learning-and-Planning\" class=\"headerlink\" title=\"Integrating Learning and Planning\"></a>Integrating Learning and Planning</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec8_Integrating_Learning_and_Planning.png\" alt=\"Integrating Learning and Planning\" title=\"Integrating Learning and Planning\"><br></div>\n\n<h3 id=\"Exploration-and-Exploitation\"><a href=\"#Exploration-and-Exploitation\" class=\"headerlink\" title=\"Exploration and Exploitation\"></a>Exploration and Exploitation</h3><div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/09/09/note-reinforcement-learning/lec9_Exploration_and_Exploitation.png\" alt=\"Exploration and Exploitation\" title=\"Exploration and Exploitation\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html\">强化学习课程</a></li>\n<li><a href=\"http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\">强化学习教材</a></li>\n</ul>\n"},{"title":"论文笔记《Reciprocal n-body Collision Avoidance》","description":["应用广泛的经典底层避障算法"],"date":"2017-01-14T13:41:51.000Z","_content":"\n\n## 简介\n\nORCA是经典的分布式底层避障算法，其任务是（对于A）：\n\n- **输入**：A与B的形状（$r\\_A$、$r\\_B$）、位置（$p\\_A$、$p\\_B$）、速度（$v\\_A$、$v\\_B$），以及A的偏好量（$v\\_A^{pref}$）和限制量（$v\\_A^{max}$）。\n- **输出**：不会发生碰撞的$v\\_A^{\\prime}$。\n- **特点**：双方只要都采用ORCA，那么双方**无需通信**，**分布式**地求出来的$v\\_A^{\\prime}$与$v\\_B^{\\prime}$也不会发生碰撞。\n\n跟RVO一样，ORCA也是基于VO的，但更加实用，因为：\n\n- **效果上**：考虑了**速度的大小**，使得筛选粒度更细。不像VO和RVO只考虑速度方向。\n- **效率上**：求解过程基本只用到了**线性规划**，比较高效。不像VO和RVO有大量的非线性求解。\n\n## 引入时间窗口$\\tau$\n\n1. 如下图(a)，在**空间坐标**上，有\n    + 位于$\\bf{p\\_B}$，半径为$r\\_B$的$B$\n    + 位于$\\bf{p\\_A}$，半径为$r\\_A$的$A$\n2. 如下图(b)，引入时间$\\tau$，将**空间坐标**转换为**速度坐标**\n    1. **相对化空间坐标**：将图(a)化为以$A$为原点，并将$A$化为质点\n    2. **转为速度坐标**：假设$B$静止，那么当$v\\_A * \\tau$达到红色弧线时，$A$与$B$会发生碰撞，所以VO内，速度上限为$\\frac{p\\_{弧线上的点}}{\\tau}$，得到绿色弧线。\n    3. 至此得到$VO^{\\tau}\\_{A|B}$，即绿色弧线与两射线组成的类扇形\n3. 如下图(c)，**考虑$B$取的速度集合$V\\_B$**，对假设$B$静止得到的$VO^{\\tau}\\_{A|B}$求闵氏和，得$CA^{\\tau}\\_{A|B}(V\\_B)$\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img time_interval.png 时间窗口 %}\n</div>\n\n下面给出各变量规范的数学定义：\n\n- $D({\\bf{p}},r)$：以$p$为圆心$r$为半径的圆\n $$D({\\bf{p}},r)=\\\\{ \\ {\\bf{q}} \\ | \\ \\\\| {\\bf{q}} - {\\bf{p}} \\\\| < r \\ \\\\}$$\n\n- $VO^{\\tau}\\_{A|B}$：绿色弧线和两条射线组成的类扇形\n $$VO^{\\tau}\\_{A|B} = \\\\{ \\ {\\bf{v}} \\ | \\ \\exists t \\in [0, \\tau], t {\\bf{v}} \\in D({\\bf{p\\_B}} -{\\bf{p\\_A}}, r\\_A+r\\_B) \\  \\\\}$$\n\n- $CA^{\\tau}\\_{A|B}(V\\_B)$：考虑$B$取速度集合$V\\_B$\n $$CA^{\\tau}\\_{A|B}(V\\_B)=\\\\{ \\ {\\bf{v}} \\ | \\ {\\bf{v}} \\notin VO^{\\tau}\\_{A|B} \\oplus V\\_B \\ \\\\}，　其中\\oplus为闵氏和$$\n\n至此，成功在VO的基础上引入了时间窗口，避障的时候不仅考虑**速度方向**，还考虑**速度大小**。\n\np.s. 关于VO，闵氏和等内容可以参考上一篇文章{% post_link paper-rvo 论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》 %}\n\n## ORCA定义\n\nORCA全称为Optimal Reciprocal Collision Avoidance，含义就是在上述的$CA^{\\tau}\\_{A|B}(V\\_B)$中选择最优的一个。\n\n论文对最优的定义大概可以理解为**在$v^{opt}$附近的合法值越多越好**，即为满足下列条件的$V\\_A$与$V\\_B$对：\n\n- **Reciprocally collision avoiding**：很好理解，在对方的$CA$内即可，即\n$$V\\_A \\subseteq CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B \\subseteq CA^{\\tau}\\_{B|A}(V\\_A)$$\n- **Reciprocally maximal**：两个速度集是最大的不碰撞速度集，$V\\_A$或$V\\_B$都没有更多的避撞速度可选\n$$V\\_A = CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B = CA^{\\tau}\\_{B|A}(V\\_A)$$\n- **包含最多$v^{opt}$附近的避撞速度**：$v^{opt}$是一个预设值，可以理解为偏好值，论文中的最优希望：\n\t+ $v^{opt}$附近的值越多越好\n- **双方在各自$v^{opt}$附近的避撞速度数量一样**：$A$和$B$各自有$v^{opt}\\_A$和$v^{opt}\\_B$，论文中的最优希望：\n\t+ $A$求出来在$v^{opt}\\_A$附近的避撞速度的数量$n\\_A$\n\t+ $B$求出来在$v^{opt}\\_B$附近的避撞速度的数量$n\\_B$\n\t+ $n\\_A = n\\_B$\n\n那么用数学语言来描述上面几条性质就是：\n$$| ORCA^{\\tau}\\_{A|B} \\cap D({\\bf{v^{opt}\\_A}}, r) | = | ORCA^{\\tau}\\_{B|A} \\cap D({\\bf{v^{opt}\\_B}}, r) | \\ge min(|V\\_A \\cap D({\\bf{v^{opt}\\_A}}, r)|, |V\\_B \\cap D({\\bf{v^{opt}\\_B}}, r)|)$$\n\n其中\n\n- $ORCA^{\\tau}\\_{A|B}$与$ORCA^{\\tau}\\_{B|A}$：表示要求的最优速度集对，且满足reciprocally maximal，即\n$$ORCA^{\\tau}\\_{A|B} = CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A}) \\land ORCA^{\\tau}\\_{B|A} = CA^{\\tau}\\_{B|A}(ORCA^{\\tau}\\_{A|B})$$\n- $V\\_A$与$V\\_B$：表示任意reciprocally collision avoiding的速度集对，即\n$$V\\_A \\subseteq CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B \\subseteq CA^{\\tau}\\_{B|A}(V\\_A)$$\n- $r$：表示了“附近”的大小\n- $D({\\bf{v^{opt}}},r)$：表示$v^{opt}$附近的速度\n\n## ORCA求解\n\n论文采用构造法求解出满足上述性质的ORCA，下面先说怎么做，再说为什么这样做就能满足性质。\n\n下面从$A$的角度介绍构造过程（图示如下），这里假设$A$知道$\\bf{v^{opt}\\_B}$\n\n1. 求解**相对速度**${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$\n2. 求解$VO^{\\tau}\\_{A|B}$**边界上**离相对速度${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$**最近的点**$\\bf{x}$\n3. 求解从相对速度${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$指向$\\bf{x}$的向量$\\bf{u}$\n4. 求解与${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$垂直的直线，将空间分为两个平面\n5. 直线${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$指向的平面即为所求$ORCA^{\\tau}\\_{A|B}$\n\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img orca_sol.png ORCA求解 %}\n</div>\n\n下面逐条解释为什么符合性质：\n\n- **Reciprocally collision avoiding**：\n\t+ 由构造过程可知${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}+{\\bf{u}}$不在$VO^{\\tau}\\_{A|B}$内\n\t+ $A$取新速度${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$\n\t+ $B$取新速度${\\bf{v^{opt}\\_B}} - \\frac{1}{2}{\\bf{u}}$\n\t+ 此时，相对速度为${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}} - ({\\bf{v^{opt}\\_B}} - \\frac{1}{2}{\\bf{u}}) = {\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}+{\\bf{u}}$，所以不在$VO^{\\tau}\\_{A|B}$内\n- **Reciprocally maximal**：\n\t+ 要证$ORCA^{\\tau}\\_{A|B} = CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$\n\t+ 先求$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$：\n\t\t* 回忆之前引入时间窗口$\\tau$部分，构造$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$分两步\n\t\t\t1. 先将类扇形往${\\bf{v^{opt}\\_B}}-\\frac{1}{2}{\\bf{u}}$移动\n\t\t\t2. 沿直线$ORCA^{\\tau}\\_{B|A}$方向扫\n\t\t* 先将类扇形往$\\bf{v^{opt}\\_B}$方向平移（此时，类扇形中的${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$到了$\\bf{v^{opt}\\_A}$，且$\\bf{x}$与$\\bf{v^{opt}\\_A}$的中点在直线$ORCA^{\\tau}\\_{A|B}$上）\n\t\t* 别忘了还要将类扇形往$-{\\frac{1}{2}\\bf{u}}$方向移动（此时，$\\bf{x}$刚好位于直线$ORCA^{\\tau}\\_{A|B}$上）\n\t\t* 最后再沿直线$ORCA^{\\tau}\\_{B|A}$方向扫，得$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$\n\t+ 而显然$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$正是$ORCA^{\\tau}\\_{A|B}$\n- **包含最多$v^{opt}$附近的避撞速度**：\n\t+ 选择了最近的$\\bf{x}$进行“突围”，那“舍弃”的速度应该是最少的\n\t+ p.s. 这一点的证明不太严谨，有更好的证明欢迎留言探讨\n- **双方在各自$v^{opt}$附近的避撞速度数量一样**：\n\t+ $\\bf{v^{opt}\\_A}$到$ORCA^{\\tau}\\_{A|B}$的距离等于$\\bf{v^{opt}\\_B}$到$ORCA^{\\tau}\\_{B|A}$的距离\n\t+ 定义“附近”用的是圆\n\n最后给出ORCA的数学定义\n\n$$ORCA^{\\tau}\\_{A|B} = \\\\{ \\ {\\bf{v}} \\ | \\ ({\\bf{v}} - ({\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}})) \\cdot {\\bf{n}} \\ge 0 \\  \\\\}$$\n\n其中\n\n- ${\\bf{u}}$是最小偏移\n$${\\bf{u}} = (\\mathop{argmin}\\_{ {\\bf{v}} \\in \\partial VO^{\\tau}\\_{A|B}}{\\\\| {\\bf{v}} - ( {\\bf{v^{opt}\\_A}} - {\\bf{v^{opt}\\_B}} )  \\\\|}) - ({\\bf{v^{opt}\\_A}} - {\\bf{v^{opt}\\_B}})$$\n- ${\\bf{n}}$是跟${\\bf{u}}$同向的法向量\n- 用${\\bf{v}} \\cdot {\\bf{n}} \\ge 0$来表示一个平面\n\n## ORCA基本用法\n\n1. 对其他所有agents的$ORCA$求交（线性规划），再与自己可选速度求交集，得候选速度集$ORCA^{\\tau}\\_{A}$\n$$ORCA^{\\tau}\\_{A}=D({\\bf{0}}, {\\bf{v^{max}\\_A}}) \\cap \\mathop{\\bigcap}\\_{B \\ne A}{ORCA^{\\tau}\\_{A|B}}$$\n2. 在候选集中求解跟自己偏好速度最近的一个速度${\\bf{v^{new}\\_A}}$\n$${\\bf{v^{new}\\_A}} = \\mathop{argmin}\\_{ {\\bf{v}} \\in ORCA^{\\tau}\\_A}{\\\\| {\\bf{v}} - {\\bf{v^{opt}\\_A}} \\\\|}$$\n\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img orca_app.png ORCA基本用法 %}\n</div>\n\n可见ORCA的求解，就是在$ORCA^{\\tau}\\_{A}$内，优化目标函数$\\\\| {\\bf{v}} - {\\bf{v^{opt}\\_A}} \\\\|$，所以计算效率很高。\n\n## 关于${\\bf{v^{opt}}}$的选择\n\n为方便阐述，假设${\\bf{v\\_A}} = {\\bf{0}}$，这样$ORCA^{\\tau}\\_{A|B}$就只是从原点移$\\frac{1}{2}{\\bf{u}}$\n\n讨论${\\bf{v^{opt}}}$的选择跟$ORCA^{\\tau}\\_{A}$的关系\n\n- ${\\bf{0}}$\n\t+ 一定有解，如下图(c)。很好理解，这相当于假设了别人都静止，那只要自己也静止，必然有解。\n\t+ 密集情况下容易死锁，因为这样相当于只考虑位置信息，而不考虑速度信息\n- ${\\bf{v^{pref}}}$\n\t+ 密集情况下容易无解，如下图(b)。因为一般来说${\\bf{v^{pref}}}$都比较大\n\t+ 而且实际上也不可能观察得到别人的${\\bf{v^{pref}}}$\n- ${\\bf{v\\_{curr}}}$\n\t+ 两者的折中\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img opt.png opt选择 %}\n</div>\n\n## 无解情况的处理\n\n基本思路跟线性代数里面的投影差不多：选择离合法速度最近的速度\n\n$$ {\\bf{v^{new}\\_{A}}} = \\mathop{argmin}\\_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}\\_A}})} {\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}} $$\n\n下面慢慢拆分来看\n\n1. $d\\_{A|B}({\\bf{v}})$：表示${\\bf{v}}$的“违规”程度。定义为${\\bf{v}}$到ORCA分割线的有向距离，${\\bf{v}}$在ORCA外则取正\n2. ${\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}}$：对每个速度${\\bf{v}}$，取“违规”最多的值作为它的“违规程度”\n3. $\\mathop{argmin}\\_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}\\_A}})} {\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}}$：在可行速度内，求”违规程度”最小的速度\n\n## 收获\n- 改进可以从以下角度考虑：\n\t+ **效果**：只考虑了方向，没有考虑速度\n\t+ **效率**：计算复杂度\n\n## References\n\n- [Reciprocal n-body Collision Avoidance](http://gamma.cs.unc.edu/ORCA/publications/ORCA.pdf)\n","source":"_posts/paper-orca.md","raw":"---\ntitle: 论文笔记《Reciprocal n-body Collision Avoidance》\ntags:\n  - ORCA\ndescription:\n  - 应用广泛的经典底层避障算法\ncategories:\n  - 论文笔记\ndate: 2017-01-14 21:41:51\n---\n\n\n## 简介\n\nORCA是经典的分布式底层避障算法，其任务是（对于A）：\n\n- **输入**：A与B的形状（$r\\_A$、$r\\_B$）、位置（$p\\_A$、$p\\_B$）、速度（$v\\_A$、$v\\_B$），以及A的偏好量（$v\\_A^{pref}$）和限制量（$v\\_A^{max}$）。\n- **输出**：不会发生碰撞的$v\\_A^{\\prime}$。\n- **特点**：双方只要都采用ORCA，那么双方**无需通信**，**分布式**地求出来的$v\\_A^{\\prime}$与$v\\_B^{\\prime}$也不会发生碰撞。\n\n跟RVO一样，ORCA也是基于VO的，但更加实用，因为：\n\n- **效果上**：考虑了**速度的大小**，使得筛选粒度更细。不像VO和RVO只考虑速度方向。\n- **效率上**：求解过程基本只用到了**线性规划**，比较高效。不像VO和RVO有大量的非线性求解。\n\n## 引入时间窗口$\\tau$\n\n1. 如下图(a)，在**空间坐标**上，有\n    + 位于$\\bf{p\\_B}$，半径为$r\\_B$的$B$\n    + 位于$\\bf{p\\_A}$，半径为$r\\_A$的$A$\n2. 如下图(b)，引入时间$\\tau$，将**空间坐标**转换为**速度坐标**\n    1. **相对化空间坐标**：将图(a)化为以$A$为原点，并将$A$化为质点\n    2. **转为速度坐标**：假设$B$静止，那么当$v\\_A * \\tau$达到红色弧线时，$A$与$B$会发生碰撞，所以VO内，速度上限为$\\frac{p\\_{弧线上的点}}{\\tau}$，得到绿色弧线。\n    3. 至此得到$VO^{\\tau}\\_{A|B}$，即绿色弧线与两射线组成的类扇形\n3. 如下图(c)，**考虑$B$取的速度集合$V\\_B$**，对假设$B$静止得到的$VO^{\\tau}\\_{A|B}$求闵氏和，得$CA^{\\tau}\\_{A|B}(V\\_B)$\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img time_interval.png 时间窗口 %}\n</div>\n\n下面给出各变量规范的数学定义：\n\n- $D({\\bf{p}},r)$：以$p$为圆心$r$为半径的圆\n $$D({\\bf{p}},r)=\\\\{ \\ {\\bf{q}} \\ | \\ \\\\| {\\bf{q}} - {\\bf{p}} \\\\| < r \\ \\\\}$$\n\n- $VO^{\\tau}\\_{A|B}$：绿色弧线和两条射线组成的类扇形\n $$VO^{\\tau}\\_{A|B} = \\\\{ \\ {\\bf{v}} \\ | \\ \\exists t \\in [0, \\tau], t {\\bf{v}} \\in D({\\bf{p\\_B}} -{\\bf{p\\_A}}, r\\_A+r\\_B) \\  \\\\}$$\n\n- $CA^{\\tau}\\_{A|B}(V\\_B)$：考虑$B$取速度集合$V\\_B$\n $$CA^{\\tau}\\_{A|B}(V\\_B)=\\\\{ \\ {\\bf{v}} \\ | \\ {\\bf{v}} \\notin VO^{\\tau}\\_{A|B} \\oplus V\\_B \\ \\\\}，　其中\\oplus为闵氏和$$\n\n至此，成功在VO的基础上引入了时间窗口，避障的时候不仅考虑**速度方向**，还考虑**速度大小**。\n\np.s. 关于VO，闵氏和等内容可以参考上一篇文章{% post_link paper-rvo 论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》 %}\n\n## ORCA定义\n\nORCA全称为Optimal Reciprocal Collision Avoidance，含义就是在上述的$CA^{\\tau}\\_{A|B}(V\\_B)$中选择最优的一个。\n\n论文对最优的定义大概可以理解为**在$v^{opt}$附近的合法值越多越好**，即为满足下列条件的$V\\_A$与$V\\_B$对：\n\n- **Reciprocally collision avoiding**：很好理解，在对方的$CA$内即可，即\n$$V\\_A \\subseteq CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B \\subseteq CA^{\\tau}\\_{B|A}(V\\_A)$$\n- **Reciprocally maximal**：两个速度集是最大的不碰撞速度集，$V\\_A$或$V\\_B$都没有更多的避撞速度可选\n$$V\\_A = CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B = CA^{\\tau}\\_{B|A}(V\\_A)$$\n- **包含最多$v^{opt}$附近的避撞速度**：$v^{opt}$是一个预设值，可以理解为偏好值，论文中的最优希望：\n\t+ $v^{opt}$附近的值越多越好\n- **双方在各自$v^{opt}$附近的避撞速度数量一样**：$A$和$B$各自有$v^{opt}\\_A$和$v^{opt}\\_B$，论文中的最优希望：\n\t+ $A$求出来在$v^{opt}\\_A$附近的避撞速度的数量$n\\_A$\n\t+ $B$求出来在$v^{opt}\\_B$附近的避撞速度的数量$n\\_B$\n\t+ $n\\_A = n\\_B$\n\n那么用数学语言来描述上面几条性质就是：\n$$| ORCA^{\\tau}\\_{A|B} \\cap D({\\bf{v^{opt}\\_A}}, r) | = | ORCA^{\\tau}\\_{B|A} \\cap D({\\bf{v^{opt}\\_B}}, r) | \\ge min(|V\\_A \\cap D({\\bf{v^{opt}\\_A}}, r)|, |V\\_B \\cap D({\\bf{v^{opt}\\_B}}, r)|)$$\n\n其中\n\n- $ORCA^{\\tau}\\_{A|B}$与$ORCA^{\\tau}\\_{B|A}$：表示要求的最优速度集对，且满足reciprocally maximal，即\n$$ORCA^{\\tau}\\_{A|B} = CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A}) \\land ORCA^{\\tau}\\_{B|A} = CA^{\\tau}\\_{B|A}(ORCA^{\\tau}\\_{A|B})$$\n- $V\\_A$与$V\\_B$：表示任意reciprocally collision avoiding的速度集对，即\n$$V\\_A \\subseteq CA^{\\tau}\\_{A|B}(V\\_B) \\land V\\_B \\subseteq CA^{\\tau}\\_{B|A}(V\\_A)$$\n- $r$：表示了“附近”的大小\n- $D({\\bf{v^{opt}}},r)$：表示$v^{opt}$附近的速度\n\n## ORCA求解\n\n论文采用构造法求解出满足上述性质的ORCA，下面先说怎么做，再说为什么这样做就能满足性质。\n\n下面从$A$的角度介绍构造过程（图示如下），这里假设$A$知道$\\bf{v^{opt}\\_B}$\n\n1. 求解**相对速度**${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$\n2. 求解$VO^{\\tau}\\_{A|B}$**边界上**离相对速度${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$**最近的点**$\\bf{x}$\n3. 求解从相对速度${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$指向$\\bf{x}$的向量$\\bf{u}$\n4. 求解与${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$垂直的直线，将空间分为两个平面\n5. 直线${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$指向的平面即为所求$ORCA^{\\tau}\\_{A|B}$\n\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img orca_sol.png ORCA求解 %}\n</div>\n\n下面逐条解释为什么符合性质：\n\n- **Reciprocally collision avoiding**：\n\t+ 由构造过程可知${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}+{\\bf{u}}$不在$VO^{\\tau}\\_{A|B}$内\n\t+ $A$取新速度${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}}$\n\t+ $B$取新速度${\\bf{v^{opt}\\_B}} - \\frac{1}{2}{\\bf{u}}$\n\t+ 此时，相对速度为${\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}} - ({\\bf{v^{opt}\\_B}} - \\frac{1}{2}{\\bf{u}}) = {\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}+{\\bf{u}}$，所以不在$VO^{\\tau}\\_{A|B}$内\n- **Reciprocally maximal**：\n\t+ 要证$ORCA^{\\tau}\\_{A|B} = CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$\n\t+ 先求$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$：\n\t\t* 回忆之前引入时间窗口$\\tau$部分，构造$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$分两步\n\t\t\t1. 先将类扇形往${\\bf{v^{opt}\\_B}}-\\frac{1}{2}{\\bf{u}}$移动\n\t\t\t2. 沿直线$ORCA^{\\tau}\\_{B|A}$方向扫\n\t\t* 先将类扇形往$\\bf{v^{opt}\\_B}$方向平移（此时，类扇形中的${\\bf{v^{opt}\\_A}}-{\\bf{v^{opt}\\_B}}$到了$\\bf{v^{opt}\\_A}$，且$\\bf{x}$与$\\bf{v^{opt}\\_A}$的中点在直线$ORCA^{\\tau}\\_{A|B}$上）\n\t\t* 别忘了还要将类扇形往$-{\\frac{1}{2}\\bf{u}}$方向移动（此时，$\\bf{x}$刚好位于直线$ORCA^{\\tau}\\_{A|B}$上）\n\t\t* 最后再沿直线$ORCA^{\\tau}\\_{B|A}$方向扫，得$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$\n\t+ 而显然$CA^{\\tau}\\_{A|B}(ORCA^{\\tau}\\_{B|A})$正是$ORCA^{\\tau}\\_{A|B}$\n- **包含最多$v^{opt}$附近的避撞速度**：\n\t+ 选择了最近的$\\bf{x}$进行“突围”，那“舍弃”的速度应该是最少的\n\t+ p.s. 这一点的证明不太严谨，有更好的证明欢迎留言探讨\n- **双方在各自$v^{opt}$附近的避撞速度数量一样**：\n\t+ $\\bf{v^{opt}\\_A}$到$ORCA^{\\tau}\\_{A|B}$的距离等于$\\bf{v^{opt}\\_B}$到$ORCA^{\\tau}\\_{B|A}$的距离\n\t+ 定义“附近”用的是圆\n\n最后给出ORCA的数学定义\n\n$$ORCA^{\\tau}\\_{A|B} = \\\\{ \\ {\\bf{v}} \\ | \\ ({\\bf{v}} - ({\\bf{v^{opt}\\_A}} + \\frac{1}{2}{\\bf{u}})) \\cdot {\\bf{n}} \\ge 0 \\  \\\\}$$\n\n其中\n\n- ${\\bf{u}}$是最小偏移\n$${\\bf{u}} = (\\mathop{argmin}\\_{ {\\bf{v}} \\in \\partial VO^{\\tau}\\_{A|B}}{\\\\| {\\bf{v}} - ( {\\bf{v^{opt}\\_A}} - {\\bf{v^{opt}\\_B}} )  \\\\|}) - ({\\bf{v^{opt}\\_A}} - {\\bf{v^{opt}\\_B}})$$\n- ${\\bf{n}}$是跟${\\bf{u}}$同向的法向量\n- 用${\\bf{v}} \\cdot {\\bf{n}} \\ge 0$来表示一个平面\n\n## ORCA基本用法\n\n1. 对其他所有agents的$ORCA$求交（线性规划），再与自己可选速度求交集，得候选速度集$ORCA^{\\tau}\\_{A}$\n$$ORCA^{\\tau}\\_{A}=D({\\bf{0}}, {\\bf{v^{max}\\_A}}) \\cap \\mathop{\\bigcap}\\_{B \\ne A}{ORCA^{\\tau}\\_{A|B}}$$\n2. 在候选集中求解跟自己偏好速度最近的一个速度${\\bf{v^{new}\\_A}}$\n$${\\bf{v^{new}\\_A}} = \\mathop{argmin}\\_{ {\\bf{v}} \\in ORCA^{\\tau}\\_A}{\\\\| {\\bf{v}} - {\\bf{v^{opt}\\_A}} \\\\|}$$\n\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img orca_app.png ORCA基本用法 %}\n</div>\n\n可见ORCA的求解，就是在$ORCA^{\\tau}\\_{A}$内，优化目标函数$\\\\| {\\bf{v}} - {\\bf{v^{opt}\\_A}} \\\\|$，所以计算效率很高。\n\n## 关于${\\bf{v^{opt}}}$的选择\n\n为方便阐述，假设${\\bf{v\\_A}} = {\\bf{0}}$，这样$ORCA^{\\tau}\\_{A|B}$就只是从原点移$\\frac{1}{2}{\\bf{u}}$\n\n讨论${\\bf{v^{opt}}}$的选择跟$ORCA^{\\tau}\\_{A}$的关系\n\n- ${\\bf{0}}$\n\t+ 一定有解，如下图(c)。很好理解，这相当于假设了别人都静止，那只要自己也静止，必然有解。\n\t+ 密集情况下容易死锁，因为这样相当于只考虑位置信息，而不考虑速度信息\n- ${\\bf{v^{pref}}}$\n\t+ 密集情况下容易无解，如下图(b)。因为一般来说${\\bf{v^{pref}}}$都比较大\n\t+ 而且实际上也不可能观察得到别人的${\\bf{v^{pref}}}$\n- ${\\bf{v\\_{curr}}}$\n\t+ 两者的折中\n\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img opt.png opt选择 %}\n</div>\n\n## 无解情况的处理\n\n基本思路跟线性代数里面的投影差不多：选择离合法速度最近的速度\n\n$$ {\\bf{v^{new}\\_{A}}} = \\mathop{argmin}\\_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}\\_A}})} {\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}} $$\n\n下面慢慢拆分来看\n\n1. $d\\_{A|B}({\\bf{v}})$：表示${\\bf{v}}$的“违规”程度。定义为${\\bf{v}}$到ORCA分割线的有向距离，${\\bf{v}}$在ORCA外则取正\n2. ${\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}}$：对每个速度${\\bf{v}}$，取“违规”最多的值作为它的“违规程度”\n3. $\\mathop{argmin}\\_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}\\_A}})} {\\mathop{max}\\_{B \\ne A}{\\ d\\_{A|B}({\\bf{v}})}}$：在可行速度内，求”违规程度”最小的速度\n\n## 收获\n- 改进可以从以下角度考虑：\n\t+ **效果**：只考虑了方向，没有考虑速度\n\t+ **效率**：计算复杂度\n\n## References\n\n- [Reciprocal n-body Collision Avoidance](http://gamma.cs.unc.edu/ORCA/publications/ORCA.pdf)\n","slug":"paper-orca","published":1,"updated":"2018-01-24T03:42:48.496Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrq90010xuvt5g3q2j7z","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>ORCA是经典的分布式底层避障算法，其任务是（对于A）：</p>\n<ul>\n<li><strong>输入</strong>：A与B的形状（$r_A$、$r_B$）、位置（$p_A$、$p_B$）、速度（$v_A$、$v_B$），以及A的偏好量（$v_A^{pref}$）和限制量（$v_A^{max}$）。</li>\n<li><strong>输出</strong>：不会发生碰撞的$v_A^{\\prime}$。</li>\n<li><strong>特点</strong>：双方只要都采用ORCA，那么双方<strong>无需通信</strong>，<strong>分布式</strong>地求出来的$v_A^{\\prime}$与$v_B^{\\prime}$也不会发生碰撞。</li>\n</ul>\n<p>跟RVO一样，ORCA也是基于VO的，但更加实用，因为：</p>\n<ul>\n<li><strong>效果上</strong>：考虑了<strong>速度的大小</strong>，使得筛选粒度更细。不像VO和RVO只考虑速度方向。</li>\n<li><strong>效率上</strong>：求解过程基本只用到了<strong>线性规划</strong>，比较高效。不像VO和RVO有大量的非线性求解。</li>\n</ul>\n<h2 id=\"引入时间窗口-tau\"><a href=\"#引入时间窗口-tau\" class=\"headerlink\" title=\"引入时间窗口$\\tau$\"></a>引入时间窗口$\\tau$</h2><ol>\n<li>如下图(a)，在<strong>空间坐标</strong>上，有<ul>\n<li>位于$\\bf{p_B}$，半径为$r_B$的$B$</li>\n<li>位于$\\bf{p_A}$，半径为$r_A$的$A$</li>\n</ul>\n</li>\n<li>如下图(b)，引入时间$\\tau$，将<strong>空间坐标</strong>转换为<strong>速度坐标</strong><ol>\n<li><strong>相对化空间坐标</strong>：将图(a)化为以$A$为原点，并将$A$化为质点</li>\n<li><strong>转为速度坐标</strong>：假设$B$静止，那么当$v_A * \\tau$达到红色弧线时，$A$与$B$会发生碰撞，所以VO内，速度上限为$\\frac{p_{弧线上的点}}{\\tau}$，得到绿色弧线。</li>\n<li>至此得到$VO^{\\tau}_{A|B}$，即绿色弧线与两射线组成的类扇形</li>\n</ol>\n</li>\n<li>如下图(c)，<strong>考虑$B$取的速度集合$V_B$</strong>，对假设$B$静止得到的$VO^{\\tau}_{A|B}$求闵氏和，得$CA^{\\tau}_{A|B}(V_B)$</li>\n</ol>\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/14/paper-orca/time_interval.png\" alt=\"时间窗口\" title=\"时间窗口\"><br></div>\n\n<p>下面给出各变量规范的数学定义：</p>\n<ul>\n<li><p>$D({\\bf{p}},r)$：以$p$为圆心$r$为半径的圆<br>$$D({\\bf{p}},r)=\\{ \\ {\\bf{q}} \\ | \\ \\| {\\bf{q}} - {\\bf{p}} \\| &lt; r \\ \\}$$</p>\n</li>\n<li><p>$VO^{\\tau}_{A|B}$：绿色弧线和两条射线组成的类扇形<br>$$VO^{\\tau}_{A|B} = \\{ \\ {\\bf{v}} \\ | \\ \\exists t \\in [0, \\tau], t {\\bf{v}} \\in D({\\bf{p_B}} -{\\bf{p_A}}, r_A+r_B) \\  \\}$$</p>\n</li>\n<li><p>$CA^{\\tau}_{A|B}(V_B)$：考虑$B$取速度集合$V_B$<br>$$CA^{\\tau}_{A|B}(V_B)=\\{ \\ {\\bf{v}} \\ | \\ {\\bf{v}} \\notin VO^{\\tau}_{A|B} \\oplus V_B \\ \\}，　其中\\oplus为闵氏和$$</p>\n</li>\n</ul>\n<p>至此，成功在VO的基础上引入了时间窗口，避障的时候不仅考虑<strong>速度方向</strong>，还考虑<strong>速度大小</strong>。</p>\n<p>p.s. 关于VO，闵氏和等内容可以参考上一篇文章<a href=\"/2017/01/13/paper-rvo/\" title=\"论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》\">论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》</a></p>\n<h2 id=\"ORCA定义\"><a href=\"#ORCA定义\" class=\"headerlink\" title=\"ORCA定义\"></a>ORCA定义</h2><p>ORCA全称为Optimal Reciprocal Collision Avoidance，含义就是在上述的$CA^{\\tau}_{A|B}(V_B)$中选择最优的一个。</p>\n<p>论文对最优的定义大概可以理解为<strong>在$v^{opt}$附近的合法值越多越好</strong>，即为满足下列条件的$V_A$与$V_B$对：</p>\n<ul>\n<li><strong>Reciprocally collision avoiding</strong>：很好理解，在对方的$CA$内即可，即<br>$$V_A \\subseteq CA^{\\tau}_{A|B}(V_B) \\land V_B \\subseteq CA^{\\tau}_{B|A}(V_A)$$</li>\n<li><strong>Reciprocally maximal</strong>：两个速度集是最大的不碰撞速度集，$V_A$或$V_B$都没有更多的避撞速度可选<br>$$V_A = CA^{\\tau}_{A|B}(V_B) \\land V_B = CA^{\\tau}_{B|A}(V_A)$$</li>\n<li><strong>包含最多$v^{opt}$附近的避撞速度</strong>：$v^{opt}$是一个预设值，可以理解为偏好值，论文中的最优希望：<ul>\n<li>$v^{opt}$附近的值越多越好</li>\n</ul>\n</li>\n<li><strong>双方在各自$v^{opt}$附近的避撞速度数量一样</strong>：$A$和$B$各自有$v^{opt}_A$和$v^{opt}_B$，论文中的最优希望：<ul>\n<li>$A$求出来在$v^{opt}_A$附近的避撞速度的数量$n_A$</li>\n<li>$B$求出来在$v^{opt}_B$附近的避撞速度的数量$n_B$</li>\n<li>$n_A = n_B$</li>\n</ul>\n</li>\n</ul>\n<p>那么用数学语言来描述上面几条性质就是：<br>$$| ORCA^{\\tau}_{A|B} \\cap D({\\bf{v^{opt}_A}}, r) | = | ORCA^{\\tau}_{B|A} \\cap D({\\bf{v^{opt}_B}}, r) | \\ge min(|V_A \\cap D({\\bf{v^{opt}_A}}, r)|, |V_B \\cap D({\\bf{v^{opt}_B}}, r)|)$$</p>\n<p>其中</p>\n<ul>\n<li>$ORCA^{\\tau}_{A|B}$与$ORCA^{\\tau}_{B|A}$：表示要求的最优速度集对，且满足reciprocally maximal，即<br>$$ORCA^{\\tau}_{A|B} = CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A}) \\land ORCA^{\\tau}_{B|A} = CA^{\\tau}_{B|A}(ORCA^{\\tau}_{A|B})$$</li>\n<li>$V_A$与$V_B$：表示任意reciprocally collision avoiding的速度集对，即<br>$$V_A \\subseteq CA^{\\tau}_{A|B}(V_B) \\land V_B \\subseteq CA^{\\tau}_{B|A}(V_A)$$</li>\n<li>$r$：表示了“附近”的大小</li>\n<li>$D({\\bf{v^{opt}}},r)$：表示$v^{opt}$附近的速度</li>\n</ul>\n<h2 id=\"ORCA求解\"><a href=\"#ORCA求解\" class=\"headerlink\" title=\"ORCA求解\"></a>ORCA求解</h2><p>论文采用构造法求解出满足上述性质的ORCA，下面先说怎么做，再说为什么这样做就能满足性质。</p>\n<p>下面从$A$的角度介绍构造过程（图示如下），这里假设$A$知道$\\bf{v^{opt}_B}$</p>\n<ol>\n<li>求解<strong>相对速度</strong>${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$</li>\n<li>求解$VO^{\\tau}_{A|B}$<strong>边界上</strong>离相对速度${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$<strong>最近的点</strong>$\\bf{x}$</li>\n<li>求解从相对速度${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$指向$\\bf{x}$的向量$\\bf{u}$</li>\n<li>求解与${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$垂直的直线，将空间分为两个平面</li>\n<li>直线${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$指向的平面即为所求$ORCA^{\\tau}_{A|B}$</li>\n</ol>\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/14/paper-orca/orca_sol.png\" alt=\"ORCA求解\" title=\"ORCA求解\"><br></div>\n\n<p>下面逐条解释为什么符合性质：</p>\n<ul>\n<li><strong>Reciprocally collision avoiding</strong>：<ul>\n<li>由构造过程可知${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}+{\\bf{u}}$不在$VO^{\\tau}_{A|B}$内</li>\n<li>$A$取新速度${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$</li>\n<li>$B$取新速度${\\bf{v^{opt}_B}} - \\frac{1}{2}{\\bf{u}}$</li>\n<li>此时，相对速度为${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}} - ({\\bf{v^{opt}_B}} - \\frac{1}{2}{\\bf{u}}) = {\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}+{\\bf{u}}$，所以不在$VO^{\\tau}_{A|B}$内</li>\n</ul>\n</li>\n<li><strong>Reciprocally maximal</strong>：<ul>\n<li>要证$ORCA^{\\tau}_{A|B} = CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$</li>\n<li>先求$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$：<ul>\n<li>回忆之前引入时间窗口$\\tau$部分，构造$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$分两步<ol>\n<li>先将类扇形往${\\bf{v^{opt}_B}}-\\frac{1}{2}{\\bf{u}}$移动</li>\n<li>沿直线$ORCA^{\\tau}_{B|A}$方向扫</li>\n</ol>\n</li>\n<li>先将类扇形往$\\bf{v^{opt}_B}$方向平移（此时，类扇形中的${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$到了$\\bf{v^{opt}_A}$，且$\\bf{x}$与$\\bf{v^{opt}_A}$的中点在直线$ORCA^{\\tau}_{A|B}$上）</li>\n<li>别忘了还要将类扇形往$-{\\frac{1}{2}\\bf{u}}$方向移动（此时，$\\bf{x}$刚好位于直线$ORCA^{\\tau}_{A|B}$上）</li>\n<li>最后再沿直线$ORCA^{\\tau}_{B|A}$方向扫，得$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$</li>\n</ul>\n</li>\n<li>而显然$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$正是$ORCA^{\\tau}_{A|B}$</li>\n</ul>\n</li>\n<li><strong>包含最多$v^{opt}$附近的避撞速度</strong>：<ul>\n<li>选择了最近的$\\bf{x}$进行“突围”，那“舍弃”的速度应该是最少的</li>\n<li>p.s. 这一点的证明不太严谨，有更好的证明欢迎留言探讨</li>\n</ul>\n</li>\n<li><strong>双方在各自$v^{opt}$附近的避撞速度数量一样</strong>：<ul>\n<li>$\\bf{v^{opt}_A}$到$ORCA^{\\tau}_{A|B}$的距离等于$\\bf{v^{opt}_B}$到$ORCA^{\\tau}_{B|A}$的距离</li>\n<li>定义“附近”用的是圆</li>\n</ul>\n</li>\n</ul>\n<p>最后给出ORCA的数学定义</p>\n<p>$$ORCA^{\\tau}_{A|B} = \\{ \\ {\\bf{v}} \\ | \\ ({\\bf{v}} - ({\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}})) \\cdot {\\bf{n}} \\ge 0 \\  \\}$$</p>\n<p>其中</p>\n<ul>\n<li>${\\bf{u}}$是最小偏移<br>$${\\bf{u}} = (\\mathop{argmin}_{ {\\bf{v}} \\in \\partial VO^{\\tau}_{A|B}}{\\| {\\bf{v}} - ( {\\bf{v^{opt}_A}} - {\\bf{v^{opt}_B}} )  \\|}) - ({\\bf{v^{opt}_A}} - {\\bf{v^{opt}_B}})$$</li>\n<li>${\\bf{n}}$是跟${\\bf{u}}$同向的法向量</li>\n<li>用${\\bf{v}} \\cdot {\\bf{n}} \\ge 0$来表示一个平面</li>\n</ul>\n<h2 id=\"ORCA基本用法\"><a href=\"#ORCA基本用法\" class=\"headerlink\" title=\"ORCA基本用法\"></a>ORCA基本用法</h2><ol>\n<li>对其他所有agents的$ORCA$求交（线性规划），再与自己可选速度求交集，得候选速度集$ORCA^{\\tau}_{A}$<br>$$ORCA^{\\tau}_{A}=D({\\bf{0}}, {\\bf{v^{max}_A}}) \\cap \\mathop{\\bigcap}_{B \\ne A}{ORCA^{\\tau}_{A|B}}$$</li>\n<li>在候选集中求解跟自己偏好速度最近的一个速度${\\bf{v^{new}_A}}$<br>$${\\bf{v^{new}_A}} = \\mathop{argmin}_{ {\\bf{v}} \\in ORCA^{\\tau}_A}{\\| {\\bf{v}} - {\\bf{v^{opt}_A}} \\|}$$</li>\n</ol>\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/14/paper-orca/orca_app.png\" alt=\"ORCA基本用法\" title=\"ORCA基本用法\"><br></div>\n\n<p>可见ORCA的求解，就是在$ORCA^{\\tau}_{A}$内，优化目标函数$\\| {\\bf{v}} - {\\bf{v^{opt}_A}} \\|$，所以计算效率很高。</p>\n<h2 id=\"关于-bf-v-opt-的选择\"><a href=\"#关于-bf-v-opt-的选择\" class=\"headerlink\" title=\"关于${\\bf{v^{opt}}}$的选择\"></a>关于${\\bf{v^{opt}}}$的选择</h2><p>为方便阐述，假设${\\bf{v_A}} = {\\bf{0}}$，这样$ORCA^{\\tau}_{A|B}$就只是从原点移$\\frac{1}{2}{\\bf{u}}$</p>\n<p>讨论${\\bf{v^{opt}}}$的选择跟$ORCA^{\\tau}_{A}$的关系</p>\n<ul>\n<li>${\\bf{0}}$<ul>\n<li>一定有解，如下图(c)。很好理解，这相当于假设了别人都静止，那只要自己也静止，必然有解。</li>\n<li>密集情况下容易死锁，因为这样相当于只考虑位置信息，而不考虑速度信息</li>\n</ul>\n</li>\n<li>${\\bf{v^{pref}}}$<ul>\n<li>密集情况下容易无解，如下图(b)。因为一般来说${\\bf{v^{pref}}}$都比较大</li>\n<li>而且实际上也不可能观察得到别人的${\\bf{v^{pref}}}$</li>\n</ul>\n</li>\n<li>${\\bf{v_{curr}}}$<ul>\n<li>两者的折中</li>\n</ul>\n</li>\n</ul>\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/14/paper-orca/opt.png\" alt=\"opt选择\" title=\"opt选择\"><br></div>\n\n<h2 id=\"无解情况的处理\"><a href=\"#无解情况的处理\" class=\"headerlink\" title=\"无解情况的处理\"></a>无解情况的处理</h2><p>基本思路跟线性代数里面的投影差不多：选择离合法速度最近的速度</p>\n<p>$$ {\\bf{v^{new}_{A}}} = \\mathop{argmin}_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}_A}})} {\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}} $$</p>\n<p>下面慢慢拆分来看</p>\n<ol>\n<li>$d_{A|B}({\\bf{v}})$：表示${\\bf{v}}$的“违规”程度。定义为${\\bf{v}}$到ORCA分割线的有向距离，${\\bf{v}}$在ORCA外则取正</li>\n<li>${\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}}$：对每个速度${\\bf{v}}$，取“违规”最多的值作为它的“违规程度”</li>\n<li>$\\mathop{argmin}_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}_A}})} {\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}}$：在可行速度内，求”违规程度”最小的速度</li>\n</ol>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>改进可以从以下角度考虑：<ul>\n<li><strong>效果</strong>：只考虑了方向，没有考虑速度</li>\n<li><strong>效率</strong>：计算复杂度</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://gamma.cs.unc.edu/ORCA/publications/ORCA.pdf\" target=\"_blank\" rel=\"external\">Reciprocal n-body Collision Avoidance</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>ORCA是经典的分布式底层避障算法，其任务是（对于A）：</p>\n<ul>\n<li><strong>输入</strong>：A与B的形状（$r_A$、$r_B$）、位置（$p_A$、$p_B$）、速度（$v_A$、$v_B$），以及A的偏好量（$v_A^{pref}$）和限制量（$v_A^{max}$）。</li>\n<li><strong>输出</strong>：不会发生碰撞的$v_A^{\\prime}$。</li>\n<li><strong>特点</strong>：双方只要都采用ORCA，那么双方<strong>无需通信</strong>，<strong>分布式</strong>地求出来的$v_A^{\\prime}$与$v_B^{\\prime}$也不会发生碰撞。</li>\n</ul>\n<p>跟RVO一样，ORCA也是基于VO的，但更加实用，因为：</p>\n<ul>\n<li><strong>效果上</strong>：考虑了<strong>速度的大小</strong>，使得筛选粒度更细。不像VO和RVO只考虑速度方向。</li>\n<li><strong>效率上</strong>：求解过程基本只用到了<strong>线性规划</strong>，比较高效。不像VO和RVO有大量的非线性求解。</li>\n</ul>\n<h2 id=\"引入时间窗口-tau\"><a href=\"#引入时间窗口-tau\" class=\"headerlink\" title=\"引入时间窗口$\\tau$\"></a>引入时间窗口$\\tau$</h2><ol>\n<li>如下图(a)，在<strong>空间坐标</strong>上，有<ul>\n<li>位于$\\bf{p_B}$，半径为$r_B$的$B$</li>\n<li>位于$\\bf{p_A}$，半径为$r_A$的$A$</li>\n</ul>\n</li>\n<li>如下图(b)，引入时间$\\tau$，将<strong>空间坐标</strong>转换为<strong>速度坐标</strong><ol>\n<li><strong>相对化空间坐标</strong>：将图(a)化为以$A$为原点，并将$A$化为质点</li>\n<li><strong>转为速度坐标</strong>：假设$B$静止，那么当$v_A * \\tau$达到红色弧线时，$A$与$B$会发生碰撞，所以VO内，速度上限为$\\frac{p_{弧线上的点}}{\\tau}$，得到绿色弧线。</li>\n<li>至此得到$VO^{\\tau}_{A|B}$，即绿色弧线与两射线组成的类扇形</li>\n</ol>\n</li>\n<li>如下图(c)，<strong>考虑$B$取的速度集合$V_B$</strong>，对假设$B$静止得到的$VO^{\\tau}_{A|B}$求闵氏和，得$CA^{\\tau}_{A|B}(V_B)$</li>\n</ol>\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/14/paper-orca/time_interval.png\" alt=\"时间窗口\" title=\"时间窗口\"><br></div>\n\n<p>下面给出各变量规范的数学定义：</p>\n<ul>\n<li><p>$D({\\bf{p}},r)$：以$p$为圆心$r$为半径的圆<br>$$D({\\bf{p}},r)=\\{ \\ {\\bf{q}} \\ | \\ \\| {\\bf{q}} - {\\bf{p}} \\| &lt; r \\ \\}$$</p>\n</li>\n<li><p>$VO^{\\tau}_{A|B}$：绿色弧线和两条射线组成的类扇形<br>$$VO^{\\tau}_{A|B} = \\{ \\ {\\bf{v}} \\ | \\ \\exists t \\in [0, \\tau], t {\\bf{v}} \\in D({\\bf{p_B}} -{\\bf{p_A}}, r_A+r_B) \\  \\}$$</p>\n</li>\n<li><p>$CA^{\\tau}_{A|B}(V_B)$：考虑$B$取速度集合$V_B$<br>$$CA^{\\tau}_{A|B}(V_B)=\\{ \\ {\\bf{v}} \\ | \\ {\\bf{v}} \\notin VO^{\\tau}_{A|B} \\oplus V_B \\ \\}，　其中\\oplus为闵氏和$$</p>\n</li>\n</ul>\n<p>至此，成功在VO的基础上引入了时间窗口，避障的时候不仅考虑<strong>速度方向</strong>，还考虑<strong>速度大小</strong>。</p>\n<p>p.s. 关于VO，闵氏和等内容可以参考上一篇文章<a href=\"/2017/01/13/paper-rvo/\" title=\"论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》\">论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》</a></p>\n<h2 id=\"ORCA定义\"><a href=\"#ORCA定义\" class=\"headerlink\" title=\"ORCA定义\"></a>ORCA定义</h2><p>ORCA全称为Optimal Reciprocal Collision Avoidance，含义就是在上述的$CA^{\\tau}_{A|B}(V_B)$中选择最优的一个。</p>\n<p>论文对最优的定义大概可以理解为<strong>在$v^{opt}$附近的合法值越多越好</strong>，即为满足下列条件的$V_A$与$V_B$对：</p>\n<ul>\n<li><strong>Reciprocally collision avoiding</strong>：很好理解，在对方的$CA$内即可，即<br>$$V_A \\subseteq CA^{\\tau}_{A|B}(V_B) \\land V_B \\subseteq CA^{\\tau}_{B|A}(V_A)$$</li>\n<li><strong>Reciprocally maximal</strong>：两个速度集是最大的不碰撞速度集，$V_A$或$V_B$都没有更多的避撞速度可选<br>$$V_A = CA^{\\tau}_{A|B}(V_B) \\land V_B = CA^{\\tau}_{B|A}(V_A)$$</li>\n<li><strong>包含最多$v^{opt}$附近的避撞速度</strong>：$v^{opt}$是一个预设值，可以理解为偏好值，论文中的最优希望：<ul>\n<li>$v^{opt}$附近的值越多越好</li>\n</ul>\n</li>\n<li><strong>双方在各自$v^{opt}$附近的避撞速度数量一样</strong>：$A$和$B$各自有$v^{opt}_A$和$v^{opt}_B$，论文中的最优希望：<ul>\n<li>$A$求出来在$v^{opt}_A$附近的避撞速度的数量$n_A$</li>\n<li>$B$求出来在$v^{opt}_B$附近的避撞速度的数量$n_B$</li>\n<li>$n_A = n_B$</li>\n</ul>\n</li>\n</ul>\n<p>那么用数学语言来描述上面几条性质就是：<br>$$| ORCA^{\\tau}_{A|B} \\cap D({\\bf{v^{opt}_A}}, r) | = | ORCA^{\\tau}_{B|A} \\cap D({\\bf{v^{opt}_B}}, r) | \\ge min(|V_A \\cap D({\\bf{v^{opt}_A}}, r)|, |V_B \\cap D({\\bf{v^{opt}_B}}, r)|)$$</p>\n<p>其中</p>\n<ul>\n<li>$ORCA^{\\tau}_{A|B}$与$ORCA^{\\tau}_{B|A}$：表示要求的最优速度集对，且满足reciprocally maximal，即<br>$$ORCA^{\\tau}_{A|B} = CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A}) \\land ORCA^{\\tau}_{B|A} = CA^{\\tau}_{B|A}(ORCA^{\\tau}_{A|B})$$</li>\n<li>$V_A$与$V_B$：表示任意reciprocally collision avoiding的速度集对，即<br>$$V_A \\subseteq CA^{\\tau}_{A|B}(V_B) \\land V_B \\subseteq CA^{\\tau}_{B|A}(V_A)$$</li>\n<li>$r$：表示了“附近”的大小</li>\n<li>$D({\\bf{v^{opt}}},r)$：表示$v^{opt}$附近的速度</li>\n</ul>\n<h2 id=\"ORCA求解\"><a href=\"#ORCA求解\" class=\"headerlink\" title=\"ORCA求解\"></a>ORCA求解</h2><p>论文采用构造法求解出满足上述性质的ORCA，下面先说怎么做，再说为什么这样做就能满足性质。</p>\n<p>下面从$A$的角度介绍构造过程（图示如下），这里假设$A$知道$\\bf{v^{opt}_B}$</p>\n<ol>\n<li>求解<strong>相对速度</strong>${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$</li>\n<li>求解$VO^{\\tau}_{A|B}$<strong>边界上</strong>离相对速度${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$<strong>最近的点</strong>$\\bf{x}$</li>\n<li>求解从相对速度${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$指向$\\bf{x}$的向量$\\bf{u}$</li>\n<li>求解与${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$垂直的直线，将空间分为两个平面</li>\n<li>直线${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$指向的平面即为所求$ORCA^{\\tau}_{A|B}$</li>\n</ol>\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/14/paper-orca/orca_sol.png\" alt=\"ORCA求解\" title=\"ORCA求解\"><br></div>\n\n<p>下面逐条解释为什么符合性质：</p>\n<ul>\n<li><strong>Reciprocally collision avoiding</strong>：<ul>\n<li>由构造过程可知${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}+{\\bf{u}}$不在$VO^{\\tau}_{A|B}$内</li>\n<li>$A$取新速度${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}}$</li>\n<li>$B$取新速度${\\bf{v^{opt}_B}} - \\frac{1}{2}{\\bf{u}}$</li>\n<li>此时，相对速度为${\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}} - ({\\bf{v^{opt}_B}} - \\frac{1}{2}{\\bf{u}}) = {\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}+{\\bf{u}}$，所以不在$VO^{\\tau}_{A|B}$内</li>\n</ul>\n</li>\n<li><strong>Reciprocally maximal</strong>：<ul>\n<li>要证$ORCA^{\\tau}_{A|B} = CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$</li>\n<li>先求$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$：<ul>\n<li>回忆之前引入时间窗口$\\tau$部分，构造$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$分两步<ol>\n<li>先将类扇形往${\\bf{v^{opt}_B}}-\\frac{1}{2}{\\bf{u}}$移动</li>\n<li>沿直线$ORCA^{\\tau}_{B|A}$方向扫</li>\n</ol>\n</li>\n<li>先将类扇形往$\\bf{v^{opt}_B}$方向平移（此时，类扇形中的${\\bf{v^{opt}_A}}-{\\bf{v^{opt}_B}}$到了$\\bf{v^{opt}_A}$，且$\\bf{x}$与$\\bf{v^{opt}_A}$的中点在直线$ORCA^{\\tau}_{A|B}$上）</li>\n<li>别忘了还要将类扇形往$-{\\frac{1}{2}\\bf{u}}$方向移动（此时，$\\bf{x}$刚好位于直线$ORCA^{\\tau}_{A|B}$上）</li>\n<li>最后再沿直线$ORCA^{\\tau}_{B|A}$方向扫，得$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$</li>\n</ul>\n</li>\n<li>而显然$CA^{\\tau}_{A|B}(ORCA^{\\tau}_{B|A})$正是$ORCA^{\\tau}_{A|B}$</li>\n</ul>\n</li>\n<li><strong>包含最多$v^{opt}$附近的避撞速度</strong>：<ul>\n<li>选择了最近的$\\bf{x}$进行“突围”，那“舍弃”的速度应该是最少的</li>\n<li>p.s. 这一点的证明不太严谨，有更好的证明欢迎留言探讨</li>\n</ul>\n</li>\n<li><strong>双方在各自$v^{opt}$附近的避撞速度数量一样</strong>：<ul>\n<li>$\\bf{v^{opt}_A}$到$ORCA^{\\tau}_{A|B}$的距离等于$\\bf{v^{opt}_B}$到$ORCA^{\\tau}_{B|A}$的距离</li>\n<li>定义“附近”用的是圆</li>\n</ul>\n</li>\n</ul>\n<p>最后给出ORCA的数学定义</p>\n<p>$$ORCA^{\\tau}_{A|B} = \\{ \\ {\\bf{v}} \\ | \\ ({\\bf{v}} - ({\\bf{v^{opt}_A}} + \\frac{1}{2}{\\bf{u}})) \\cdot {\\bf{n}} \\ge 0 \\  \\}$$</p>\n<p>其中</p>\n<ul>\n<li>${\\bf{u}}$是最小偏移<br>$${\\bf{u}} = (\\mathop{argmin}_{ {\\bf{v}} \\in \\partial VO^{\\tau}_{A|B}}{\\| {\\bf{v}} - ( {\\bf{v^{opt}_A}} - {\\bf{v^{opt}_B}} )  \\|}) - ({\\bf{v^{opt}_A}} - {\\bf{v^{opt}_B}})$$</li>\n<li>${\\bf{n}}$是跟${\\bf{u}}$同向的法向量</li>\n<li>用${\\bf{v}} \\cdot {\\bf{n}} \\ge 0$来表示一个平面</li>\n</ul>\n<h2 id=\"ORCA基本用法\"><a href=\"#ORCA基本用法\" class=\"headerlink\" title=\"ORCA基本用法\"></a>ORCA基本用法</h2><ol>\n<li>对其他所有agents的$ORCA$求交（线性规划），再与自己可选速度求交集，得候选速度集$ORCA^{\\tau}_{A}$<br>$$ORCA^{\\tau}_{A}=D({\\bf{0}}, {\\bf{v^{max}_A}}) \\cap \\mathop{\\bigcap}_{B \\ne A}{ORCA^{\\tau}_{A|B}}$$</li>\n<li>在候选集中求解跟自己偏好速度最近的一个速度${\\bf{v^{new}_A}}$<br>$${\\bf{v^{new}_A}} = \\mathop{argmin}_{ {\\bf{v}} \\in ORCA^{\\tau}_A}{\\| {\\bf{v}} - {\\bf{v^{opt}_A}} \\|}$$</li>\n</ol>\n<div style=\"width:600px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/14/paper-orca/orca_app.png\" alt=\"ORCA基本用法\" title=\"ORCA基本用法\"><br></div>\n\n<p>可见ORCA的求解，就是在$ORCA^{\\tau}_{A}$内，优化目标函数$\\| {\\bf{v}} - {\\bf{v^{opt}_A}} \\|$，所以计算效率很高。</p>\n<h2 id=\"关于-bf-v-opt-的选择\"><a href=\"#关于-bf-v-opt-的选择\" class=\"headerlink\" title=\"关于${\\bf{v^{opt}}}$的选择\"></a>关于${\\bf{v^{opt}}}$的选择</h2><p>为方便阐述，假设${\\bf{v_A}} = {\\bf{0}}$，这样$ORCA^{\\tau}_{A|B}$就只是从原点移$\\frac{1}{2}{\\bf{u}}$</p>\n<p>讨论${\\bf{v^{opt}}}$的选择跟$ORCA^{\\tau}_{A}$的关系</p>\n<ul>\n<li>${\\bf{0}}$<ul>\n<li>一定有解，如下图(c)。很好理解，这相当于假设了别人都静止，那只要自己也静止，必然有解。</li>\n<li>密集情况下容易死锁，因为这样相当于只考虑位置信息，而不考虑速度信息</li>\n</ul>\n</li>\n<li>${\\bf{v^{pref}}}$<ul>\n<li>密集情况下容易无解，如下图(b)。因为一般来说${\\bf{v^{pref}}}$都比较大</li>\n<li>而且实际上也不可能观察得到别人的${\\bf{v^{pref}}}$</li>\n</ul>\n</li>\n<li>${\\bf{v_{curr}}}$<ul>\n<li>两者的折中</li>\n</ul>\n</li>\n</ul>\n<div style=\"width:800px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/14/paper-orca/opt.png\" alt=\"opt选择\" title=\"opt选择\"><br></div>\n\n<h2 id=\"无解情况的处理\"><a href=\"#无解情况的处理\" class=\"headerlink\" title=\"无解情况的处理\"></a>无解情况的处理</h2><p>基本思路跟线性代数里面的投影差不多：选择离合法速度最近的速度</p>\n<p>$$ {\\bf{v^{new}_{A}}} = \\mathop{argmin}_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}_A}})} {\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}} $$</p>\n<p>下面慢慢拆分来看</p>\n<ol>\n<li>$d_{A|B}({\\bf{v}})$：表示${\\bf{v}}$的“违规”程度。定义为${\\bf{v}}$到ORCA分割线的有向距离，${\\bf{v}}$在ORCA外则取正</li>\n<li>${\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}}$：对每个速度${\\bf{v}}$，取“违规”最多的值作为它的“违规程度”</li>\n<li>$\\mathop{argmin}_{ {\\bf{v}} \\in D({\\bf{0}}, {\\bf{v^{max}_A}})} {\\mathop{max}_{B \\ne A}{\\ d_{A|B}({\\bf{v}})}}$：在可行速度内，求”违规程度”最小的速度</li>\n</ol>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>改进可以从以下角度考虑：<ul>\n<li><strong>效果</strong>：只考虑了方向，没有考虑速度</li>\n<li><strong>效率</strong>：计算复杂度</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://gamma.cs.unc.edu/ORCA/publications/ORCA.pdf\">Reciprocal n-body Collision Avoidance</a></li>\n</ul>\n"},{"title":"论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》","description":["经典的底层避障算法"],"date":"2017-01-13T13:08:51.000Z","_content":"\n\n\n## 简介\n\n在介绍VO，RVO之前，需要先介绍路径规划。\n\n对Agent进行路径规划，实际上要完成的**任务**就是让Agent从点A**无碰撞地**移动到点B。而路径规划的过程是层次化的，其基本框架大致如下：\n\n- **High level**: dijkstra等算法。\n- **Low level**: VO, RVO, ORCA等底层避障算法。\n\n很容易可以跟我们的日常生活进行类比，比如说我们要从学校的教学楼走到宿舍楼，那么以上框架对应的就是：\n\n- **High level**: 通过dijkstra算法，得到路径为: 教学楼→饭堂→体育馆→图书馆→宿舍楼。\n- **Low level**: 通过底层避障算法如VO，RVO，ORCA等底层避障算法，保证我们走的每一段路（e.g. 教学楼→饭堂），都不会跟别的同学发生碰撞。\n\nVO和RVO就是经典的底层避障算法。其中VO是最经典的，RVO则在VO的基础上进行了一些改进，解决了VO抖动的问题。\n\n## VO(Velocity Obstacle)\n\n**一句话总结**VO的思路：只要在未来有可能会发生碰撞的速度，都排除在外。\n\n为方便描述，以下都假设是在**平面**内，**圆形物体**之间的避障。\n\n## VO的直观理解\n\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img prob_disc.png 问题描述 %}\n</div>\n\n> Q: 假设B静止，那么A取什么速度能够保证一定不会跟B发生碰撞呢？\n\n> A: 一种很粗暴的方法，就是**把A化作质点**，选择跟**$\\bar{B}$(扩展后的B)不相交**的速度方向。以后只要在每个周期里面，都选择不在VO的速度，就能够保证不会碰撞。\n\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img prob_sol.png 解决办法 %}\n</div>\n\n以上就是VO的直观理解，需要注意的是：\n\n- VO是指速度方向与$\\bar{B}$相交的部分，即**会发生碰撞的部分（图中灰色斜线部分）**。\n- VO是抱着**宁杀错，不放过**的思想，把所有未来有可能会发生碰撞的速度都放弃了。\n- 实际上假如仅要求**一定时间内**不发生碰撞的话，有更多的速度可供选择，比如说上图中的($v\\_{A}^{\\prime}$)。\n\n## VO的图示理解\n\n有了直观理解之后就可以用更加严谨一点的数学语言图示VO了。\n\n首先将直观理解中口语化的表达转换成对应的数学语言表示。\n\n- **物体A(B)**：以$\\bf{p\\_{A}}$为圆心，$r\\_{A}$为半径的**点集$A$**\n- **假设B静止**：A相对于B的速度，即**相对速度$\\bf{v\\_{A}}-v\\_{B}$**\n- **把A化作质点**：求集合$B$与集合$-A$的Minkowski sum，即**闵氏和，$B\\oplus-A$**，其中\n\t+ $A \\oplus B = \\\\{    {\\bf{a}} + {\\bf{b}}\\ |\\ {\\bf{a}} \\in A, {\\bf{b}} \\in B \\\\} $\n\t+ $-A = \\\\{ -{\\bf{a}}\\ |\\ {\\bf{a}} \\in A \\\\}$\n\t+ [更多关于Minkowski sum](http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences)\n\n于是就有了下图的左半部分（浅色三角形）：\n\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img vo.png VO %}\n</div>\n\n而为了直接求$\\bf{v\\_{A}}$绝对速度的VO而不是$\\bf{v\\_{A}}-v\\_{B}$相对速度的VO，将相对速度下的VO延$\\bf{v\\_{B}}$方向平移，就有了图中右半部分（深色三角形）。\n\n## VO的数学定义\n\n理解了图示，数学定义就很好理解了。\n\n- 首先给出**射线的定义**，用$\\lambda({\\bf{p}},{\\bf{v}})$表示以点$\\bf{p}$为顶点，方向为$\\bf{v}$的射线。\n$$\\lambda({\\bf{p}}, {\\bf{v}}) = \\\\{ {\\bf{p}} +t{\\bf{v}} | t\\ge 0\\\\}$$\n- 接下来就是**VO的定义**了，用$VO^{A}\\_{B}({\\bf{v\\_{B}}})$表示速度为$\\bf{v\\_{B}}$的$B$对$A$的VO \n$$VO^{A}\\_{B}({\\bf{v\\_{B}}}) = \\\\{ {\\bf{v\\_A}} | \\lambda({\\bf{p\\_{A}}}, {\\bf{v\\_{A} - v\\_{B}}}) \\cap B \\oplus -A \\ne \\emptyset \\\\}$$\n\n## RVO(Reciprocal Velocity Obstacle)\n\nVO给出了很漂亮的避障条件，所以后面很多底层的避障算法都是基于VO的，而RVO就是其中之一。\n\nRVO主要解决了VO的抖动问题\n\n- **抖动现象**：如下左图所示，即$A$会在$\\bf{v\\_{A}}$与$\\bf{v\\_{A}^{\\prime}}$之间来回切换\n- **RVO的效果**：如下右图所示，保持$\\bf{v\\_{A}}$，不会抖动\n\n<div style=\"width:700px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img oscillation.png 抖动现象 %}\n</div>\n\n## 证明VO抖动现象存在\n\n首先论文给出了VO的三条性质\n\n- **Symmetry**：$\\bf{v\\_A}$的$A$会撞上$\\bf{v\\_B}$的$B$，则$\\bf{v\\_B}$的$B$也会撞上$\\bf{v\\_A}$的$A$\n$${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Leftrightarrow {\\bf{v\\_B}} \\in VO^{B}\\_{A}({\\bf{v\\_{A}}})$$\n- **Translation Invariance**：$\\bf{v\\_A}$的$A$会撞上$\\bf{v\\_B}$的$B$，则$\\bf{v\\_A+u}$的$A$会撞上$\\bf{v\\_B+u}$的$B$\n$${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Leftrightarrow {\\bf{v\\_A+u}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}+u}})$$\n- **Convexity**：在$VO^{A}\\_{B}({\\bf{v\\_{B}}})$的左（右）侧的两个速度之间的任意速度，也在$VO^{A}\\_{B}({\\bf{v\\_{B}}})$的左（右）侧。VO左（右）侧如下图所示：\n$${\\bf{v\\_A}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\land  {\\bf{v\\_A^{\\prime}}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Rightarrow (1-\\alpha){\\bf{v\\_A}} + \\alpha{\\bf{v\\_{A}^{\\prime}}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ for\\ 0\\le \\alpha \\le 1$$\n\n<div style=\"width:300px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img left_right.png VO左（右）侧示意 %}\n</div>\n\n接下来是抖动现象存在的证明\n\n1. 假设初始状态为会发生碰撞：${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ {\\bf{v\\_B}} \\in VO^{B}\\_{A}({\\bf{v\\_{A}}})$\n2. 由于在对方的VO内，所以各自选择新的速度以防止碰撞：${\\bf{v\\_A^{\\prime}}} \\notin VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ {\\bf{v\\_B^{\\prime}}} \\notin VO^{B}\\_{A}({\\bf{v\\_{A}}})$\n3. 由前面VO的Symmetry性质可知：此时，**原来的速度不在当前速度的VO内**：${\\bf{v\\_B}} \\notin VO^{B}\\_{A}({\\bf{v\\_{A}^{\\prime}}}),\\ {\\bf{v\\_A}} \\notin VO^{A}\\_{B}({\\bf{v\\_{B}^{\\prime}}})$\n4. 假设我们**更加prefer原来的速度**，则又会回到原来的$\\bf{v\\_A}$与$\\bf{v\\_B}$\n5. 于是在**1→4之间循环**，即发生抖动\n\n## RVO的Insight\n\n首先回想一下为什么会发生抖动：\n\n> 双方为了避障，都偏移了当前速度太多，导致更新速度后，原来速度不再会发生碰撞。\n\n那么我们有没有办法减少对当前速度的偏移，同时又能保证避障呢，RVO的回答是肯定的：\n\n- 缩小VO的大小，新的\"VO\"就叫做RVO\n\t+ p.s. 我个人对Reciprocal的理解是：相对于VO**完全把对方当做木头**，RVO假设对方在避障中也**会承担一定责任**，所以**不用完全靠自己**改变速度来走出VO，有种**互相合作**避障的感觉。\n- 或者换一个角度理解，不再直接选择VO外的速度$\\bf{v\\_A^{\\prime}}$作为新的速度，而是average当前速度$\\bf{v\\_A}$与VO外的速度$\\bf{v\\_A^{\\prime}}$\n\n## RVO的定义与图示\n\n- 速度为$\\bf{v\\_{B}}$的$B$对速度为$\\bf{v\\_A}$的$A$产生的RVO为：\n\n$$ RVO\\_{B}^{A}({\\bf{v\\_B}}, {\\bf{v\\_A}}) = \\\\{ {\\bf{v\\_{A}^{\\prime}}}\\ |\\ 2{\\bf{v\\_{A}^{\\prime}}} - {\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}})\\\\}$$\n\n- 图示理解如下：\n\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img rvo.png RVO %}\n</div>\n\n- 释意：\n\t+ $2{\\bf{v\\_{A}^{\\prime}}} - {\\bf{v\\_A}}$：${\\bf{v\\_A}}$相对于$\\bf{v\\_A^{\\prime}}$的对称点。\n\t+ 所以**公式的含义**是：对称点在原VO中，则中点在RVO中。\n\t+ 所以**RVO的构成**是：$\\bf{v_A}$与原VO中的点的中点。\n\n## RVO不会发生碰撞且没有抖动现象的证明\n\n这一部分不赘述了，论文中写得很详尽，只说一下证明的思路：\n\n- 双方选择**同侧**避障时，不会发生碰撞。\n- 双方一定会选择同侧避障。\n- 不会有抖动现象：原来会撞的在选择新速度后**依然**会撞。\n\n## 收获\n\n- **用数学语言来描述问题**：化作质点的描述、抖动的描述。\n- **从实际应用中发现问题**：抖动问题的发现。\n- **特殊到一般的推广**：论文后面还将RVO推广到一般情况，很漂亮的推广。\n\n## References\n\n- [Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation](http://gamma.cs.unc.edu/RVO/)\n- [Minkowski Sums](http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences)\n","source":"_posts/paper-rvo.md","raw":"---\ntitle: 论文笔记《Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation》\ntags:\n  - VO(Velocity Obstacle)\n  - RVO(Reciprocal Velocity Obstacle)\ndescription:\n  - 经典的底层避障算法\ncategories:\n  - 论文笔记\ndate: 2017-01-13 21:08:51\n---\n\n\n\n## 简介\n\n在介绍VO，RVO之前，需要先介绍路径规划。\n\n对Agent进行路径规划，实际上要完成的**任务**就是让Agent从点A**无碰撞地**移动到点B。而路径规划的过程是层次化的，其基本框架大致如下：\n\n- **High level**: dijkstra等算法。\n- **Low level**: VO, RVO, ORCA等底层避障算法。\n\n很容易可以跟我们的日常生活进行类比，比如说我们要从学校的教学楼走到宿舍楼，那么以上框架对应的就是：\n\n- **High level**: 通过dijkstra算法，得到路径为: 教学楼→饭堂→体育馆→图书馆→宿舍楼。\n- **Low level**: 通过底层避障算法如VO，RVO，ORCA等底层避障算法，保证我们走的每一段路（e.g. 教学楼→饭堂），都不会跟别的同学发生碰撞。\n\nVO和RVO就是经典的底层避障算法。其中VO是最经典的，RVO则在VO的基础上进行了一些改进，解决了VO抖动的问题。\n\n## VO(Velocity Obstacle)\n\n**一句话总结**VO的思路：只要在未来有可能会发生碰撞的速度，都排除在外。\n\n为方便描述，以下都假设是在**平面**内，**圆形物体**之间的避障。\n\n## VO的直观理解\n\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img prob_disc.png 问题描述 %}\n</div>\n\n> Q: 假设B静止，那么A取什么速度能够保证一定不会跟B发生碰撞呢？\n\n> A: 一种很粗暴的方法，就是**把A化作质点**，选择跟**$\\bar{B}$(扩展后的B)不相交**的速度方向。以后只要在每个周期里面，都选择不在VO的速度，就能够保证不会碰撞。\n\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img prob_sol.png 解决办法 %}\n</div>\n\n以上就是VO的直观理解，需要注意的是：\n\n- VO是指速度方向与$\\bar{B}$相交的部分，即**会发生碰撞的部分（图中灰色斜线部分）**。\n- VO是抱着**宁杀错，不放过**的思想，把所有未来有可能会发生碰撞的速度都放弃了。\n- 实际上假如仅要求**一定时间内**不发生碰撞的话，有更多的速度可供选择，比如说上图中的($v\\_{A}^{\\prime}$)。\n\n## VO的图示理解\n\n有了直观理解之后就可以用更加严谨一点的数学语言图示VO了。\n\n首先将直观理解中口语化的表达转换成对应的数学语言表示。\n\n- **物体A(B)**：以$\\bf{p\\_{A}}$为圆心，$r\\_{A}$为半径的**点集$A$**\n- **假设B静止**：A相对于B的速度，即**相对速度$\\bf{v\\_{A}}-v\\_{B}$**\n- **把A化作质点**：求集合$B$与集合$-A$的Minkowski sum，即**闵氏和，$B\\oplus-A$**，其中\n\t+ $A \\oplus B = \\\\{    {\\bf{a}} + {\\bf{b}}\\ |\\ {\\bf{a}} \\in A, {\\bf{b}} \\in B \\\\} $\n\t+ $-A = \\\\{ -{\\bf{a}}\\ |\\ {\\bf{a}} \\in A \\\\}$\n\t+ [更多关于Minkowski sum](http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences)\n\n于是就有了下图的左半部分（浅色三角形）：\n\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img vo.png VO %}\n</div>\n\n而为了直接求$\\bf{v\\_{A}}$绝对速度的VO而不是$\\bf{v\\_{A}}-v\\_{B}$相对速度的VO，将相对速度下的VO延$\\bf{v\\_{B}}$方向平移，就有了图中右半部分（深色三角形）。\n\n## VO的数学定义\n\n理解了图示，数学定义就很好理解了。\n\n- 首先给出**射线的定义**，用$\\lambda({\\bf{p}},{\\bf{v}})$表示以点$\\bf{p}$为顶点，方向为$\\bf{v}$的射线。\n$$\\lambda({\\bf{p}}, {\\bf{v}}) = \\\\{ {\\bf{p}} +t{\\bf{v}} | t\\ge 0\\\\}$$\n- 接下来就是**VO的定义**了，用$VO^{A}\\_{B}({\\bf{v\\_{B}}})$表示速度为$\\bf{v\\_{B}}$的$B$对$A$的VO \n$$VO^{A}\\_{B}({\\bf{v\\_{B}}}) = \\\\{ {\\bf{v\\_A}} | \\lambda({\\bf{p\\_{A}}}, {\\bf{v\\_{A} - v\\_{B}}}) \\cap B \\oplus -A \\ne \\emptyset \\\\}$$\n\n## RVO(Reciprocal Velocity Obstacle)\n\nVO给出了很漂亮的避障条件，所以后面很多底层的避障算法都是基于VO的，而RVO就是其中之一。\n\nRVO主要解决了VO的抖动问题\n\n- **抖动现象**：如下左图所示，即$A$会在$\\bf{v\\_{A}}$与$\\bf{v\\_{A}^{\\prime}}$之间来回切换\n- **RVO的效果**：如下右图所示，保持$\\bf{v\\_{A}}$，不会抖动\n\n<div style=\"width:700px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img oscillation.png 抖动现象 %}\n</div>\n\n## 证明VO抖动现象存在\n\n首先论文给出了VO的三条性质\n\n- **Symmetry**：$\\bf{v\\_A}$的$A$会撞上$\\bf{v\\_B}$的$B$，则$\\bf{v\\_B}$的$B$也会撞上$\\bf{v\\_A}$的$A$\n$${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Leftrightarrow {\\bf{v\\_B}} \\in VO^{B}\\_{A}({\\bf{v\\_{A}}})$$\n- **Translation Invariance**：$\\bf{v\\_A}$的$A$会撞上$\\bf{v\\_B}$的$B$，则$\\bf{v\\_A+u}$的$A$会撞上$\\bf{v\\_B+u}$的$B$\n$${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Leftrightarrow {\\bf{v\\_A+u}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}+u}})$$\n- **Convexity**：在$VO^{A}\\_{B}({\\bf{v\\_{B}}})$的左（右）侧的两个速度之间的任意速度，也在$VO^{A}\\_{B}({\\bf{v\\_{B}}})$的左（右）侧。VO左（右）侧如下图所示：\n$${\\bf{v\\_A}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\land  {\\bf{v\\_A^{\\prime}}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}) \\Rightarrow (1-\\alpha){\\bf{v\\_A}} + \\alpha{\\bf{v\\_{A}^{\\prime}}} \\overrightarrow{\\notin} VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ for\\ 0\\le \\alpha \\le 1$$\n\n<div style=\"width:300px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img left_right.png VO左（右）侧示意 %}\n</div>\n\n接下来是抖动现象存在的证明\n\n1. 假设初始状态为会发生碰撞：${\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ {\\bf{v\\_B}} \\in VO^{B}\\_{A}({\\bf{v\\_{A}}})$\n2. 由于在对方的VO内，所以各自选择新的速度以防止碰撞：${\\bf{v\\_A^{\\prime}}} \\notin VO^{A}\\_{B}({\\bf{v\\_{B}}}),\\ {\\bf{v\\_B^{\\prime}}} \\notin VO^{B}\\_{A}({\\bf{v\\_{A}}})$\n3. 由前面VO的Symmetry性质可知：此时，**原来的速度不在当前速度的VO内**：${\\bf{v\\_B}} \\notin VO^{B}\\_{A}({\\bf{v\\_{A}^{\\prime}}}),\\ {\\bf{v\\_A}} \\notin VO^{A}\\_{B}({\\bf{v\\_{B}^{\\prime}}})$\n4. 假设我们**更加prefer原来的速度**，则又会回到原来的$\\bf{v\\_A}$与$\\bf{v\\_B}$\n5. 于是在**1→4之间循环**，即发生抖动\n\n## RVO的Insight\n\n首先回想一下为什么会发生抖动：\n\n> 双方为了避障，都偏移了当前速度太多，导致更新速度后，原来速度不再会发生碰撞。\n\n那么我们有没有办法减少对当前速度的偏移，同时又能保证避障呢，RVO的回答是肯定的：\n\n- 缩小VO的大小，新的\"VO\"就叫做RVO\n\t+ p.s. 我个人对Reciprocal的理解是：相对于VO**完全把对方当做木头**，RVO假设对方在避障中也**会承担一定责任**，所以**不用完全靠自己**改变速度来走出VO，有种**互相合作**避障的感觉。\n- 或者换一个角度理解，不再直接选择VO外的速度$\\bf{v\\_A^{\\prime}}$作为新的速度，而是average当前速度$\\bf{v\\_A}$与VO外的速度$\\bf{v\\_A^{\\prime}}$\n\n## RVO的定义与图示\n\n- 速度为$\\bf{v\\_{B}}$的$B$对速度为$\\bf{v\\_A}$的$A$产生的RVO为：\n\n$$ RVO\\_{B}^{A}({\\bf{v\\_B}}, {\\bf{v\\_A}}) = \\\\{ {\\bf{v\\_{A}^{\\prime}}}\\ |\\ 2{\\bf{v\\_{A}^{\\prime}}} - {\\bf{v\\_A}} \\in VO^{A}\\_{B}({\\bf{v\\_{B}}})\\\\}$$\n\n- 图示理解如下：\n\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" >\n  {% asset_img rvo.png RVO %}\n</div>\n\n- 释意：\n\t+ $2{\\bf{v\\_{A}^{\\prime}}} - {\\bf{v\\_A}}$：${\\bf{v\\_A}}$相对于$\\bf{v\\_A^{\\prime}}$的对称点。\n\t+ 所以**公式的含义**是：对称点在原VO中，则中点在RVO中。\n\t+ 所以**RVO的构成**是：$\\bf{v_A}$与原VO中的点的中点。\n\n## RVO不会发生碰撞且没有抖动现象的证明\n\n这一部分不赘述了，论文中写得很详尽，只说一下证明的思路：\n\n- 双方选择**同侧**避障时，不会发生碰撞。\n- 双方一定会选择同侧避障。\n- 不会有抖动现象：原来会撞的在选择新速度后**依然**会撞。\n\n## 收获\n\n- **用数学语言来描述问题**：化作质点的描述、抖动的描述。\n- **从实际应用中发现问题**：抖动问题的发现。\n- **特殊到一般的推广**：论文后面还将RVO推广到一般情况，很漂亮的推广。\n\n## References\n\n- [Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation](http://gamma.cs.unc.edu/RVO/)\n- [Minkowski Sums](http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences)\n","slug":"paper-rvo","published":1,"updated":"2018-01-24T03:42:48.496Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrqc0013xuvtvb59xcen","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>在介绍VO，RVO之前，需要先介绍路径规划。</p>\n<p>对Agent进行路径规划，实际上要完成的<strong>任务</strong>就是让Agent从点A<strong>无碰撞地</strong>移动到点B。而路径规划的过程是层次化的，其基本框架大致如下：</p>\n<ul>\n<li><strong>High level</strong>: dijkstra等算法。</li>\n<li><strong>Low level</strong>: VO, RVO, ORCA等底层避障算法。</li>\n</ul>\n<p>很容易可以跟我们的日常生活进行类比，比如说我们要从学校的教学楼走到宿舍楼，那么以上框架对应的就是：</p>\n<ul>\n<li><strong>High level</strong>: 通过dijkstra算法，得到路径为: 教学楼→饭堂→体育馆→图书馆→宿舍楼。</li>\n<li><strong>Low level</strong>: 通过底层避障算法如VO，RVO，ORCA等底层避障算法，保证我们走的每一段路（e.g. 教学楼→饭堂），都不会跟别的同学发生碰撞。</li>\n</ul>\n<p>VO和RVO就是经典的底层避障算法。其中VO是最经典的，RVO则在VO的基础上进行了一些改进，解决了VO抖动的问题。</p>\n<h2 id=\"VO-Velocity-Obstacle\"><a href=\"#VO-Velocity-Obstacle\" class=\"headerlink\" title=\"VO(Velocity Obstacle)\"></a>VO(Velocity Obstacle)</h2><p><strong>一句话总结</strong>VO的思路：只要在未来有可能会发生碰撞的速度，都排除在外。</p>\n<p>为方便描述，以下都假设是在<strong>平面</strong>内，<strong>圆形物体</strong>之间的避障。</p>\n<h2 id=\"VO的直观理解\"><a href=\"#VO的直观理解\" class=\"headerlink\" title=\"VO的直观理解\"></a>VO的直观理解</h2><div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/prob_disc.png\" alt=\"问题描述\" title=\"问题描述\"><br></div>\n\n<blockquote>\n<p>Q: 假设B静止，那么A取什么速度能够保证一定不会跟B发生碰撞呢？</p>\n<p>A: 一种很粗暴的方法，就是<strong>把A化作质点</strong>，选择跟<strong>$\\bar{B}$(扩展后的B)不相交</strong>的速度方向。以后只要在每个周期里面，都选择不在VO的速度，就能够保证不会碰撞。</p>\n</blockquote>\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/prob_sol.png\" alt=\"解决办法\" title=\"解决办法\"><br></div>\n\n<p>以上就是VO的直观理解，需要注意的是：</p>\n<ul>\n<li>VO是指速度方向与$\\bar{B}$相交的部分，即<strong>会发生碰撞的部分（图中灰色斜线部分）</strong>。</li>\n<li>VO是抱着<strong>宁杀错，不放过</strong>的思想，把所有未来有可能会发生碰撞的速度都放弃了。</li>\n<li>实际上假如仅要求<strong>一定时间内</strong>不发生碰撞的话，有更多的速度可供选择，比如说上图中的($v_{A}^{\\prime}$)。</li>\n</ul>\n<h2 id=\"VO的图示理解\"><a href=\"#VO的图示理解\" class=\"headerlink\" title=\"VO的图示理解\"></a>VO的图示理解</h2><p>有了直观理解之后就可以用更加严谨一点的数学语言图示VO了。</p>\n<p>首先将直观理解中口语化的表达转换成对应的数学语言表示。</p>\n<ul>\n<li><strong>物体A(B)</strong>：以$\\bf{p_{A}}$为圆心，$r_{A}$为半径的<strong>点集$A$</strong></li>\n<li><strong>假设B静止</strong>：A相对于B的速度，即<strong>相对速度$\\bf{v_{A}}-v_{B}$</strong></li>\n<li><strong>把A化作质点</strong>：求集合$B$与集合$-A$的Minkowski sum，即<strong>闵氏和，$B\\oplus-A$</strong>，其中<ul>\n<li>$A \\oplus B = \\{    {\\bf{a}} + {\\bf{b}}\\ |\\ {\\bf{a}} \\in A, {\\bf{b}} \\in B \\} $</li>\n<li>$-A = \\{ -{\\bf{a}}\\ |\\ {\\bf{a}} \\in A \\}$</li>\n<li><a href=\"http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences\" target=\"_blank\" rel=\"external\">更多关于Minkowski sum</a></li>\n</ul>\n</li>\n</ul>\n<p>于是就有了下图的左半部分（浅色三角形）：</p>\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/vo.png\" alt=\"VO\" title=\"VO\"><br></div>\n\n<p>而为了直接求$\\bf{v_{A}}$绝对速度的VO而不是$\\bf{v_{A}}-v_{B}$相对速度的VO，将相对速度下的VO延$\\bf{v_{B}}$方向平移，就有了图中右半部分（深色三角形）。</p>\n<h2 id=\"VO的数学定义\"><a href=\"#VO的数学定义\" class=\"headerlink\" title=\"VO的数学定义\"></a>VO的数学定义</h2><p>理解了图示，数学定义就很好理解了。</p>\n<ul>\n<li>首先给出<strong>射线的定义</strong>，用$\\lambda({\\bf{p}},{\\bf{v}})$表示以点$\\bf{p}$为顶点，方向为$\\bf{v}$的射线。<br>$$\\lambda({\\bf{p}}, {\\bf{v}}) = \\{ {\\bf{p}} +t{\\bf{v}} | t\\ge 0\\}$$</li>\n<li>接下来就是<strong>VO的定义</strong>了，用$VO^{A}_{B}({\\bf{v_{B}}})$表示速度为$\\bf{v_{B}}$的$B$对$A$的VO<br>$$VO^{A}_{B}({\\bf{v_{B}}}) = \\{ {\\bf{v_A}} | \\lambda({\\bf{p_{A}}}, {\\bf{v_{A} - v_{B}}}) \\cap B \\oplus -A \\ne \\emptyset \\}$$</li>\n</ul>\n<h2 id=\"RVO-Reciprocal-Velocity-Obstacle\"><a href=\"#RVO-Reciprocal-Velocity-Obstacle\" class=\"headerlink\" title=\"RVO(Reciprocal Velocity Obstacle)\"></a>RVO(Reciprocal Velocity Obstacle)</h2><p>VO给出了很漂亮的避障条件，所以后面很多底层的避障算法都是基于VO的，而RVO就是其中之一。</p>\n<p>RVO主要解决了VO的抖动问题</p>\n<ul>\n<li><strong>抖动现象</strong>：如下左图所示，即$A$会在$\\bf{v_{A}}$与$\\bf{v_{A}^{\\prime}}$之间来回切换</li>\n<li><strong>RVO的效果</strong>：如下右图所示，保持$\\bf{v_{A}}$，不会抖动</li>\n</ul>\n<div style=\"width:700px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/oscillation.png\" alt=\"抖动现象\" title=\"抖动现象\"><br></div>\n\n<h2 id=\"证明VO抖动现象存在\"><a href=\"#证明VO抖动现象存在\" class=\"headerlink\" title=\"证明VO抖动现象存在\"></a>证明VO抖动现象存在</h2><p>首先论文给出了VO的三条性质</p>\n<ul>\n<li><strong>Symmetry</strong>：$\\bf{v_A}$的$A$会撞上$\\bf{v_B}$的$B$，则$\\bf{v_B}$的$B$也会撞上$\\bf{v_A}$的$A$<br>$${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}) \\Leftrightarrow {\\bf{v_B}} \\in VO^{B}_{A}({\\bf{v_{A}}})$$</li>\n<li><strong>Translation Invariance</strong>：$\\bf{v_A}$的$A$会撞上$\\bf{v_B}$的$B$，则$\\bf{v_A+u}$的$A$会撞上$\\bf{v_B+u}$的$B$<br>$${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}) \\Leftrightarrow {\\bf{v_A+u}} \\in VO^{A}_{B}({\\bf{v_{B}+u}})$$</li>\n<li><strong>Convexity</strong>：在$VO^{A}_{B}({\\bf{v_{B}}})$的左（右）侧的两个速度之间的任意速度，也在$VO^{A}_{B}({\\bf{v_{B}}})$的左（右）侧。VO左（右）侧如下图所示：<br>$${\\bf{v_A}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}) \\land  {\\bf{v_A^{\\prime}}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}) \\Rightarrow (1-\\alpha){\\bf{v_A}} + \\alpha{\\bf{v_{A}^{\\prime}}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}),\\ for\\ 0\\le \\alpha \\le 1$$</li>\n</ul>\n<div style=\"width:300px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/left_right.png\" alt=\"VO左（右）侧示意\" title=\"VO左（右）侧示意\"><br></div>\n\n<p>接下来是抖动现象存在的证明</p>\n<ol>\n<li>假设初始状态为会发生碰撞：${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}),\\ {\\bf{v_B}} \\in VO^{B}_{A}({\\bf{v_{A}}})$</li>\n<li>由于在对方的VO内，所以各自选择新的速度以防止碰撞：${\\bf{v_A^{\\prime}}} \\notin VO^{A}_{B}({\\bf{v_{B}}}),\\ {\\bf{v_B^{\\prime}}} \\notin VO^{B}_{A}({\\bf{v_{A}}})$</li>\n<li>由前面VO的Symmetry性质可知：此时，<strong>原来的速度不在当前速度的VO内</strong>：${\\bf{v_B}} \\notin VO^{B}_{A}({\\bf{v_{A}^{\\prime}}}),\\ {\\bf{v_A}} \\notin VO^{A}_{B}({\\bf{v_{B}^{\\prime}}})$</li>\n<li>假设我们<strong>更加prefer原来的速度</strong>，则又会回到原来的$\\bf{v_A}$与$\\bf{v_B}$</li>\n<li>于是在<strong>1→4之间循环</strong>，即发生抖动</li>\n</ol>\n<h2 id=\"RVO的Insight\"><a href=\"#RVO的Insight\" class=\"headerlink\" title=\"RVO的Insight\"></a>RVO的Insight</h2><p>首先回想一下为什么会发生抖动：</p>\n<blockquote>\n<p>双方为了避障，都偏移了当前速度太多，导致更新速度后，原来速度不再会发生碰撞。</p>\n</blockquote>\n<p>那么我们有没有办法减少对当前速度的偏移，同时又能保证避障呢，RVO的回答是肯定的：</p>\n<ul>\n<li>缩小VO的大小，新的”VO”就叫做RVO<ul>\n<li>p.s. 我个人对Reciprocal的理解是：相对于VO<strong>完全把对方当做木头</strong>，RVO假设对方在避障中也<strong>会承担一定责任</strong>，所以<strong>不用完全靠自己</strong>改变速度来走出VO，有种<strong>互相合作</strong>避障的感觉。</li>\n</ul>\n</li>\n<li>或者换一个角度理解，不再直接选择VO外的速度$\\bf{v_A^{\\prime}}$作为新的速度，而是average当前速度$\\bf{v_A}$与VO外的速度$\\bf{v_A^{\\prime}}$</li>\n</ul>\n<h2 id=\"RVO的定义与图示\"><a href=\"#RVO的定义与图示\" class=\"headerlink\" title=\"RVO的定义与图示\"></a>RVO的定义与图示</h2><ul>\n<li>速度为$\\bf{v_{B}}$的$B$对速度为$\\bf{v_A}$的$A$产生的RVO为：</li>\n</ul>\n<p>$$ RVO_{B}^{A}({\\bf{v_B}}, {\\bf{v_A}}) = \\{ {\\bf{v_{A}^{\\prime}}}\\ |\\ 2{\\bf{v_{A}^{\\prime}}} - {\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}})\\}$$</p>\n<ul>\n<li>图示理解如下：</li>\n</ul>\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\"><br>  <img src=\"/2017/01/13/paper-rvo/rvo.png\" alt=\"RVO\" title=\"RVO\"><br></div>\n\n<ul>\n<li>释意：<ul>\n<li>$2{\\bf{v_{A}^{\\prime}}} - {\\bf{v_A}}$：${\\bf{v_A}}$相对于$\\bf{v_A^{\\prime}}$的对称点。</li>\n<li>所以<strong>公式的含义</strong>是：对称点在原VO中，则中点在RVO中。</li>\n<li>所以<strong>RVO的构成</strong>是：$\\bf{v_A}$与原VO中的点的中点。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"RVO不会发生碰撞且没有抖动现象的证明\"><a href=\"#RVO不会发生碰撞且没有抖动现象的证明\" class=\"headerlink\" title=\"RVO不会发生碰撞且没有抖动现象的证明\"></a>RVO不会发生碰撞且没有抖动现象的证明</h2><p>这一部分不赘述了，论文中写得很详尽，只说一下证明的思路：</p>\n<ul>\n<li>双方选择<strong>同侧</strong>避障时，不会发生碰撞。</li>\n<li>双方一定会选择同侧避障。</li>\n<li>不会有抖动现象：原来会撞的在选择新速度后<strong>依然</strong>会撞。</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li><strong>用数学语言来描述问题</strong>：化作质点的描述、抖动的描述。</li>\n<li><strong>从实际应用中发现问题</strong>：抖动问题的发现。</li>\n<li><strong>特殊到一般的推广</strong>：论文后面还将RVO推广到一般情况，很漂亮的推广。</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://gamma.cs.unc.edu/RVO/\" target=\"_blank\" rel=\"external\">Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation</a></li>\n<li><a href=\"http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences\" target=\"_blank\" rel=\"external\">Minkowski Sums</a></li>\n</ul>\n","excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>在介绍VO，RVO之前，需要先介绍路径规划。</p>\n<p>对Agent进行路径规划，实际上要完成的<strong>任务</strong>就是让Agent从点A<strong>无碰撞地</strong>移动到点B。而路径规划的过程是层次化的，其基本框架大致如下：</p>\n<ul>\n<li><strong>High level</strong>: dijkstra等算法。</li>\n<li><strong>Low level</strong>: VO, RVO, ORCA等底层避障算法。</li>\n</ul>\n<p>很容易可以跟我们的日常生活进行类比，比如说我们要从学校的教学楼走到宿舍楼，那么以上框架对应的就是：</p>\n<ul>\n<li><strong>High level</strong>: 通过dijkstra算法，得到路径为: 教学楼→饭堂→体育馆→图书馆→宿舍楼。</li>\n<li><strong>Low level</strong>: 通过底层避障算法如VO，RVO，ORCA等底层避障算法，保证我们走的每一段路（e.g. 教学楼→饭堂），都不会跟别的同学发生碰撞。</li>\n</ul>\n<p>VO和RVO就是经典的底层避障算法。其中VO是最经典的，RVO则在VO的基础上进行了一些改进，解决了VO抖动的问题。</p>\n<h2 id=\"VO-Velocity-Obstacle\"><a href=\"#VO-Velocity-Obstacle\" class=\"headerlink\" title=\"VO(Velocity Obstacle)\"></a>VO(Velocity Obstacle)</h2><p><strong>一句话总结</strong>VO的思路：只要在未来有可能会发生碰撞的速度，都排除在外。</p>\n<p>为方便描述，以下都假设是在<strong>平面</strong>内，<strong>圆形物体</strong>之间的避障。</p>\n<h2 id=\"VO的直观理解\"><a href=\"#VO的直观理解\" class=\"headerlink\" title=\"VO的直观理解\"></a>VO的直观理解</h2><div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/prob_disc.png\" alt=\"问题描述\" title=\"问题描述\"><br></div>\n\n<blockquote>\n<p>Q: 假设B静止，那么A取什么速度能够保证一定不会跟B发生碰撞呢？</p>\n<p>A: 一种很粗暴的方法，就是<strong>把A化作质点</strong>，选择跟<strong>$\\bar{B}$(扩展后的B)不相交</strong>的速度方向。以后只要在每个周期里面，都选择不在VO的速度，就能够保证不会碰撞。</p>\n</blockquote>\n<div style=\"width:400px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/prob_sol.png\" alt=\"解决办法\" title=\"解决办法\"><br></div>\n\n<p>以上就是VO的直观理解，需要注意的是：</p>\n<ul>\n<li>VO是指速度方向与$\\bar{B}$相交的部分，即<strong>会发生碰撞的部分（图中灰色斜线部分）</strong>。</li>\n<li>VO是抱着<strong>宁杀错，不放过</strong>的思想，把所有未来有可能会发生碰撞的速度都放弃了。</li>\n<li>实际上假如仅要求<strong>一定时间内</strong>不发生碰撞的话，有更多的速度可供选择，比如说上图中的($v_{A}^{\\prime}$)。</li>\n</ul>\n<h2 id=\"VO的图示理解\"><a href=\"#VO的图示理解\" class=\"headerlink\" title=\"VO的图示理解\"></a>VO的图示理解</h2><p>有了直观理解之后就可以用更加严谨一点的数学语言图示VO了。</p>\n<p>首先将直观理解中口语化的表达转换成对应的数学语言表示。</p>\n<ul>\n<li><strong>物体A(B)</strong>：以$\\bf{p_{A}}$为圆心，$r_{A}$为半径的<strong>点集$A$</strong></li>\n<li><strong>假设B静止</strong>：A相对于B的速度，即<strong>相对速度$\\bf{v_{A}}-v_{B}$</strong></li>\n<li><strong>把A化作质点</strong>：求集合$B$与集合$-A$的Minkowski sum，即<strong>闵氏和，$B\\oplus-A$</strong>，其中<ul>\n<li>$A \\oplus B = \\{    {\\bf{a}} + {\\bf{b}}\\ |\\ {\\bf{a}} \\in A, {\\bf{b}} \\in B \\} $</li>\n<li>$-A = \\{ -{\\bf{a}}\\ |\\ {\\bf{a}} \\in A \\}$</li>\n<li><a href=\"http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences\">更多关于Minkowski sum</a></li>\n</ul>\n</li>\n</ul>\n<p>于是就有了下图的左半部分（浅色三角形）：</p>\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/vo.png\" alt=\"VO\" title=\"VO\"><br></div>\n\n<p>而为了直接求$\\bf{v_{A}}$绝对速度的VO而不是$\\bf{v_{A}}-v_{B}$相对速度的VO，将相对速度下的VO延$\\bf{v_{B}}$方向平移，就有了图中右半部分（深色三角形）。</p>\n<h2 id=\"VO的数学定义\"><a href=\"#VO的数学定义\" class=\"headerlink\" title=\"VO的数学定义\"></a>VO的数学定义</h2><p>理解了图示，数学定义就很好理解了。</p>\n<ul>\n<li>首先给出<strong>射线的定义</strong>，用$\\lambda({\\bf{p}},{\\bf{v}})$表示以点$\\bf{p}$为顶点，方向为$\\bf{v}$的射线。<br>$$\\lambda({\\bf{p}}, {\\bf{v}}) = \\{ {\\bf{p}} +t{\\bf{v}} | t\\ge 0\\}$$</li>\n<li>接下来就是<strong>VO的定义</strong>了，用$VO^{A}_{B}({\\bf{v_{B}}})$表示速度为$\\bf{v_{B}}$的$B$对$A$的VO<br>$$VO^{A}_{B}({\\bf{v_{B}}}) = \\{ {\\bf{v_A}} | \\lambda({\\bf{p_{A}}}, {\\bf{v_{A} - v_{B}}}) \\cap B \\oplus -A \\ne \\emptyset \\}$$</li>\n</ul>\n<h2 id=\"RVO-Reciprocal-Velocity-Obstacle\"><a href=\"#RVO-Reciprocal-Velocity-Obstacle\" class=\"headerlink\" title=\"RVO(Reciprocal Velocity Obstacle)\"></a>RVO(Reciprocal Velocity Obstacle)</h2><p>VO给出了很漂亮的避障条件，所以后面很多底层的避障算法都是基于VO的，而RVO就是其中之一。</p>\n<p>RVO主要解决了VO的抖动问题</p>\n<ul>\n<li><strong>抖动现象</strong>：如下左图所示，即$A$会在$\\bf{v_{A}}$与$\\bf{v_{A}^{\\prime}}$之间来回切换</li>\n<li><strong>RVO的效果</strong>：如下右图所示，保持$\\bf{v_{A}}$，不会抖动</li>\n</ul>\n<div style=\"width:700px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/oscillation.png\" alt=\"抖动现象\" title=\"抖动现象\"><br></div>\n\n<h2 id=\"证明VO抖动现象存在\"><a href=\"#证明VO抖动现象存在\" class=\"headerlink\" title=\"证明VO抖动现象存在\"></a>证明VO抖动现象存在</h2><p>首先论文给出了VO的三条性质</p>\n<ul>\n<li><strong>Symmetry</strong>：$\\bf{v_A}$的$A$会撞上$\\bf{v_B}$的$B$，则$\\bf{v_B}$的$B$也会撞上$\\bf{v_A}$的$A$<br>$${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}) \\Leftrightarrow {\\bf{v_B}} \\in VO^{B}_{A}({\\bf{v_{A}}})$$</li>\n<li><strong>Translation Invariance</strong>：$\\bf{v_A}$的$A$会撞上$\\bf{v_B}$的$B$，则$\\bf{v_A+u}$的$A$会撞上$\\bf{v_B+u}$的$B$<br>$${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}) \\Leftrightarrow {\\bf{v_A+u}} \\in VO^{A}_{B}({\\bf{v_{B}+u}})$$</li>\n<li><strong>Convexity</strong>：在$VO^{A}_{B}({\\bf{v_{B}}})$的左（右）侧的两个速度之间的任意速度，也在$VO^{A}_{B}({\\bf{v_{B}}})$的左（右）侧。VO左（右）侧如下图所示：<br>$${\\bf{v_A}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}) \\land  {\\bf{v_A^{\\prime}}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}) \\Rightarrow (1-\\alpha){\\bf{v_A}} + \\alpha{\\bf{v_{A}^{\\prime}}} \\overrightarrow{\\notin} VO^{A}_{B}({\\bf{v_{B}}}),\\ for\\ 0\\le \\alpha \\le 1$$</li>\n</ul>\n<div style=\"width:300px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/left_right.png\" alt=\"VO左（右）侧示意\" title=\"VO左（右）侧示意\"><br></div>\n\n<p>接下来是抖动现象存在的证明</p>\n<ol>\n<li>假设初始状态为会发生碰撞：${\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}}),\\ {\\bf{v_B}} \\in VO^{B}_{A}({\\bf{v_{A}}})$</li>\n<li>由于在对方的VO内，所以各自选择新的速度以防止碰撞：${\\bf{v_A^{\\prime}}} \\notin VO^{A}_{B}({\\bf{v_{B}}}),\\ {\\bf{v_B^{\\prime}}} \\notin VO^{B}_{A}({\\bf{v_{A}}})$</li>\n<li>由前面VO的Symmetry性质可知：此时，<strong>原来的速度不在当前速度的VO内</strong>：${\\bf{v_B}} \\notin VO^{B}_{A}({\\bf{v_{A}^{\\prime}}}),\\ {\\bf{v_A}} \\notin VO^{A}_{B}({\\bf{v_{B}^{\\prime}}})$</li>\n<li>假设我们<strong>更加prefer原来的速度</strong>，则又会回到原来的$\\bf{v_A}$与$\\bf{v_B}$</li>\n<li>于是在<strong>1→4之间循环</strong>，即发生抖动</li>\n</ol>\n<h2 id=\"RVO的Insight\"><a href=\"#RVO的Insight\" class=\"headerlink\" title=\"RVO的Insight\"></a>RVO的Insight</h2><p>首先回想一下为什么会发生抖动：</p>\n<blockquote>\n<p>双方为了避障，都偏移了当前速度太多，导致更新速度后，原来速度不再会发生碰撞。</p>\n</blockquote>\n<p>那么我们有没有办法减少对当前速度的偏移，同时又能保证避障呢，RVO的回答是肯定的：</p>\n<ul>\n<li>缩小VO的大小，新的”VO”就叫做RVO<ul>\n<li>p.s. 我个人对Reciprocal的理解是：相对于VO<strong>完全把对方当做木头</strong>，RVO假设对方在避障中也<strong>会承担一定责任</strong>，所以<strong>不用完全靠自己</strong>改变速度来走出VO，有种<strong>互相合作</strong>避障的感觉。</li>\n</ul>\n</li>\n<li>或者换一个角度理解，不再直接选择VO外的速度$\\bf{v_A^{\\prime}}$作为新的速度，而是average当前速度$\\bf{v_A}$与VO外的速度$\\bf{v_A^{\\prime}}$</li>\n</ul>\n<h2 id=\"RVO的定义与图示\"><a href=\"#RVO的定义与图示\" class=\"headerlink\" title=\"RVO的定义与图示\"></a>RVO的定义与图示</h2><ul>\n<li>速度为$\\bf{v_{B}}$的$B$对速度为$\\bf{v_A}$的$A$产生的RVO为：</li>\n</ul>\n<p>$$ RVO_{B}^{A}({\\bf{v_B}}, {\\bf{v_A}}) = \\{ {\\bf{v_{A}^{\\prime}}}\\ |\\ 2{\\bf{v_{A}^{\\prime}}} - {\\bf{v_A}} \\in VO^{A}_{B}({\\bf{v_{B}}})\\}$$</p>\n<ul>\n<li>图示理解如下：</li>\n</ul>\n<div style=\"width:500px; margin-left:auto; margin-right:auto;\" ><br>  <img src=\"/2017/01/13/paper-rvo/rvo.png\" alt=\"RVO\" title=\"RVO\"><br></div>\n\n<ul>\n<li>释意：<ul>\n<li>$2{\\bf{v_{A}^{\\prime}}} - {\\bf{v_A}}$：${\\bf{v_A}}$相对于$\\bf{v_A^{\\prime}}$的对称点。</li>\n<li>所以<strong>公式的含义</strong>是：对称点在原VO中，则中点在RVO中。</li>\n<li>所以<strong>RVO的构成</strong>是：$\\bf{v_A}$与原VO中的点的中点。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"RVO不会发生碰撞且没有抖动现象的证明\"><a href=\"#RVO不会发生碰撞且没有抖动现象的证明\" class=\"headerlink\" title=\"RVO不会发生碰撞且没有抖动现象的证明\"></a>RVO不会发生碰撞且没有抖动现象的证明</h2><p>这一部分不赘述了，论文中写得很详尽，只说一下证明的思路：</p>\n<ul>\n<li>双方选择<strong>同侧</strong>避障时，不会发生碰撞。</li>\n<li>双方一定会选择同侧避障。</li>\n<li>不会有抖动现象：原来会撞的在选择新速度后<strong>依然</strong>会撞。</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li><strong>用数学语言来描述问题</strong>：化作质点的描述、抖动的描述。</li>\n<li><strong>从实际应用中发现问题</strong>：抖动问题的发现。</li>\n<li><strong>特殊到一般的推广</strong>：论文后面还将RVO推广到一般情况，很漂亮的推广。</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://gamma.cs.unc.edu/RVO/\">Reciprocal Velocity Obstacles for Real-Time Multi-Agent Navigation</a></li>\n<li><a href=\"http://twistedoakstudios.com/blog/Post554\\_minkowski-sums-and-differences\">Minkowski Sums</a></li>\n</ul>\n"},{"title":"论文笔记《Rich feature hierarchies for accurate object detection and semantic segmentation》","date":"2016-07-08T07:45:08.000Z","description":["用于完成object detection的RCNN"],"_content":"\n## 任务简介\n- **目标：**object detection\n- **输入：**图片\n- **输出：**框住前景物体**（有可能多种且每种有多个）**，并且给物体标上类别\n\n## 思路\n1. **分解成两个子任务**\n\t* localization \n\t* classification\n2. **localization：**枚举\n3. **classification：**CNN\n4. 最后再**筛掉**多余的\n\n## 实现流程\n1. **localization：**用selective search提取region proposals（即一个个候选框）\n2. **warped region：**为了适配CNN输入，将region proposals格式归一化\n3. **feature extraction：**CNN\n4. **classification：**对于每个类别训练一个二分类SVM\n5. **综合策略：**NMS(non-maximum suppression)，重叠的选最有信心的\n\n## 实现细节\n- **pre-training：**直接用ILSVRC 2012训练CNN\n- **fine-tuning：**将pre-training训练出来的CNN换上匹配当前任务的输入层（warped region）和分类层（1000-way to 21-way）来训练模型\n\t* **训练集：**warped region的label根据其与GT(Ground Truth)的IoU(Intersection-over-Union，交集/并集)的大小来定。这里将threshold设为0.5\n- **classifier：**将fine-tuning训练出来的模型换上SVM作为分类层\n\t* **训练集：**方法同fine-tuning的训练集，不过这里的threshold设为0.3\n\n## Visualization\n- **学习：**定义某个unit学习的内容为**让这个unit输出最高的input**，即给定一堆input，unit对某个input\\_0的输出最高，则认为这个unit识别的就是这个input\\_0。通过这个定义可以将各个unit学习什么输出出来\n\n## Ablation studies（切除）（用于验证有效性）\n- **ablation：**比较要不要某一个层，对于结果的影响\n- **结果：**\n\t* 在不进行fine-tuning的情况下，CNN中的全连接层并没有什么卵用\n\t* 在进行fine-tuning的情况下，提升很大\n\n## Detection error analysis\n- **量化误差：**给出了percentage of each type - total false positives图，说明了在各个FP时的误差类型分布。错误类型有以下\n\t* 框错位置\n\t* 分成相似的类\n\t* 分成不相似的类\n\t* 分成背景\n- **结果：**RCNN的错误基本都是框错了位置，作者认为这时因为**bottom-up region proposals**还有**positional invariance learned from pre-training CNN**导致的\n- **解决方案：**对于生成的框，要使用**BB(Bounding box regression)**进行进一步调整\n\n## 收获\n- object detection的任务\n- RCNN\n- 评价模型的指标\n\t* 定性：visualization\n\t* 定量：Ablation, detection error\n\t","source":"_posts/paper_rcnn.md","raw":"---\ntitle: 论文笔记《Rich feature hierarchies for accurate object detection and semantic segmentation》\ndate: 2016-07-08 15:45:08\ntags:\n\t- rcnn\ncategories:\n\t- 论文笔记\ndescription:\n\t- 用于完成object detection的RCNN\n---\n\n## 任务简介\n- **目标：**object detection\n- **输入：**图片\n- **输出：**框住前景物体**（有可能多种且每种有多个）**，并且给物体标上类别\n\n## 思路\n1. **分解成两个子任务**\n\t* localization \n\t* classification\n2. **localization：**枚举\n3. **classification：**CNN\n4. 最后再**筛掉**多余的\n\n## 实现流程\n1. **localization：**用selective search提取region proposals（即一个个候选框）\n2. **warped region：**为了适配CNN输入，将region proposals格式归一化\n3. **feature extraction：**CNN\n4. **classification：**对于每个类别训练一个二分类SVM\n5. **综合策略：**NMS(non-maximum suppression)，重叠的选最有信心的\n\n## 实现细节\n- **pre-training：**直接用ILSVRC 2012训练CNN\n- **fine-tuning：**将pre-training训练出来的CNN换上匹配当前任务的输入层（warped region）和分类层（1000-way to 21-way）来训练模型\n\t* **训练集：**warped region的label根据其与GT(Ground Truth)的IoU(Intersection-over-Union，交集/并集)的大小来定。这里将threshold设为0.5\n- **classifier：**将fine-tuning训练出来的模型换上SVM作为分类层\n\t* **训练集：**方法同fine-tuning的训练集，不过这里的threshold设为0.3\n\n## Visualization\n- **学习：**定义某个unit学习的内容为**让这个unit输出最高的input**，即给定一堆input，unit对某个input\\_0的输出最高，则认为这个unit识别的就是这个input\\_0。通过这个定义可以将各个unit学习什么输出出来\n\n## Ablation studies（切除）（用于验证有效性）\n- **ablation：**比较要不要某一个层，对于结果的影响\n- **结果：**\n\t* 在不进行fine-tuning的情况下，CNN中的全连接层并没有什么卵用\n\t* 在进行fine-tuning的情况下，提升很大\n\n## Detection error analysis\n- **量化误差：**给出了percentage of each type - total false positives图，说明了在各个FP时的误差类型分布。错误类型有以下\n\t* 框错位置\n\t* 分成相似的类\n\t* 分成不相似的类\n\t* 分成背景\n- **结果：**RCNN的错误基本都是框错了位置，作者认为这时因为**bottom-up region proposals**还有**positional invariance learned from pre-training CNN**导致的\n- **解决方案：**对于生成的框，要使用**BB(Bounding box regression)**进行进一步调整\n\n## 收获\n- object detection的任务\n- RCNN\n- 评价模型的指标\n\t* 定性：visualization\n\t* 定量：Ablation, detection error\n\t","slug":"paper_rcnn","published":1,"updated":"2018-01-24T03:42:48.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcsiyrqf0015xuvt4099rjyu","content":"<h2 id=\"任务简介\"><a href=\"#任务简介\" class=\"headerlink\" title=\"任务简介\"></a>任务简介</h2><ul>\n<li><strong>目标：</strong>object detection</li>\n<li><strong>输入：</strong>图片</li>\n<li><strong>输出：</strong>框住前景物体<strong>（有可能多种且每种有多个）</strong>，并且给物体标上类别</li>\n</ul>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><ol>\n<li><strong>分解成两个子任务</strong><ul>\n<li>localization </li>\n<li>classification</li>\n</ul>\n</li>\n<li><strong>localization：</strong>枚举</li>\n<li><strong>classification：</strong>CNN</li>\n<li>最后再<strong>筛掉</strong>多余的</li>\n</ol>\n<h2 id=\"实现流程\"><a href=\"#实现流程\" class=\"headerlink\" title=\"实现流程\"></a>实现流程</h2><ol>\n<li><strong>localization：</strong>用selective search提取region proposals（即一个个候选框）</li>\n<li><strong>warped region：</strong>为了适配CNN输入，将region proposals格式归一化</li>\n<li><strong>feature extraction：</strong>CNN</li>\n<li><strong>classification：</strong>对于每个类别训练一个二分类SVM</li>\n<li><strong>综合策略：</strong>NMS(non-maximum suppression)，重叠的选最有信心的</li>\n</ol>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><ul>\n<li><strong>pre-training：</strong>直接用ILSVRC 2012训练CNN</li>\n<li><strong>fine-tuning：</strong>将pre-training训练出来的CNN换上匹配当前任务的输入层（warped region）和分类层（1000-way to 21-way）来训练模型<ul>\n<li><strong>训练集：</strong>warped region的label根据其与GT(Ground Truth)的IoU(Intersection-over-Union，交集/并集)的大小来定。这里将threshold设为0.5</li>\n</ul>\n</li>\n<li><strong>classifier：</strong>将fine-tuning训练出来的模型换上SVM作为分类层<ul>\n<li><strong>训练集：</strong>方法同fine-tuning的训练集，不过这里的threshold设为0.3</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Visualization\"><a href=\"#Visualization\" class=\"headerlink\" title=\"Visualization\"></a>Visualization</h2><ul>\n<li><strong>学习：</strong>定义某个unit学习的内容为<strong>让这个unit输出最高的input</strong>，即给定一堆input，unit对某个input_0的输出最高，则认为这个unit识别的就是这个input_0。通过这个定义可以将各个unit学习什么输出出来</li>\n</ul>\n<h2 id=\"Ablation-studies（切除）（用于验证有效性）\"><a href=\"#Ablation-studies（切除）（用于验证有效性）\" class=\"headerlink\" title=\"Ablation studies（切除）（用于验证有效性）\"></a>Ablation studies（切除）（用于验证有效性）</h2><ul>\n<li><strong>ablation：</strong>比较要不要某一个层，对于结果的影响</li>\n<li><strong>结果：</strong><ul>\n<li>在不进行fine-tuning的情况下，CNN中的全连接层并没有什么卵用</li>\n<li>在进行fine-tuning的情况下，提升很大</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Detection-error-analysis\"><a href=\"#Detection-error-analysis\" class=\"headerlink\" title=\"Detection error analysis\"></a>Detection error analysis</h2><ul>\n<li><strong>量化误差：</strong>给出了percentage of each type - total false positives图，说明了在各个FP时的误差类型分布。错误类型有以下<ul>\n<li>框错位置</li>\n<li>分成相似的类</li>\n<li>分成不相似的类</li>\n<li>分成背景</li>\n</ul>\n</li>\n<li><strong>结果：</strong>RCNN的错误基本都是框错了位置，作者认为这时因为<strong>bottom-up region proposals</strong>还有<strong>positional invariance learned from pre-training CNN</strong>导致的</li>\n<li><strong>解决方案：</strong>对于生成的框，要使用<strong>BB(Bounding box regression)</strong>进行进一步调整</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>object detection的任务</li>\n<li>RCNN</li>\n<li>评价模型的指标<ul>\n<li>定性：visualization</li>\n<li>定量：Ablation, detection error</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"任务简介\"><a href=\"#任务简介\" class=\"headerlink\" title=\"任务简介\"></a>任务简介</h2><ul>\n<li><strong>目标：</strong>object detection</li>\n<li><strong>输入：</strong>图片</li>\n<li><strong>输出：</strong>框住前景物体<strong>（有可能多种且每种有多个）</strong>，并且给物体标上类别</li>\n</ul>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><ol>\n<li><strong>分解成两个子任务</strong><ul>\n<li>localization </li>\n<li>classification</li>\n</ul>\n</li>\n<li><strong>localization：</strong>枚举</li>\n<li><strong>classification：</strong>CNN</li>\n<li>最后再<strong>筛掉</strong>多余的</li>\n</ol>\n<h2 id=\"实现流程\"><a href=\"#实现流程\" class=\"headerlink\" title=\"实现流程\"></a>实现流程</h2><ol>\n<li><strong>localization：</strong>用selective search提取region proposals（即一个个候选框）</li>\n<li><strong>warped region：</strong>为了适配CNN输入，将region proposals格式归一化</li>\n<li><strong>feature extraction：</strong>CNN</li>\n<li><strong>classification：</strong>对于每个类别训练一个二分类SVM</li>\n<li><strong>综合策略：</strong>NMS(non-maximum suppression)，重叠的选最有信心的</li>\n</ol>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><ul>\n<li><strong>pre-training：</strong>直接用ILSVRC 2012训练CNN</li>\n<li><strong>fine-tuning：</strong>将pre-training训练出来的CNN换上匹配当前任务的输入层（warped region）和分类层（1000-way to 21-way）来训练模型<ul>\n<li><strong>训练集：</strong>warped region的label根据其与GT(Ground Truth)的IoU(Intersection-over-Union，交集/并集)的大小来定。这里将threshold设为0.5</li>\n</ul>\n</li>\n<li><strong>classifier：</strong>将fine-tuning训练出来的模型换上SVM作为分类层<ul>\n<li><strong>训练集：</strong>方法同fine-tuning的训练集，不过这里的threshold设为0.3</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Visualization\"><a href=\"#Visualization\" class=\"headerlink\" title=\"Visualization\"></a>Visualization</h2><ul>\n<li><strong>学习：</strong>定义某个unit学习的内容为<strong>让这个unit输出最高的input</strong>，即给定一堆input，unit对某个input_0的输出最高，则认为这个unit识别的就是这个input_0。通过这个定义可以将各个unit学习什么输出出来</li>\n</ul>\n<h2 id=\"Ablation-studies（切除）（用于验证有效性）\"><a href=\"#Ablation-studies（切除）（用于验证有效性）\" class=\"headerlink\" title=\"Ablation studies（切除）（用于验证有效性）\"></a>Ablation studies（切除）（用于验证有效性）</h2><ul>\n<li><strong>ablation：</strong>比较要不要某一个层，对于结果的影响</li>\n<li><strong>结果：</strong><ul>\n<li>在不进行fine-tuning的情况下，CNN中的全连接层并没有什么卵用</li>\n<li>在进行fine-tuning的情况下，提升很大</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Detection-error-analysis\"><a href=\"#Detection-error-analysis\" class=\"headerlink\" title=\"Detection error analysis\"></a>Detection error analysis</h2><ul>\n<li><strong>量化误差：</strong>给出了percentage of each type - total false positives图，说明了在各个FP时的误差类型分布。错误类型有以下<ul>\n<li>框错位置</li>\n<li>分成相似的类</li>\n<li>分成不相似的类</li>\n<li>分成背景</li>\n</ul>\n</li>\n<li><strong>结果：</strong>RCNN的错误基本都是框错了位置，作者认为这时因为<strong>bottom-up region proposals</strong>还有<strong>positional invariance learned from pre-training CNN</strong>导致的</li>\n<li><strong>解决方案：</strong>对于生成的框，要使用<strong>BB(Bounding box regression)</strong>进行进一步调整</li>\n</ul>\n<h2 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h2><ul>\n<li>object detection的任务</li>\n<li>RCNN</li>\n<li>评价模型的指标<ul>\n<li>定性：visualization</li>\n<li>定量：Ablation, detection error</li>\n</ul>\n</li>\n</ul>\n"}],"PostAsset":[{"_id":"source/_posts/note-learning-how-to-learn/learning_how_to_learn.png","slug":"learning_how_to_learn.png","post":"cjcsiyrq4000sxuvtjbc6k0j5","modified":0,"renderable":0},{"_id":"source/_posts/note-linear-algebra/linear_algebra.png","slug":"linear_algebra.png","post":"cjcsiyrq6000uxuvtp42ukqns","modified":0,"renderable":0},{"_id":"source/_posts/paper-orca/opt.png","slug":"opt.png","post":"cjcsiyrq90010xuvt5g3q2j7z","modified":0,"renderable":0},{"_id":"source/_posts/paper-orca/orca_app.png","slug":"orca_app.png","post":"cjcsiyrq90010xuvt5g3q2j7z","modified":0,"renderable":0},{"_id":"source/_posts/paper-orca/orca_sol.png","slug":"orca_sol.png","post":"cjcsiyrq90010xuvt5g3q2j7z","modified":0,"renderable":0},{"_id":"source/_posts/paper-orca/time_interval.png","slug":"time_interval.png","post":"cjcsiyrq90010xuvt5g3q2j7z","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/combination.png","slug":"combination.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/det.png","slug":"det.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/dot_product_order.png","slug":"dot_product_order.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/dot_product_projection.png","slug":"dot_product_projection.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/representation.png","slug":"representation.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/note-essence-of-linear-algrbra/transformation.png","slug":"transformation.png","post":"cjcsiyrq2000oxuvttfe0colc","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/five_agents.gif","slug":"five_agents.gif","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/left_right.png","slug":"left_right.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/oscillation.png","slug":"oscillation.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/prob_disc.png","slug":"prob_disc.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/prob_sol.png","slug":"prob_sol.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/rvo.png","slug":"rvo.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/paper-rvo/vo.png","slug":"vo.png","post":"cjcsiyrqc0013xuvtvb59xcen","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec1_Intro_to_RL.png","slug":"lec1_Intro_to_RL.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec2_MDP.png","slug":"lec2_MDP.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec3_Planning_by_DP.png","slug":"lec3_Planning_by_DP.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec4_Model_Free_Prediction.png","slug":"lec4_Model_Free_Prediction.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec5_Model_Free_Control.png","slug":"lec5_Model_Free_Control.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec6_Value_Function_Approximation.png","slug":"lec6_Value_Function_Approximation.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec7_Policy_Gradient.png","slug":"lec7_Policy_Gradient.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec8_Integrating_Learning_and_Planning.png","slug":"lec8_Integrating_Learning_and_Planning.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0},{"_id":"source/_posts/note-reinforcement-learning/lec9_Exploration_and_Exploitation.png","slug":"lec9_Exploration_and_Exploitation.png","post":"cjcsiyrq7000xxuvt9y2qp03n","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjcsiyroy0000xuvt1whvqdlk","category_id":"cjcsiyrp40002xuvt9fs6adoy","_id":"cjcsiyrph000bxuvt5551umx2"},{"post_id":"cjcsiyrpf000axuvt92ohs6dt","category_id":"cjcsiyrpe0007xuvtud26nh1n","_id":"cjcsiyrpo000hxuvtkiwan8rw"},{"post_id":"cjcsiyrp20001xuvtt3d3w5v9","category_id":"cjcsiyrpe0007xuvtud26nh1n","_id":"cjcsiyrpr000lxuvtp4ftlkjt"},{"post_id":"cjcsiyrp80004xuvteyei9jiw","category_id":"cjcsiyrpe0007xuvtud26nh1n","_id":"cjcsiyrq3000pxuvt3omj0vw1"},{"post_id":"cjcsiyrp90005xuvt5j9j539y","category_id":"cjcsiyrpe0007xuvtud26nh1n","_id":"cjcsiyrq6000vxuvtqu4yy5z9"},{"post_id":"cjcsiyrpc0006xuvtjie6lvpp","category_id":"cjcsiyrpe0007xuvtud26nh1n","_id":"cjcsiyrqc0011xuvtvjwqohx5"},{"post_id":"cjcsiyrph000cxuvtt1vfairk","category_id":"cjcsiyrq7000wxuvtlccsm7gm","_id":"cjcsiyrqg0017xuvtyfqmb1jt"},{"post_id":"cjcsiyrq90010xuvt5g3q2j7z","category_id":"cjcsiyrq7000wxuvtlccsm7gm","_id":"cjcsiyrqh0019xuvt4qki2wp4"},{"post_id":"cjcsiyrqc0013xuvtvb59xcen","category_id":"cjcsiyrq7000wxuvtlccsm7gm","_id":"cjcsiyrqh001bxuvtrvlizwgo"},{"post_id":"cjcsiyrpm000fxuvt7vxd3scc","category_id":"cjcsiyrqc0012xuvtb9d5iijj","_id":"cjcsiyrqi001exuvta81729mr"},{"post_id":"cjcsiyrqf0015xuvt4099rjyu","category_id":"cjcsiyrq7000wxuvtlccsm7gm","_id":"cjcsiyrqi001gxuvt0h89qs1u"},{"post_id":"cjcsiyrpo000ixuvt4ztmt213","category_id":"cjcsiyrqc0012xuvtb9d5iijj","_id":"cjcsiyrqj001jxuvtr33fgn54"},{"post_id":"cjcsiyrpr000mxuvtg1qdehwe","category_id":"cjcsiyrqc0012xuvtb9d5iijj","_id":"cjcsiyrqk001lxuvt522x78ms"},{"post_id":"cjcsiyrq2000oxuvttfe0colc","category_id":"cjcsiyrqj001ixuvtu89u5sa5","_id":"cjcsiyrql001pxuvteccpb61b"},{"post_id":"cjcsiyrq4000sxuvtjbc6k0j5","category_id":"cjcsiyrqj001ixuvtu89u5sa5","_id":"cjcsiyrqm001txuvtdb4qlciw"},{"post_id":"cjcsiyrq6000uxuvtp42ukqns","category_id":"cjcsiyrqj001ixuvtu89u5sa5","_id":"cjcsiyrqn001wxuvtvt9n0t77"},{"post_id":"cjcsiyrq7000xxuvt9y2qp03n","category_id":"cjcsiyrqj001ixuvtu89u5sa5","_id":"cjcsiyrqn001zxuvtckxe404j"}],"PostTag":[{"post_id":"cjcsiyroy0000xuvt1whvqdlk","tag_id":"cjcsiyrp70003xuvtnvtnugzs","_id":"cjcsiyrpe0009xuvtk17iil5c"},{"post_id":"cjcsiyrp20001xuvtt3d3w5v9","tag_id":"cjcsiyrpe0008xuvthju34ucq","_id":"cjcsiyrpn000gxuvtpju4rtto"},{"post_id":"cjcsiyrp80004xuvteyei9jiw","tag_id":"cjcsiyrpl000exuvtp1iqypkf","_id":"cjcsiyrps000nxuvtphqef10q"},{"post_id":"cjcsiyrp90005xuvt5j9j539y","tag_id":"cjcsiyrpq000kxuvtlhubmayd","_id":"cjcsiyrq5000txuvtzx89d0mi"},{"post_id":"cjcsiyrpc0006xuvtjie6lvpp","tag_id":"cjcsiyrq4000rxuvt5e2lqmfi","_id":"cjcsiyrq9000zxuvt81eb2obs"},{"post_id":"cjcsiyrpf000axuvt92ohs6dt","tag_id":"cjcsiyrq9000yxuvt95nk3kgs","_id":"cjcsiyrqg0016xuvt4ex41j24"},{"post_id":"cjcsiyrph000cxuvtt1vfairk","tag_id":"cjcsiyrqe0014xuvtbtyt93kf","_id":"cjcsiyrqh001cxuvtytlc5f6r"},{"post_id":"cjcsiyrpm000fxuvt7vxd3scc","tag_id":"cjcsiyrqh001axuvti06whxrj","_id":"cjcsiyrqi001hxuvtwnimx8aa"},{"post_id":"cjcsiyrpo000ixuvt4ztmt213","tag_id":"cjcsiyrqi001fxuvthcbrff76","_id":"cjcsiyrqk001mxuvtsiesedjp"},{"post_id":"cjcsiyrpr000mxuvtg1qdehwe","tag_id":"cjcsiyrqk001kxuvte8hqr9oo","_id":"cjcsiyrqm001rxuvt898i3bny"},{"post_id":"cjcsiyrq2000oxuvttfe0colc","tag_id":"cjcsiyrql001oxuvt0pqi48a9","_id":"cjcsiyrqn001vxuvt4v8cuarx"},{"post_id":"cjcsiyrq4000sxuvtjbc6k0j5","tag_id":"cjcsiyrqm001sxuvtsfleawmv","_id":"cjcsiyrqn001yxuvtwzzth1fv"},{"post_id":"cjcsiyrq6000uxuvtp42ukqns","tag_id":"cjcsiyrqn001xxuvtbbbkkife","_id":"cjcsiyrqo0021xuvtjqx0dw36"},{"post_id":"cjcsiyrq7000xxuvt9y2qp03n","tag_id":"cjcsiyrqo0020xuvt5353vnx1","_id":"cjcsiyrqo0023xuvtybo7og4v"},{"post_id":"cjcsiyrq90010xuvt5g3q2j7z","tag_id":"cjcsiyrqo0022xuvta7ec7kxo","_id":"cjcsiyrqo0025xuvt1l19mj29"},{"post_id":"cjcsiyrqc0013xuvtvb59xcen","tag_id":"cjcsiyrqo0024xuvtkna1p1gg","_id":"cjcsiyrqp0028xuvtys936xkn"},{"post_id":"cjcsiyrqc0013xuvtvb59xcen","tag_id":"cjcsiyrqo0026xuvtccuzwlzd","_id":"cjcsiyrqp0029xuvt897err0h"},{"post_id":"cjcsiyrqf0015xuvt4099rjyu","tag_id":"cjcsiyrqo0027xuvtcqig362j","_id":"cjcsiyrqp002axuvtddgr6oad"}],"Tag":[{"name":"caffe配置","_id":"cjcsiyrp70003xuvtnvtnugzs"},{"name":"caffe学习","_id":"cjcsiyrpe0008xuvthju34ucq"},{"name":"rnn","_id":"cjcsiyrpl000exuvtp1iqypkf"},{"name":"s2vt data","_id":"cjcsiyrpq000kxuvtlhubmayd"},{"name":"lstm","_id":"cjcsiyrq4000rxuvt5e2lqmfi"},{"name":"s2vt_captioner","_id":"cjcsiyrq9000yxuvt95nk3kgs"},{"name":"AlexNet","_id":"cjcsiyrqe0014xuvtbtyt93kf"},{"name":"沟通的艺术","_id":"cjcsiyrqh001axuvti06whxrj"},{"name":"精进","_id":"cjcsiyrqi001fxuvthcbrff76"},{"name":"哲学家们都干了些什么","_id":"cjcsiyrqk001kxuvte8hqr9oo"},{"name":"线性代数的本质","_id":"cjcsiyrql001oxuvt0pqi48a9"},{"name":"学习方法","_id":"cjcsiyrqm001sxuvtsfleawmv"},{"name":"线性代数","_id":"cjcsiyrqn001xxuvtbbbkkife"},{"name":"强化学习","_id":"cjcsiyrqo0020xuvt5353vnx1"},{"name":"ORCA","_id":"cjcsiyrqo0022xuvta7ec7kxo"},{"name":"VO(Velocity Obstacle)","_id":"cjcsiyrqo0024xuvtkna1p1gg"},{"name":"RVO(Reciprocal Velocity Obstacle)","_id":"cjcsiyrqo0026xuvtccuzwlzd"},{"name":"rcnn","_id":"cjcsiyrqo0027xuvtcqig362j"}]}}