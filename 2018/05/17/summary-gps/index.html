<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Guided Policy Search,GPS," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一种Model-Based的强化学习算法">
<meta property="og:type" content="article">
<meta property="og:title" content="学习总结《Guided Policy Search》">
<meta property="og:url" content="https://www.meltycriss.com/2018/05/17/summary-gps/index.html">
<meta property="og:site_name" content="陪你度过漫长岁月">
<meta property="og:description" content="一种Model-Based的强化学习算法">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/1.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/2.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/3.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/4.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/3_1.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/lqr.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/3.png">
<meta property="og:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/3.png">
<meta property="og:updated_time" content="2024-08-13T16:03:47.876Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="学习总结《Guided Policy Search》">
<meta name="twitter:description" content="一种Model-Based的强化学习算法">
<meta name="twitter:image" content="https://www.meltycriss.com/2018/05/17/summary-gps/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.meltycriss.com/2018/05/17/summary-gps/"/>





  <title> 学习总结《Guided Policy Search》 | 陪你度过漫长岁月 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W3LFZLC5CC"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W3LFZLC5CC');
</script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">陪你度过漫长岁月</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://www.meltycriss.com/2018/05/17/summary-gps/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Criss">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="陪你度过漫长岁月">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="陪你度过漫长岁月" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                学习总结《Guided Policy Search》
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-17T18:49:29+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习总结/" itemprop="url" rel="index">
                    <span itemprop="name">学习总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/17/summary-gps/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/05/17/summary-gps/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
              <div class="post-description">
                  一种Model-Based的强化学习算法
              </div>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Overivew"><a href="#Overivew" class="headerlink" title="Overivew"></a>Overivew</h1><p>本文将介绍<em>Guided Policy Search(GPS)</em>这种Model-Based的强化学习算法，并会按以下方式展开：</p>
<ol>
<li>Motivation：介绍<em>GPS</em>这种方法的<strong>特点</strong>；</li>
<li>Problem formulation：介绍<em>GPS</em>所解决<strong>问题的具体形式</strong>；</li>
<li>Framework：介绍<em>GPS</em>的<strong>整体框架</strong>；</li>
<li>Deterministic policy case：从相对简单的deterministic policy case开始介绍<em>GPS</em>的<strong>具体流程</strong>；</li>
<li>Stochastic policy case：从deterministic policy case过渡到stochastic policy case，介绍<strong>完整</strong>的<em>GPS</em>算法。</li>
</ol>
<hr>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>下面通过将<em>GPS</em>跟不同类型的算法进行比较来说明<em>GPS</em>的特点。</p>
<ul>
<li>与Model-Free算法相比：由于<em>GPS</em>对环境建立了模型（i.e., model/dynamics），在计算q value时，不仅仅是“记录”，还能进行“推算”，因此<strong>sample efficiency会比较高，收敛速度会比较快</strong>；</li>
<li>与其他Model-Based算法相比：在test的时候，部分Model-Based算法需要进行online optimization（i.e., 根据当前拟合的模型，使用某种优化算法，得到当前的action）；而由于<em>GPS</em>最终得到的是一个parametric policy（e.g., 神经网络），因此<strong>在test的时候会比较快</strong>（无需做online optimization，只需要做一次forward pass）；</li>
<li>与简单的Imitation Learning算法相比：在简单的IL中，teacher单方面地给student传授知识（i.e., supervised learning），而不管student的学习能力如何；而在<em>GPS</em>中，<strong>除了teacher给student传授知识以外，student还会给teacher反馈</strong>，要求teacher适配student的学习能力，形成闭环，因此train的效果会更好。</li>
</ul>
<hr>
<h1 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h1><p>下面对我们所讨论的问题做一个界定。</p>
<ul>
<li>setting<ul>
<li><strong>fixed time length</strong> task（e.g, 完成任务的时长限定在20s内，决策的频率是20Hz，则总步长$T$为400）</li>
</ul>
</li>
<li>assumption<ul>
<li>deterministic dynamics：$x_t = f(x_{t-1}, u_{t-1})$</li>
</ul>
</li>
<li>input<ul>
<li>environment</li>
<li>immediate cost(reward) function：$c(x_t, u_t)$</li>
</ul>
</li>
<li>output<ul>
<li>parametric policy<ul>
<li>deterministic case：$u_t = \pi_\theta(x_t)$</li>
<li>stochastic case：$\pi_\theta(u_t \vert x_t) \sim \mathcal{N}(\mu_t, \Sigma_t)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h1><p>下面介绍<em>GPS</em>的整体框架。</p>
<ol>
<li>collect data：通过controller与environment进行交互，收集数据；<div style="width:300px; margin-left:auto; margin-right:auto;"><br><img src="/2018/05/17/summary-gps/1.png" alt="1.png" title=""><br></div></li>
<li>fit dynamics：根据第一步收集到的数据，拟合dynamics；<div style="width:300px; margin-left:auto; margin-right:auto;"><br><img src="/2018/05/17/summary-gps/2.png" alt="2.png" title=""><br></div></li>
<li>optimization：优化controller（最优控制器）与policy（最终输出的parametric policy）；<div style="width:400px; margin-left:auto; margin-right:auto;"><br><img src="/2018/05/17/summary-gps/3.png" alt="3.png" title=""><br></div></li>
<li>next iteration：迭代。<div style="width:500px; margin-left:auto; margin-right:auto;"><br><img src="/2018/05/17/summary-gps/4.png" alt="4.png" title=""><br></div>

</li>
</ol>
<hr>
<h1 id="Deterministic-policy-case"><a href="#Deterministic-policy-case" class="headerlink" title="Deterministic policy case"></a>Deterministic policy case</h1><p>下面开始介绍Deterministic policy case下<em>GPS</em>的具体流程。</p>
<h2 id="Collect-data"><a href="#Collect-data" class="headerlink" title="Collect data"></a>Collect data</h2><p>使用controller与environment进行交互，得到轨迹数据集$\mathcal{D} = { \tau_i }$，其中轨迹$\tau_i = { x_{1i}, u_{1i}, \dots , x_{1T}, u_{1T} }$。</p>
<h2 id="Fit-dynamics"><a href="#Fit-dynamics" class="headerlink" title="Fit dynamics"></a>Fit dynamics</h2><p>使用第一步得到的轨迹数据集来拟合一个time-varying linear model：$x_{t+1} = f(x_t, u_t) = A_tx_t + B_tu_t + c_t$，也就是说这个model<strong>对于不同时刻$t$的input会使用不同的linear model</strong>。</p>
<p>具体的拟合方法是使用同一时刻$t$，不同轨迹$\tau_i$的数据${ (x_{t1}, u_{t1}, x_{t+1,1}), \dots , (x_{ti}, u_{ti}, x_{t+1,i}) }$来做linear regression。</p>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><ul>
<li>big picture</li>
</ul>
<div style="width:250px; margin-left:auto; margin-right:auto;"><br>  <img src="/2018/05/17/summary-gps/3_1.png" alt="3_1.png" title=""><br></div>

<ul>
<li>optimization problem<ul>
<li>我们想要在满足dynamics约束的条件下，最小化总cost，因此对应的轨迹优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{x_1, u_1, \dots, x_T, u_T} &amp;\sum_{t=1}^T c(x_t, u_t) \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li>solution<ul>
<li>假如上述轨迹优化问题中$f(x_t, u_t) = F_t \begin{bmatrix} x_t \\\ u_t \end{bmatrix} + f_t$， $c(x_t, u_t) = \frac{1}{2} \begin{bmatrix} x_t \\\ u_t \end{bmatrix}^T C_t \begin{bmatrix} x_t \\\ u_t \end{bmatrix} + \begin{bmatrix} x_t \\\ u_t \end{bmatrix}^T c_t$，则有一个叫做<em>Linear Quadratic Regulator(LQR)</em>的优化方法可以对该轨迹优化问题进行求解。</li>
<li><em>LQR</em>基本原理如下：<ul>
<li>input：<ul>
<li>linear model（$F_t, f_t$）</li>
<li>quadratic cost（$C_t, c_t$）</li>
</ul>
</li>
<li>output：$K_t, k_t$</li>
<li>optimal control（上述轨迹优化问题的解）：$u_t=K_tx_t+k_t$，$x_{t+1}=f(x_t, u_t)$<div style="width:500px; margin-left:auto; margin-right:auto;"><br><img src="/2018/05/17/summary-gps/lqr.png" alt="lqr.png" title=""><br></div></li>
</ul>
</li>
<li>由于我们在fit dynamics中拟合的就是一个linear model，因此只要我们设置的cost function是quadratic的话就可以使用<em>LQR</em>来求解。</li>
<li>即使我们的cost function不是quadratic，也还可以使用<em>iterative LQR(iLQR)</em>来进行求解。<em>iLQR</em>可以求解nonlinear dynamics，nonquadratic cost下的轨迹优化问题，它的核心思想是对dynamics和cost分别进行linear及quadratic展开，然后用<em>LQR</em>进行求解。</li>
</ul>
</li>
</ul>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><ul>
<li>big picture</li>
</ul>
<div style="width:400px; margin-left:auto; margin-right:auto;"><br>  <img src="/2018/05/17/summary-gps/3.png" alt="3.png" title=""><br></div>

<ul>
<li>optimization problem<ul>
<li>因为<em>GPS</em>最后想要的是一个policy，因此在上述轨迹优化问题的基础上，加上一个优化变量$\theta$以及约束条件$u_t = \pi_\theta(x_t)$，来表示我们希望policy能够“学习”optimal control：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{x_1, u_1, \dots, x_T, u_T, \theta} &amp;\sum_{t=1}^T c(x_t, u_t) \\<br>\text{s.t. } &amp;u_t = \pi_\theta(x_t) \quad t=1, \dots, T\\<br>&amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li>solution<ul>
<li>求解上述优化问题用到了一种叫做<em>Dual Gradient Descent(DGD)</em>的优化方法。<em>DGD</em>是一种求解形如以下优化问题的方法：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_x &amp;f(x) \\<br>\text{s.t. } &amp; C(x)=0.<br>\end{aligned}<br>$$其具体求解过程为：<ol>
<li>写出对应的Lagrangian函数：$\mathcal{L}(x, \lambda) = f(x) + \lambda C(x)$；</li>
<li>求解最小化Lagrangian函数的$x$：$x^* = \mathop{\text{argmin }}_x \mathcal{L}(x, \lambda)$；</li>
<li>将$x^*$代入$\mathcal{L}(x, \lambda)$得到原问题的下界函数$g(\lambda) = \mathcal{L}(x^*, \lambda)$；</li>
<li>更新$\lambda$（类似对$\lambda$做gradient ascent）：$\lambda = \lambda + \alpha \frac{\partial g}{\partial \lambda}$；</li>
<li>回到第二步，求新$\lambda$下对应的$x^*$，迭代。</li>
</ol>
</li>
<li>接下来改变一下优化问题的表述方式，<strong>将dynamics约束单独提出来</strong>（之所以这么做，是<strong>为了将问题化归为会解的问题</strong>，下面会详细说），也就是说现在把问题理解为在满足dynamics约束下求解一个约束优化问题（i.e., 满足policy约束下最小化cost），而原来是满足dynamics和policy约束下求解一个无约束优化问题（i.e., 最小化cost）：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{x_1, u_1, \dots, x_T, u_T, \theta} &amp;\sum_{t=1}^T c(x_t, u_t) \\<br>\text{s.t. } &amp;u_t = \pi_\theta(x_t) \quad t=1, \dots, T\\<br>\\<br>\text{s.t. } x_t = f&amp;(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
<li>然后进行<strong>约束条件下的<em>DGD</em></strong>（i.e., 整个<em>DGD</em>的过程都要满足约束）：<ol>
<li>仅将policy约束写进Lagrangian函数：<br>$$<br>\begin{aligned}<br>\mathcal{L}(\tau, \theta, \lambda)<br>&amp;= \mathcal{L}(x_1, u_1, \dots, x_T, u_T, \theta, \lambda_1, \dots, \lambda_T) \\<br>&amp;=  \sum_{t=1}^T c(x_t, u_t) + \sum_{t=1}^T\lambda_t [u_t - \pi_\theta(x_t)] \\<br>&amp;= \sum_{t=1}^T { c(x_t, u_t) + \lambda_t [u_t - \pi_\theta(x_t)] }.<br>\end{aligned}<br>$$</li>
<li>本来应该将$\begin{bmatrix}\tau \\\ \theta\end{bmatrix}$这一整个向量看作优化变量（对应<em>DGD</em>中的$x$），然后求解$\begin{bmatrix}\tau \\\ \theta\end{bmatrix}^*$（对应<em>DGD</em>中的$x^*$）。但这里<strong>假设$\tau$和$\theta$相互独立</strong>（为了能够求解），因此下面可以分别单独对$\tau$与$\theta$做优化，求得对应的$\tau^*$和$\theta^*$，然后用$\begin{bmatrix}\tau^* \\\ \theta^*\end{bmatrix}$来近似$\begin{bmatrix}\tau \\\ \theta\end{bmatrix}^*$。</li>
<li>求解$\tau^*$：对应的优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_\tau &amp;\sum_{t=1}^T { c(x_t, u_t) + \lambda_t [u_t - \pi_\theta(x_t)] } \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$设$\tilde{c}(x_t, u_t)=c(x_t, u_t) + \lambda_t [u_t - \pi_\theta(x_t)]$，我们有：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_\tau &amp;\sum_{t=1}^T \tilde{c}(x_t, u_t)\\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$而<strong>这个形式的优化问题我们能够通过<em>iLQR</em>进行求解</strong>，因此能够求得$\tau^*$。</li>
<li>求解$\theta^*$：对应的优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_\theta &amp;\sum_{t=1}^T { c(x_t, u_t) + \lambda_t [u_t - \pi_\theta(x_t)] } \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$由于优化变量是$\theta$，所以约束条件可以去掉，而且目标函数也可以简化，等价的优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_\theta &amp;\sum_{t=1}^T \lambda_t [u_t - \pi_\theta(x_t)].<br>\end{aligned}<br>$$这是一个无约束优化问题（带权拟合问题），可以用诸如<em>SGD</em>等方法进行优化，因此也能够求得$\theta^*$。</li>
<li>将$\tau^*$和$\theta^*$代入Lagrangian函数，得到$g(\lambda) = \mathcal{L}(\tau^*, \theta^*, \lambda)$。</li>
<li>对$\lambda$做单步gradient ascent（跟求解$\theta^*$时一样，由于约束条件跟优化变量无关，所以等价于无约束优化），更新$\lambda$，迭代。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="Next-iteration"><a href="#Next-iteration" class="headerlink" title="Next iteration"></a>Next iteration</h2><p>回到第一步collect data，即用新的controller跟environment交互，迭代。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>至此我们已经通过相对简单的deterministic policy case过了一遍<em>GPS</em>的整个流程，下面补充一些小小的细节：</p>
<ul>
<li>为什么称之为<em>GPS</em>：guided主要体现在最后优化问题的约束$u_t = \pi_\theta(x_t)$，相当于有一个teacher（controller）在“指引”我们的policy。</li>
<li>collect data使用controller还是policy：应该是使用controller，因为policy仅仅拟合了某些具体的$(x_t, u_t)$，而controller对各个$x_t$都是“最优”的。</li>
<li>为什么流程图中，train policy部分会跟$\tau_i$相关：这是因为在求解$\theta^*$时，$\tau^*$还没代入Lagrangian函数，也就是说Lagrangian函数中用的是$\tau$而不是$\tau^*$。</li>
</ul>
<hr>
<h1 id="Stochastic-policy-case"><a href="#Stochastic-policy-case" class="headerlink" title="Stochastic policy case"></a>Stochastic policy case</h1><p>deterministic controller对于一个起始点只能采集到一条轨迹，<strong>不利于拟合一个robust的dynamics</strong>，因此我们想要一个stochastic的controller。之后通过对stochastic controller进行supervised learning，我们最后会得到一个stochastic policy。</p>
<p>stochastic policy case跟deterministic policy case的<strong>唯一区别在于optimization这步</strong>（如红框所示），其他都是一样的。因此下面仅介绍stochastic policy case如何做optimization。</p>
<div style="width:400px; margin-left:auto; margin-right:auto;"><br>  <img src="/2018/05/17/summary-gps/3.png" alt="3.png" title=""><br></div>

<h2 id="Using-stochastic-controller"><a href="#Using-stochastic-controller" class="headerlink" title="Using stochastic controller"></a>Using stochastic controller</h2><p>首先介绍一个过渡性的内容，是关于使用stochastic controller的。</p>
<ul>
<li>最直观的想法：把原来的约束问题（暂时先不考虑policy）改一下，将controller表示为$p(u_t \vert x_t) \sim \mathcal{N}(\mu_t, \Sigma_t)$，然后求解优化问题：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T,<br>\end{aligned}<br>$$其中$p = \mu_1, \Sigma_1, \dots, \mu_T, \Sigma_T$。但这样子是不work的，因为<strong>最后求出来的还是一个determinisitc controller</strong>（因为贪婪地采取最优策略效果是最好的）。</li>
<li>于是就强行把controller设成linear-gaussian的形式：<br>$$<br>p(u_t \vert x_t) = \mathcal{N}(K_t(x_t - \hat{x}_t) + k_t + \hat{u}_t, Q^{-1}_{u_t,u_t}),<br>$$其中，gaussian的均值为<em>iLQR</em>的解，方差为<em>iLQR</em>中的某个矩阵的逆，具体含义可以不用管，只需要知道这些东西都可以通过<em>iLQR</em>得到。</li>
<li>而该形式的controller恰好是以下优化问题的最优解：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\sum_{t=1}^T\mathbb{E}_{p(x_t, u_t)}[ c(x_t, u_t) - \mathcal{H}(p(u_t \vert x_t))] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
<li>这部分主要为下面做铺垫，核心在于<strong>我们现在知道了某种形式的优化问题的最优解</strong>。</li>
</ul>
<h2 id="Constrain-controller-in-trust-region"><a href="#Constrain-controller-in-trust-region" class="headerlink" title="Constrain controller in trust region"></a>Constrain controller in trust region</h2><ul>
<li>motivation<ul>
<li>在<em>GPS</em>中我们所fit的dynamics是local dynamics，也就是说它<strong>仅在当前controller“附近”是有效的</strong>；</li>
<li>在优化问题中，我们评价controller好坏会用到该local dynamics；</li>
<li>因此，为了确保评价的有效性，我们要<strong>限定controller在当前controller“附近”</strong>；</li>
<li>否则，即使contoller评价很高，也是虚假的/无效的/没有意义的。</li>
</ul>
</li>
<li>solution<ul>
<li>将controller在当前controller“附近”用数学语言表述出来，即<br>$$<br>D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) \leq \epsilon,<br>$$其中$p(\tau)$表示轨迹$\tau$在新controller下发生的概率，$\bar{p}(\tau)$表示轨迹$\tau$在当前controller下发生的概率，$D_{KL}$表示KL-divergence，用于衡量两个分布的“相似程度”，KL-divergence越小，两个分布越相似。<ul>
<li>这个表示能够<strong>间接地限制controller和当前controller比较相似</strong>，因为$p(\tau) = p(x_1)\prod_{t=1}^{T-1} p(u_t \vert x_t)f(x_{t+1} \vert x_t, u_t)$，而对于新旧controller来说，$p(x_1)$以及$f(x_{t+1} \vert x_t, u_t)$都是一样的，因此区别仅仅在于$p(u_t \vert x_t)$。换句话说，$p(\tau)$和$\bar{p}(\tau)$的区别仅仅在于controller，因此控制$p(\tau)$和$\bar{p}(\tau)$的相似程度就可以间接地控制controller的相似程度。</li>
</ul>
</li>
<li>new optimization problem<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] \\<br>\text{s.t. } &amp;D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) \leq \epsilon \\<br>&amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
<li>进行约束条件下的<em>DGD</em>（这里我不太懂为什么可以对不等式约束做<em>DGD</em>，逻辑上来讲对于不等式约束，需要限定$\lambda \geq 0$才能保证$g(\lambda)$是原问题的下界函数）：<ol>
<li>仅将$D_{KL}$约束写进Lagrangian函数：<br>$$<br>\begin{aligned}<br>\mathcal{L}(\tau, p, \lambda) &amp;= \mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] + \lambda (D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) - \epsilon) \\<br>&amp;// D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) = \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)} [- \log \bar{p}(u_t \vert x_t) - \mathcal{H}(p(u_t \vert x_t))]  \\<br>&amp;= \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[c(x_t, u_t) - \lambda \log \bar{p}(u_t \vert x_t) - \lambda \mathcal{H}(p(u_t \vert x_t))] - \lambda \epsilon \\<br>&amp;//提\lambda \\<br>&amp;= \lambda \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[\frac{1}{\lambda} c(x_t, u_t) - \log \bar{p}(u_t \vert x_t) - \mathcal{H}(p(u_t \vert x_t))] - \epsilon<br>\end{aligned}<br>$$</li>
<li>求解$\begin{bmatrix} \tau \\\ p \end{bmatrix}^*$：对应的优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[\frac{1}{\lambda} c(x_t, u_t) - \log \bar{p}(u_t \vert x_t) - \mathcal{H}(p(u_t \vert x_t))] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$假如我们设$\tilde{c}(x_t, u_t) = \frac{1}{\lambda} c(x_t, u_t) - \log \bar{p}(u_t \vert x_t)$，则该优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[\tilde{c}(x_t, u_t) - \mathcal{H}(p(u_t \vert x_t))] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$而<strong>这个形式的优化问题我们知道其最优解的形式是什么</strong>（回忆Using stochastic controller部分），因此我们能够用<em>iLQR</em>求解$\begin{bmatrix} \tau \\\ p \end{bmatrix}^*$。</li>
<li>对$\lambda$做单步gradient ascent，迭代。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="Introduce-stochastic-policy"><a href="#Introduce-stochastic-policy" class="headerlink" title="Introduce stochastic policy"></a>Introduce stochastic policy</h2><ul>
<li>statement<ul>
<li><strong>这部分由于没有细看论文，所以有些地方我也不是很清楚，只是介绍我自己的理解，希望谅解：）</strong>。</li>
</ul>
</li>
<li>motivation<ul>
<li>我们想要得到的是一个parametric policy，因此采用跟deterministic policy case一样的思路，添加一个policy约束。</li>
</ul>
</li>
<li>solution<ul>
<li>optimization problem<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] \\<br>\text{s.t. } &amp;p(u_t \vert x_t) = \pi_\theta(u_t \vert x_t) \quad t=1, \dots, T \\<br>&amp;D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) \leq \epsilon \\<br>&amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$</li>
<li>进行约束条件下的<em>DGD</em>：<ol>
<li>仅将policy约束和$D_{KL}$约束写进Lagrangian函数：<br>$$<br>\begin{aligned}<br>\mathcal{L}(\tau, p, \theta, \lambda, \eta) &amp;= \mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] + \sum_{t=1}^T \lambda_t [p(u_t \vert x_t) - \pi_\theta(u_t \vert x_t)] + \eta (D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) - \epsilon) \\<br>&amp;// \sum_{t=1}^T \lambda_t [p(u_t \vert x_t) - \pi_\theta(u_t \vert x_t)] \overset{?}{\approx} \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[u_t^T\lambda_t + \rho_t D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))]\\<br>&amp;\approx \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[c(x_t, u_t) + u_t^T\lambda_t + \rho_t D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))] + \eta (D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) - \epsilon) \\<br>&amp;// \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))] = \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[- \log \pi_\theta(u_t \vert x_t) - \mathcal{H}(p(u_t \vert x_t))] \\<br>&amp;// D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) = \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)} [- \log \bar{p}(u_t \vert x_t) - \mathcal{H}(p(u_t \vert x_t))]  \\<br>&amp;= \sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[c(x_t, u_t) + u_t^T\lambda_t - \rho_t \log\pi_\theta(u_t \vert x_t) - \eta \log \bar{p}(u_t \vert x_t) - ( \rho_t + \eta) \mathcal{H}(p(u_t \vert x_t))] - \eta \epsilon<br>\end{aligned}<br>$$</li>
<li>求解$\begin{bmatrix} \tau \\\ p \end{bmatrix}^*$：对应的优化问题为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[c(x_t, u_t) + u_t^T\lambda_t - \rho_t \log\pi_\theta(u_t \vert x_t) - \eta \log \bar{p}(u_t \vert x_t) - ( \rho_t + \eta) \mathcal{H}(p(u_t \vert x_t))] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$设$\tilde{c}(x_t, u_t) = c(x_t, u_t) + u_t^T\lambda_t - \rho_t \log\pi_\theta(u_t \vert x_t) - \eta \log \bar{p}(u_t \vert x_t)$，$\nu_t = \rho_t + \eta$，则上述优化问题可以表示为：<br>$$<br>\begin{aligned}<br>\mathop{\text{min }}_{\tau, p} &amp;\sum_{t=1}^T \mathbb{E}_{p(x_t, u_t)}[\tilde{c}(x_t, u_t) - \nu_t \mathcal{H}(p(u_t \vert x_t))] \\<br>\text{s.t. } &amp;x_t = f(x_{t-1}, u_{t-1}) \quad t=1, \dots, T.<br>\end{aligned}<br>$$而<strong>这个形式的优化问题我们知道最优解</strong>（回忆Using stochastic controller和Constrain controller in trust region部分），因此得到$\begin{bmatrix} \tau \\\ p \end{bmatrix}^*$。</li>
<li>求解$\theta^*$：跟deterministic policy case一样，由于约束条件跟待优化变量无关，所以实际上是一个无约束优化问题，不过问题是这里并没有用直接用上面的Lagrangian函数，我猜测问题是出在上面我没搞懂的带问号的约等于号那里，也许上面只是为了求解方便做了近似，因此在求解$\theta^*$时没有把Lagrangian函数展开到最后的形式，而是用了以下Lagrangian函数：<br>$$<br>\begin{aligned}<br>\mathcal{L}(\tau, p, \theta, \lambda, \eta) &amp;= \mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] + \sum_{t=1}^T \lambda_t [p(u_t \vert x_t) - \pi_\theta(u_t \vert x_t)] + \eta (D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) - \epsilon) \\<br>&amp;// \sum_{t=1}^T \lambda_t [p(u_t \vert x_t) - \pi_\theta(u_t \vert x_t)] \overset{?}{=} \sum_{t=1}^T \mathbb{E}_{p(x_t)}[\mathbb{E}_{\pi_\theta(u_t \vert x_t)}[u_t]^T\lambda_t + \rho_t D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))]\\<br>&amp;= \mathbb{E}_\tau[\sum_{t=1}^T c(x_t, u_t)] + \sum_{t=1}^T \mathbb{E}_{p(x_t)}[\mathbb{E}_{\pi_\theta(u_t \vert x_t)}[u_t]^T\lambda_t + \rho_t D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))] \\<br>&amp;+ \eta (D_{KL}(p(\tau) \Vert \bar{p}(\tau) ) - \epsilon) \\<br>\end{aligned}.<br>$$因此，对应的关于$\theta$的无约束优化问题为：<br>$$<br>\mathop{\text{min }}_{\theta}\sum_{t=1}^T \mathbb{E}_{p(x_t)}[\mathbb{E}_{\pi_\theta(u_t \vert x_t)}[u_t]^T\lambda_t + \rho_t D_{KL}(p(u_t \vert x_t) \Vert \pi_\theta(u_t \vert x_t))].<br>$$应该可以使用诸如<em>SGD</em>等优化方法进行求解，于是我们得到$\theta^*$</li>
<li>对$\begin{bmatrix} \lambda \\\ \eta \end{bmatrix}$做单步gradient ascent，迭代。</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>至此已经介绍完<em>GPS</em>的流程，这个算法给我印象最深刻的地方就是<strong>化归</strong>的思想，即将新构建的问题转化为之前已经知道解决方法的问题，令人叹为观止。</p>
<hr>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="external">CS294 Fall 2017 lecture 8-10</a></li>
<li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa15/slides/lecture20-guided-policy-search.pdf" target="_blank" rel="external">CS287 Guest Lecture Sergey Levine Slide</a></li>
<li><a href="https://www.youtube.com/watch?v=CW1s6psByxk" target="_blank" rel="external">Sergey Levine UCB Talk</a></li>
<li><a href="https://www.youtube.com/watch?v=EtMyH_--vnU&amp;feature=youtu.be" target="_blank" rel="external">Sergey Levine UW Talk</a></li>
<li><a href="https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf" target="_blank" rel="external">Learning neural network policies with guided policy search under unknown dynamics</a></li>
<li><a href="http://www.jmlr.org/papers/volume17/15-522/15-522.pdf" target="_blank" rel="external">End-to-end training of deep visuomotor policies</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Guided-Policy-Search/" rel="tag"># Guided Policy Search</a>
          
            <a href="/tags/GPS/" rel="tag"># GPS</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/31/summary-derivative/" rel="next" title="学习总结《神经网络常用求导》">
                <i class="fa fa-chevron-left"></i> 学习总结《神经网络常用求导》
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/27/tech-vimrc/" rel="prev" title="技术总结《配置Vim》">
                技术总结《配置Vim》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Criss" />
          <p class="site-author-name" itemprop="name">Criss</p>
          <p class="site-description motion-element" itemprop="description">Talk is cheap</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Overivew"><span class="nav-number">1.</span> <span class="nav-text">Overivew</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Motivation"><span class="nav-number">2.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Problem-Formulation"><span class="nav-number">3.</span> <span class="nav-text">Problem Formulation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Framework"><span class="nav-number">4.</span> <span class="nav-text">Framework</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deterministic-policy-case"><span class="nav-number">5.</span> <span class="nav-text">Deterministic policy case</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Collect-data"><span class="nav-number">5.1.</span> <span class="nav-text">Collect data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fit-dynamics"><span class="nav-number">5.2.</span> <span class="nav-text">Fit dynamics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimization"><span class="nav-number">5.3.</span> <span class="nav-text">Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Controller"><span class="nav-number">5.3.1.</span> <span class="nav-text">Controller</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy"><span class="nav-number">5.3.2.</span> <span class="nav-text">Policy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Next-iteration"><span class="nav-number">5.4.</span> <span class="nav-text">Next iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">5.5.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stochastic-policy-case"><span class="nav-number">6.</span> <span class="nav-text">Stochastic policy case</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Using-stochastic-controller"><span class="nav-number">6.1.</span> <span class="nav-text">Using stochastic controller</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Constrain-controller-in-trust-region"><span class="nav-number">6.2.</span> <span class="nav-text">Constrain controller in trust region</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduce-stochastic-policy"><span class="nav-number">6.3.</span> <span class="nav-text">Introduce stochastic policy</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">8.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Criss</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'meltycriss';
      var disqus_identifier = '2018/05/17/summary-gps/';

      var disqus_title = "学习总结《Guided Policy Search》";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  









  
  

  

  

  

  


</body>
</html>
